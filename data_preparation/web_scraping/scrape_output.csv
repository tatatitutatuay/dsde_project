abstract,keyword,country
"We show evidence of particle acceleration at GEV energies associated directly with protons from the prompt emission of a long-duration M6-class solar flare on July 17, 2023, rather than from protons acceleration by shocks from its associated Coronal Mass Ejection (CME), which erupted with a speed of 1342 km/s. Solar Energetic Particles (SEP) accelerated by the blast have reached Earth, up to an almost S3 (strong) category of a radiation storm on the NOAA scale.
Also, we show a temporal correlation between the fast rising of GOES-16 proton and muon excess at ground level in the count rate of the New-Tupi muon detector at the central SAA region.
A Monte Carlo spectral analysis based on muon excess at New-Tupi is consistent with the acceleration of electrons and protons (ions) up to relativistic energies (GeV energy range) in the impulsive phase of the flare. In addition, we present another two marginal particle excesses (with low confidence) at ground-level detectors in correlation with the solar flare prompt emission.","['sun:activity, high-speed stream, cosmic rays modulation']","['Brazil', 'Brazil', 'Brazil']"
"In the wake of large language models, there has been a resurgence of claims and questions about the Turing test and its value for AI, which are reminiscent of decades of practical “Turing” tests. If AI were quantum physics, by now several “Schrödinger’s” cats could have been killed. Better late than never, it is time for a historical reconstruction of Turing’s beautiful thought experiment. In this paper I present a wealth of evidence, including new archival sources, give original answers to several open questions about Turing’s 1950 paper, and address the core question of the value of Turing’s test.","['Alan', 'Turing,', 'Turing test,', 'Thought experiment,', 'Foundations of', 'AI & computer science,', 'Galileo', 'Galilei,', 'History of science,', 'History of', 'AI']",['PauloBrazil']
"Online recruitment platforms typically employ Person-Job Fit models in the core service that automatically match suitable job seekers with appropriate job positions. While existing works leverage historical or contextual information, they often disregard a crucial aspect: job seekers’ social relationships in professional networks. This paper emphasizes the importance of incorporating professional networks into the Person-Job Fit model. Our innovative approach consists of two stages: (1) defining a Workplace Heterogeneous Information Network (WHIN) to capture heterogeneous knowledge, including professional connections and pre-training representations of various entities using a heterogeneous graph neural network; (2) designing a Contextual Social Attention Graph Neural Network (CSAGNN) that supplements users’ missing information with professional connections’ contextual information. We introduce a job-specific attention mechanism in CSAGNN to handle noisy professional networks, leveraging pre-trained entity representations from WHIN. We demonstrate the effectiveness of our approach through experimental evaluations conducted across three real-world recruitment datasets from LinkedIn, showing superior performance compared to baseline models.","['Person-Job', 'Fit,', 'Heterogeneous', 'Information', 'Network,', 'Graph', 'Neural', 'Network']","['UniversityBeijingChina', 'ResearchBeijingChina', 'UniversityBostonUSA', 'ResearchBeijingChina', 'ResearchBeijingChina', 'ResearchBeijingChina', 'CorporationBeijingChina', 'CorporationBeijingChina', 'CorporationBeijingChina']"
"The prediction of tumor progression and chemotherapy response has been recently tackled exploiting Tumor Infiltrating Lymphocytes (TILs) and the nuclear protein Ki67 as prognostic factors.
Recently, deep neural networks (DNNs) have been shown to achieve top results in estimating Ki67 expression and simultaneous determination of intratumoral TILs score in breast cancer cells. However,
in the last ten years the extraordinary progress induced by deep models proliferated
at least as much as their resource demand.
The exorbitant computational costs required to query (and in some cases also to store) a deep model represent a strong limitation in resource-limited contexts, like that of IoT-based applications to support healthcare personnel.
To this end, we propose a resource consumption-aware DNN for the effective estimate of the percentage of Ki67-positive cells in breast cancer screenings. Our approach reduced up to 75%percent7575\%75 % and 89%percent8989\%89 % the usage of memory and disk space respectively, up to 1.5×1.5\times1.5 × the energy consumption, and preserved or improved the overall accuracy of a benchmark state-of-the-art solution. Encouraged by such positive results, we developed and structured the adopted framework so as to allow its general purpose usage, along with a public software repository to support its usage.","['Tumor infiltrating lymphocytes,', 'Ki67 protein,', 'Resource-limited learning,', 'Resource-limited devices,', 'DNN compression,', 'Deep learning.']","['18MilanItaly20133', '(JRC)IspraItaly', '18MilanItaly20133', 'MilanItaly20072', 'MilanItaly20089', '18MilanItaly20133', 'SystemsRomeItaly', '18MilanItaly20133', 'SystemsRomeItaly']"
"Based on the covariant underdamped and overdamped Langevin equations
with Stratonovich coupling to multiplicative noises and the associated
Fokker-Planck equations on Riemannian manifold, we present
the first law of stochastic thermodynamics on the trajectory level.
The corresponding fluctuation theorems are also
established, with the total entropy production of the Brownian particle
and the heat reservoir playing the role of dissipation function.","['Langevin equation,', 'Fokker-Planck equation, fluctuation theorem,', 'Riemannian manifold']","['China', 'China', 'China']"
"Observatories need to measure and evaluate the scientific output and overall impact of their facilities. An observatory bibliography consists of the papers published using that observatory’s data, typically gathered by searching the major journals for relevant keywords. Recently, the volume of literature and methods by which the publications pool is evaluated have increased. Efficient and standardized procedures are necessary to assign meaningful metadata, enable user-friendly retrieval, and provide the opportunity to derive reports, statistics, and visualizations to impart a deeper understanding of the research output.

In 2021, a group of observatory bibliographers from around the world convened online to continue the discussions presented in Lagerstrom (2015). We worked to extract general guidelines from our experiences, techniques, and lessons learnt. This paper explores the development, application, and current status of telescope bibliographies and future trends. The paper briefly describes the methodologies employed in constructing the databases, along with the various bibliometric techniques used to analyze and interpret them. We explain reasons for non-standardization and why it is essential for each observatory to identify metadata and metrics that are meaningful for them; caution the (over-)use of comparisons among various facilities that are, ultimately, not comparable through bibliometrics; and highlight the benefits of telescope bibliographies, both for researchers within the astronomical community and for stakeholders beyond the specific observatories. There is tremendous diversity in the ways bibliographers track publications and maintain databases, due to parameters such as resources (personnel, time, budget, IT capabilities), type of observatory, historical practices, and reporting requirements to funders and outside agencies. However, there are also common sets of Best Practices. This paper describes some of our results from our collaborative discussions.","['Astronomy', 'Databases (83) —', 'Astronomical reference materials (90) —', 'Observatories (1147) —', 'Telescopes (1689)']","['USA', 'Spain', 'Germany', 'USA', 'USA', 'Japan', 'USA', 'USA', 'USA', 'USA', 'USA', 'USA', 'USA', 'USA']"
"A critical function of an organization is to foster the level of integration (coordination and cooperation) necessary to achieve its objectives. The need to coordinate and motivation to cooperate emerges from the myriad dependencies between an organization’s members and their work. Therefore, to reason about solutions to coordination and cooperation problems requires a robust representation that includes the underlying dependencies. We find that such a representation remains missing from formal organizational models, and we leverage semantics to bridge this gap. Drawing on well-established organizational research and our extensive fieldwork with one of North America’s largest municipalities, (1) we introduce an ontology, formalized in first-order logic, that operationalizes concepts like outcome, reward, and epistemic dependence, and their links to potential integration risks; and (2) present real-world applications of this ontology to analyze and support integration in complex government infrastructure projects. Our ontology is implemented and validated in both Z3 and OWL. Key features of our model include inferable dependencies, explainable coordination and cooperation risks, and actionable insights on how dependency structures within an organization can be altered to mitigate the risks. Conceptualizing real-world challenges like incentive misalignment, free-riding, and subgoal optimization in terms of dependency structures, our semantics-based approach represents a novel method for modelling and enhancing coordination and cooperation. Integrated within a decision-support system, our model may serve as an impactful aid for organizational design and effectiveness. More broadly, our approach underscores the transformative potential of semantics in deriving tangible, real-world value from existing organization theory.","['Index', 'Terms: ', 'Cooperation,', 'Coordination,', 'Dependence,', 'Organization,', 'Ontology,', 'OWL,', 'Z3']",['msf@eil.utoronto.ca']
"We present a general framework for modeling materials using deep neural networks. Material represented by multidimensional characteristics (that mimic measurements) is used to train the neural autoencoder model in an unsupervised manner. The encoder is trying to predict the material parameters of a theoretical model, which is then used in a decoder part. The decoder, using the predicted parameters, reconstructs the input characteristics. The neural model is trained to capture a synthetically generated set of characteristics that can cover a broad range of material behaviors, leading to a model that can generalize on the underlying physics rather than just optimize the model parameters for a single measurement. After setting up the model we prove its usefulness in the complex problem of modeling magnetic materials in the frequency and current (out-of-linear range) domains simultaneously.","['Index', 'Terms: \n\nmaterials modeling, deep neural networks, synthetic data']","['Poland', 'Poland', 'Poland']"
"This paper proposes a computational model for policy administration.
As an organization evolves, new users and resources are gradually
placed under the mediation of the access control model. Each time
such new entities are added, the policy administrator must
deliberate on how the access control policy shall be revised to
reflect the new reality. A well-designed access control model must
anticipate such changes so that the administration cost does not
become prohibitive when the organization scales up. Unfortunately,
past Access Control research does not offer a formal way to quantify
the cost of policy administration. In this work, we propose to
model ongoing policy administration in an active learning
framework. Administration cost can be quantified in terms of query
complexity. We demonstrate the utility of this approach by applying
it to the evolution of protection domains. We also modelled
different policy administration strategies in our framework. This
allowed us to formally demonstrate that domain-based policies have a
cost advantage over access control matrices because of the use of
heuristic reasoning when the policy evolves. To the best of our
knowledge, this is the first work to employ an active learning
framework to study the cost of policy deliberation and demonstrate
the cost advantage of heuristic policy administration.","['Access control, policy administration, active learning,\nquery complexity, heuristics']","['CalgaryCalgaryCanada', 'CalgaryCalgaryCanada']"
"Rollback recovery strategies are well-known in
concurrent and distributed systems. In this context,
recovering from unexpected failures is even more
relevant given the non-deterministic nature of
execution, which means that it is practically
impossible to foresee all possible process interactions.

In this work, we consider a message-passing concurrent
programming language where processes interact through
message sending and receiving, but shared memory
is not allowed. In this context, we design a
checkpoint-based
rollback recovery strategy which does not
need a central coordination.
For this purpose, we extend the language
with three new operators: 𝖼𝗁𝖾𝖼𝗄𝖼𝗁𝖾𝖼𝗄\mathsf{check}sansserif_check,
𝖼𝗈𝗆𝗆𝗂𝗍𝖼𝗈𝗆𝗆𝗂𝗍\mathsf{commit}sansserif_commit, and 𝗋𝗈𝗅𝗅𝖻𝖺𝖼𝗄𝗋𝗈𝗅𝗅𝖻𝖺𝖼𝗄\mathsf{rollback}sansserif_rollback.
Furthermore, our approach
is purely asynchronous, which is an essential
ingredient to develop a source-to-source program
instrumentation implementing a rollback recovery
strategy.","['message-passing concurrency, rollback recovery, checkpointing']",['S/NValenciaSpain46022']
"Demagnetization in ferromagnetic transition metals driven by a
femtosecond laser pulse is a fundamental problem in solid state
physics, and its understanding is essential to the development of
spintronics devices. Ab initio calculation of time-dependent magnetic
moment in the velocity gauge so far has not been successful in
reproducing the large amount of demagnetization observed in
experiments. In this work, we propose a method to incorporate
intraband transitions within the velocity gauge through a convective
derivative in the crystal momentum space. Our results for
transition-element bulk crystals (bcc Fe, hcp Co and fcc Ni) based on
the time-dependent quantum Liouville equation show a dramatic
enhancement in the amount of demagnetization after the inclusion of an
intraband term, in agreement with experiments. We also find that the
effect of intraband transitions to each ferromagnetic material is
distinctly different because of their band structure and spin property
differences. Our finding has a far-reaching impact on understanding
of ultrafast demagnetization.","['femtomagnetism, all optical spin switching, time dependent quantum', 'Liouville equation, m-mixing, circularly-polarized laser field']","['USA', 'USA']"
"In this paper, we elaborate on correctly predicting Échelle spectrograms by employing the fully three-dimensional representation of Snell’s law to model the effects of prisms as cross-dispersers in Échelle spectrographs.
We find that it is not sufficient to simply apply the frequently used trigonometric prism dispersion equation to describe recorded spectra.
This vector equation approach is not limited to a single dispersive element when modeling multi-prism cross-disperser configurations.
Our results help to understand the main levers in an Échelle spectrograph as well as contribute to auto-calibration algorithms for minimizing calibration efforts in daily operation.","['Prism;', 'Echelle;', 'Cross-Disperser;', 'Snell’s law;', 'Sellmeier;', 'QtYETI']","['München', 'work', 'work']"
"The integration of sensorized vessels, enabling real-time data collection and machine learning-driven data analysis marks a pivotal advancement in the maritime industry. This transformative technology not only can enhance safety, efficiency, and sustainability but also usher in a new era of cost-effective and smart maritime transportation in our increasingly interconnected world. This study presents a deep learning-driven anomaly detection system augmented with interpretable machine learning models for identifying performance anomalies in an industrial sensorized vessel, called TUCANA. We Leverage a human-in-the-loop unsupervised process that involves utilizing standard and Long Short-Term Memory (LSTM) autoencoders augmented with interpretable surrogate models, i.e., random forest and decision tree, to add transparency and interpretability to the results provided by the deep learning models. The interpretable models also enable automated rule generation for translating the inference into human-readable rules. Additionally, the process also includes providing a projection of the results using t-distributed stochastic neighbor embedding (t-SNE), which helps with a better understanding of the structure and relationships within the data and assessment of the identified anomalies. We empirically evaluate the system using real data acquired from the vessel TUCANA and the results involve achieving over 80% precision and 90% recall with the LSTM model used in the process. The interpretable models also provide logical rules aligned with expert thinking, and the t-SNE-based projection enhances interpretability. Our system demonstrates that the proposed approach can be used effectively in real-world scenarios, offering transparency and precision in performance anomaly detection.","['Index', 'Terms: ', 'Performance', 'Anomaly', 'Detection,', 'Human-in-the-loop', 'Learning', 'Process,', 'Deep', 'Learning,', 'Interpretable', 'Machine', 'Learning,', 'Sensorized', 'Vessels,', 'Maritime', 'Industry.']","['mahshid.helali@mdu.se', 'lukasz.kulas}@pg.edu.pl']"
"The synergy of language and vision models has given rise to Large Language and Vision Assistant models (LLVAs), designed to engage users in rich conversational experiences intertwined with image-based queries. These comprehensive multimodal models seamlessly integrate vision encoders with Large Language Models (LLMs), expanding their applications in general-purpose language and visual comprehension. The advent of Large Multimodal Models (LMMs) heralds a new era in Artificial Intelligence (AI) assistance, extending the horizons of AI utilization. This paper takes a unique perspective on LMMs, exploring their efficacy in performing image classification tasks using tailored prompts designed for specific datasets. We also investigate the LLVAs zero-shot learning capabilities. Our study includes a benchmarking analysis across four diverse datasets: MNIST, Cats Vs. Dogs, Hymnoptera (Ants Vs. Bees), and an unconventional dataset comprising Pox Vs. Non-Pox skin images. The results of our experiments demonstrate the model’s remarkable performance, achieving classification accuracies of 85%, 100%, 77%, and 79% for the respective datasets without any fine-tuning. To bolster our analysis, we assess the model’s performance post fine-tuning for specific tasks. In one instance, fine-tuning is conducted over a dataset comprising images of faces of children with and without autism. Prior to fine-tuning, the model demonstrated a test accuracy of 55%, which significantly improved to 83% post fine-tuning. These results, coupled with our prior findings, underscore the transformative potential of LLVAs and their versatile applications in real-world scenarios.","['Index', 'Terms: ', 'Large', 'Language', 'Models,', 'Large', 'Multimodal', 'Models,', 'Prompt', 'Engineering,', 'Classification']","['0000-0002-9717-3252', '0000-0002-5145-1990', '0000-0003-1521-5568', '0000-0003-2336-0490', '0000-0001-7389-3274']"
"With rising concerns about the security of IoT devices, network operators need better ways to handle potential risks. Luckily, IoT devices show consistent patterns in how they communicate. But despite previous efforts, it remains unclear how knowledge of these patterns can be made available.
As data marketplaces become popular in different domains, this paper111This manuscript is the full version of our paper [1] accepted to the IEEE/IFIP NOMS 2024 conference. proposes creating a special marketplace focused on IoT cybersecurity. The goal is to openly share knowledge about IoT devices’ behavior, using structured data formats like Manufacturer Usage Description (MUD) files. To make this work, we employ technologies like blockchain and smart contracts to build a practical and secure foundation for sharing and accessing important information about how IoT devices should behave on the network.
Our contributions are two-fold.
(1) We identify the essential features of an effective marketplace for sharing data related to the expected behaviors of IoT devices. We develop a smart contract on the Ethereum blockchain with five concrete functions; and,
(2) We implement a prototype of our marketplace in a private chain environment—our codes are publicly released. We demonstrate how effectively our marketplace functions through experiments involving MUD files from consumer IoT devices. Our marketplace enables suppliers and consumers to share MUD data on the Ethereum blockchain for under a hundred dollars, promoting accessibility and participation.","['Index', 'Terms: \n', 'Decentralized data marketplace,', 'IoT behaviors,', 'MUD files']",['h.habibi@}unsw.edu.au']
"This work introduces the L3Cube-MahaSocialNER dataset, the first and largest social media dataset specifically designed for Named Entity Recognition (NER) in the Marathi language. The dataset comprises 18,000 manually labeled sentences covering eight entity classes, addressing challenges posed by social media data, including non-standard language and informal idioms. Deep learning models, including CNN, LSTM, BiLSTM, and Transformer models, are evaluated on the individual dataset with IOB and non-IOB notations. The results demonstrate the effectiveness of these models in accurately recognizing named entities in Marathi informal text. The L3Cube-MahaSocialNER dataset offers user-centric information extraction and supports real-time applications, providing a valuable resource for public opinion analysis, news, and marketing on social media platforms. We also show that the zero-shot results of the regular NER model are poor on the social NER test set thus highlighting the need for more social NER datasets. The datasets and models are publicly available at https://github.com/l3cube-pune/MarathiNLP","['Named', 'Entity', 'Recognition,', 'Deep', 'Learning,', 'Natural', 'Language', 'Processing,', 'BERT, mBERT,', 'ALBERT,', 'RoBERTa,', 'Muril,', 'Indic bert,', 'Convolutional', 'Neural', 'Network,', 'Bidirectional long short-term memory,', 'Long short-term memory,', 'Marathi', 'NER,', 'Efficient', 'NLP']","['India', 'India', 'India', 'India', 'India']"
"Distributed computing in Blockchain Technology (BCT) hinges on a trust assumption among independent nodes. Without a third-party interface or what’s known as a ‘Blockchain Oracle’, it can’t interact with the external world. This Oracle plays a crucial role by feeding extrinsic data into the Blockchain, ensuring that Smart Contracts operate accurately in real time. The ‘Oracle problem’ arises from the inherent difficulty in verifying the truthfulness of the data sourced by these Oracles. The genuineness of a Blockchain Oracle is paramount, as it directly influences the Blockchain’s reliability, credibility, and scalability. To tackle these challenges, a strategy rooted in Byzantine fault-tolerance ϕitalic-ϕ\phiitalic_ϕ is introduced. Furthermore, an autonomous system for sustainability and audibility, built on heuristic detection, is put forth. The effectiveness and precision of the proposed strategy outperformed existing methods using two real-world datasets, aimed to meet the authenticity standards for Blockchain Oracles.","['Index', 'Terms: ', 'Blockchain', 'Oracles,', 'Trust', 'Assumption,', 'Asymmetric', 'Byzantine', 'Quorums,', 'Smart', 'Contracts,', 'Oracle', 'Data', 'Reliability,', 'Blockchain', 'Scalability', 'Solutions,', 'Decentralized', 'Applications (DApps).']","['France', 'France', 'USA']"
"We present a framework to assist therapists and children with autism spectrum disorder in their Applied Behavioral Analysis (ABA) therapy. The framework was designed in collaboration with Spazio Autismo, an autism center in Mantova, Italy. The framework is a first step toward transitioning from the current paper-based to fully digital-supported therapy. We evaluated the framework over four months with 18 children diagnosed with classic autism, ranging from 4 to 7 years old. The framework integrates a mobile app that children and therapists use during the sessions with a backend for managing therapy workflow and monitoring progress. Our preliminary results show that the framework can improve the efficacy of the therapy sessions, reducing non-therapeutic time, increasing patient focus, and quickening the completion of the assigned objectives. It can also support therapists in preparing learning materials, data acquisition, and reporting. Finally, the framework demonstrated improved privacy and security of patients’ data while maintaining reliability.","['Index', 'Terms: \n', 'Autism,', 'Applied', 'Behavioural', 'Analysis,', 'Serious', 'Games,', 'Gamification']","['https://www.solcomantova.it/servizi/autismo/', 'Italy']"
"Atacama Large Millimeter/Submillimeter Array (ALMA) has revolutionized the field of dust polarization in protoplanetary disks across multiple wavelengths. Previous observations and empirical modeling suggested multiple mechanisms of dust polarization toward HL Tau, including grain alignment and dust scattering. However, a detailed modeling of dust polarization based on grain alignment physics is not yet available. Here, using our updated POLARIS code, we perform numerical modeling of dust polarization arising from both grain alignment by Magnetically Enhanced Radiative Torque (MRAT) mechanism and self-scattering to reproduce the HL Tau polarization observed at three wavelengths 0.87, 1.3, and 3.1 mm. Our modeling results show that the observed multi-wavelength polarization could be reproduced only when large grains contain embedded iron inclusions and those with slow internal relaxation must have wrong internal alignment (i.e., the grain’s major axis parallel to its angular momentum). The abundance of iron embedded inside grains in the form of clusters is constrained to be ≳16greater-than-or-equivalent-toabsent16\gtrsim 16≳ 16%, and the number of iron atoms per cluster is Ncl∼2×103similar-tosubscript𝑁cl2superscript103N_{\rm cl}\sim 2\times 10^{3}italic_N start_POSTSUBSCRIPT roman_cl end_POSTSUBSCRIPT ∼ 2 × 10 start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT. Maximum grain sizes probed at wavelengths λ𝜆\lambdaitalic_λ = 0.87, 1.3, and 3.1 mm are constrained at ∼similar-to\sim∼ 60, 90, and 130μ𝜇\,\muitalic_μm, respectively. Assuming a dust differential settling effect with grain sizes from the constraint gives the value of maximum grain size at the disk mid-plane to be millimeter-scaled.","['Protoplanetary disks;', 'Polarimetry;', 'Radio astronomy;', 'Circumstellar dust;', 'Magnetic field']","['Vietnam', 'Vietnam', 'Vietnam', 'Vietnam', 'Korea', 'Korea', 'Germany', 'Vietnam', 'Vietnam', 'Vietnam', 'Vietnam', 'Korea', 'Korea']"
"The intrinsic resolution is the primary limitation on the total energy resolution of LaBr33{}_{3}start_FLOATSUBSCRIPT 3 end_FLOATSUBSCRIPT(Ce) crystal. This intrinsic resolution arises from two effects: fluctuations occurring in the process of energy transfer to luminescent centers within the LaBr33{}_{3}start_FLOATSUBSCRIPT 3 end_FLOATSUBSCRIPT(Ce) crystal and the LaBr33{}_{3}start_FLOATSUBSCRIPT 3 end_FLOATSUBSCRIPT(Ce) crystal’s non-proportional luminescence. Presently, experimental measurements regarding the intrinsic resolution of LaBr33{}_{3}start_FLOATSUBSCRIPT 3 end_FLOATSUBSCRIPT(Ce) crystal are scarce, and the underlying physical mechanisms remain incompletely understood. In this paper, we aim to elucidate the concept of intrinsic resolution. We investigated the entire physical process of luminescence following energy deposition in the LaBr33{}_{3}start_FLOATSUBSCRIPT 3 end_FLOATSUBSCRIPT(Ce) crystal, quantifying the various components in the total energy resolution. We conducted a series of experimental measurements and Geant4 simulations, determining the intrinsic resolution of LaBr33{}_{3}start_FLOATSUBSCRIPT 3 end_FLOATSUBSCRIPT(Ce) crystal to 100 keV electrons as 2.12%. The non-proportionality contributes significantly at 1.43%, while fluctuations in the energy transfer process accounted for 0.27%. It is evident that non-proportionality in light output constitutes the primary source of intrinsic resolution. Horizontal and vertical unevenness in light collection contributed 0.25% and 0.07%, respectively. Statistical fluctuations showed the largest impact on the total energy resolution, at 2.86%. The contribution from fluctuations in single-photoelectron events was 0.77%. Furthermore, we reconstructed the photon response using Geant4, and the consistency between the simulated relative light yield and the experimentally measured one confirmed the reliability of the LaBr33{}_{3}start_FLOATSUBSCRIPT 3 end_FLOATSUBSCRIPT(Ce) detector mass model employed in the simulation.","['LaBr33{}_{3}start_FLOATSUBSCRIPT 3 end_FLOATSUBSCRIPT(Ce) detector,', 'Energy', 'Response,', 'Intrinsic', 'Resolution,', 'Non-proportional', 'Light', 'Yield,', 'Energy', 'Transfer', 'Process']","['China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China']"
"The intrinsic alignment (IA) of galaxies acts as a systematic effect in weak lensing measurements and tends to introduce biases. It mimics the gravitational lensing signal which makes it difficult to distinguish it from the true gravitational weak lensing effect. Hence, it is critical to account for the noise for correctly interpreting the results. This study aims at a quantitative analysis of IA using the Tidal Alignment and Tidal Torquing (TATT) model. We also investigate how the signals for shear and galaxy-galaxy lensing behave upon changing the parameters of the TATT model.
The data for this study was prepared with a computational pipeline based on the Cocoa model to explore the parameter space of the intrinsic shape signal.
Through this work, we identify that linear terms of the intrinsic shape signal are dominant in the case of GGL while the higher-order terms dictate the shear signal.","['Intrinsic', 'Alignments —', 'Weak', 'Gravitational', 'Lensing —', 'Tidal', 'Alignment —', 'Tidal', 'Torquing —', 'Cosmic', 'Shear —', 'Galaxy', 'Alignments']",['States']
"Agency is an important human characteristic that users of automated complex technologies are usually denied.
This affects the user’s experience leading to decreased satisfaction and productivity.
In this paper, we consider the ridesharing context and interviewed 7 drivers to understand the controls that would improve the agency they feel.
The results show that they desire transparency, community and an effective ability to seek redress.","['User', 'Agency,', 'Ridesharing']","['ParkPAUSA16802', 'ParkPAUSA16802']"
"This paper explores the impact of biologically plausible neuron models on the performance of Spiking Neural Networks (SNNs) for regression tasks. While SNNs are widely recognized for classification tasks, their application to Scientific Machine Learning and regression remains underexplored. We focus on the membrane component of SNNs, comparing four neuron models: Leaky Integrate-and-Fire, FitzHugh–Nagumo, Izhikevich, and Hodgkin-Huxley. We investigate their effect on SNN accuracy and efficiency for function regression tasks, by using Euler and Runge-Kutta 4th-order approximation schemes. We show how more biologically plausible neuron models improve the accuracy of SNNs while reducing the number of spikes in the system. The latter represents an energetic gain on actual neuromorphic chips since it directly reflects the amount of energy required for the computations.","['Index', 'Terms: ', 'Spiking', 'Neural', 'Networks,', 'LIF model,', 'FitzHugh-Nagumo model,', 'Izhikevich model,', 'Hodgkin-Huxley model,', 'Regression,', 'Scientific', 'Machine', 'Learning']","['mario_de_florio@brown.edu', 'adar_kahana@brown.edu', 'george_karniadakis@brown.edu']"
"It has been more than 30 years since the enigmatic 21 μ𝜇\muitalic_μm emission feature was first discovered in protoplanetary nebulae (PPNs).
Although dozens of different dust carrier candidates have been proposed, there is as yet no widely accepted one.
We present the results of molecular observations toward 21 μ𝜇\muitalic_μm objects using the 10 m Submillimeter Telescope of Arizona Radio Observatory at the 1.3 mm band
and the 13.7 m telescope of Purple Mountain Observatory at the 3 mm band,
aiming to investigate whether the gas-phase environments of these unusual sources have some peculiarities compared to normal PPNs.
We detect 31 emission lines belonging to seven different molecular species, most of which are the first detection in 21 μ𝜇\muitalic_μm PPNs.
The observations provide clues on the identification of the 21 μ𝜇\muitalic_μm feature.
We report a correlation study between the fractional abundance of gas-phase molecules and the strengths of the 21 μ𝜇\muitalic_μm emission.
Our study shows that given the small sample size, the 21 μ𝜇\muitalic_μm feature has weak or no correlations with the gas-phase molecules.
Future radio observations of high spatial and spectral resolution toward a large sample are desirable to elucidate the 21 μ𝜇\muitalic_μm emission phenomena.","['21\u2009μ𝜇\\muitalic_μm feature —', 'ISM: molecules — circumstellar matter —', 'Line: identification —', 'Circumstellar envelopes']","['China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China']"
"Answering questions using pre-trained language models (LMs) and knowledge graphs (KGs) presents challenges in identifying relevant knowledge and performing joint reasoning. We compared LMs (fine-tuned for the task) with the previously published QAGNN method for the Question-answering (QA) objective and further measured the impact of additional factual context on the QAGNN performance. The QAGNN method employs LMs to encode QA context and estimate KG node importance, and effectively update the question choice entity representations using Graph Neural Networks (GNNs). We further experimented with enhancing the QA context encoding by incorporating relevant knowledge facts for the question stem. The models are trained on the OpenbookQA dataset, which contains ~6000 4-way multiple choice questions and is widely used as a benchmark for QA tasks.
Through our experimentation, we found that incorporating knowledge facts context led to a significant improvement in performance. In contrast, the addition of knowledge graphs to language models resulted in only a modest increase. This suggests that the integration of contextual knowledge facts may be more impactful for enhancing question-answering performance compared to solely adding knowledge graphs.","['Knowledge graphs (KG), language models (LM), question and answering (QA), graph neural networks (GNN)']","['GeorgiaAtlantaGeorgiaUSA', 'GeorgiaAtlantaGeorgiaUSA', 'GeorgiaAtlantaGeorgiaUSA', 'GeorgiaAtlantaGeorgiaUSA']"
"We perform a combined fit of the invariant mass distribution of X⁢(3872)→J/ψ⁢π+⁢π−→𝑋3872𝐽𝜓superscript𝜋superscript𝜋{X(3872)}\rightarrow{J}/{\psi}\pi^{+}\pi^{-}italic_X ( 3872 ) → italic_J / italic_ψ italic_π start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT italic_π start_POSTSUPERSCRIPT - end_POSTSUPERSCRIPT from LHCb and X⁢(3872)→D0⁢D¯0⁣*→𝑋3872superscript𝐷0superscript¯𝐷0{X(3872)}\rightarrow{D}^{0}\overline{D}^{0*}italic_X ( 3872 ) → italic_D start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT over¯ start_ARG italic_D end_ARG start_POSTSUPERSCRIPT 0 * end_POSTSUPERSCRIPT from Belle using an effective field theory approach. In this approach, we can directly determine the Z𝑍Zitalic_Z which is the probability of finding the compact component in X⁢(3872)𝑋3872{X(3872)}italic_X ( 3872 ). In the combined analysis, we find that the Z𝑍Zitalic_Z is 0.52±0.11plus-or-minus0.520.110.52\pm 0.110.52 ± 0.11 for X⁢(3872)𝑋3872{X(3872)}italic_X ( 3872 ).","['Exotic hadron,', 'X\u2062(3872)𝑋3872{X(3872)}italic_X ( 3872 ),', 'Compositeness criterion,', 'Effective field theory,', 'Lineshape']","['China', 'China']"
"Deep neural networks have significantly improved the performance of face forgery detection models in discriminating Artificial Intelligent Generated Content (AIGC). However, their security is significantly threatened by the injection of triggers during model training (i.e., backdoor attacks). Although existing backdoor defenses and manual data selection can mitigate those using human-eye-sensitive triggers, such as patches or adversarial noises, the more challenging natural backdoor triggers remain insufficiently researched. To further investigate natural triggers, we propose a novel analysis-by-synthesis backdoor attack against face forgery detection models, which embeds natural triggers in the latent space. We thoroughly study such backdoor vulnerability from two perspectives: (1) Model Discrimination (Optimization-Based Trigger): we adopt a substitute detection model and find the trigger by minimizing the cross-entropy loss; (2) Data Distribution (Custom Trigger): we manipulate the uncommon facial attributes in the long-tailed distribution to generate poisoned samples without the supervision from detection models. Furthermore, to completely evaluate the detection models towards the latest AIGC, we utilize both state-of-the-art StyleGAN and Stable Diffusion for trigger generation. Finally, these backdoor triggers introduce specific semantic features to the generated poisoned samples (e.g., skin textures and smile), which are more natural and robust. Extensive experiments show that our method is superior from three levels: (1) Attack Success Rate: ours achieves a high attack success rate (over 99%percent\%%) and incurs a small model accuracy drop (below 0.2%percent\%%) with a low poisoning rate (less than 3%percent\%%); (2) Backdoor Defense: ours shows better robust performance when faced with existing backdoor defense methods; (3) Human Inspection: ours is less human-eye-sensitive from a comprehensive user study.",['Backdoor attacks; face forgery detection; facial attribute editing'],"['SciencesBeijingChina', 'SciencesBeijingChina', 'SciencesBeijingChina', 'TechnologyNanjingChina', 'SciencesBeijingChina']"
"In today’s era, users have increasingly high expectations regarding the performance and efficiency of communication networks. Network operators aspire to achieve efficient network planning, operation, and optimization through Digital Twin Networks (DTN). The effectiveness of DTN heavily relies on the network model, with graph neural networks (GNN) playing a crucial role in network modeling. However, existing network modeling methods still lack a comprehensive understanding of communication networks. In this paper, we propose DWNet (Deeper and Wider Networks), a heterogeneous graph neural network modeling method based on data-driven approaches that aims to address end-to-end latency and jitter prediction in network models. This method stands out due to two distinctive features: firstly, it introduces deeper levels of state participation in the message passing process; secondly, it extensively integrates relevant features during the feature fusion process. Through experimental validation and evaluation, our model achieves higher prediction accuracy compared to previous research achievements, particularly when dealing with unseen network topologies during model training. Our model not only provides more accurate predictions but also demonstrates stronger generalization capabilities across diverse topological structures.","['Index', 'Terms: \ndigital twin, graph neural networks, deep learning, network modeling']",['0000-0002-2978-0659']
"The following paper proposes a new target localization system design using an architecture based on reconfigurable intelligent surfaces (RISs) and passive radars (PRs) for integrated sensing and communications systems.
The preamble of the communication signal is exploited in order to perform target sensing tasks, which involve detection and localization.
The RIS in this case can aid the PR in sensing targets that are otherwise not seen by the PR itself, due to the many obstacles encountered within the propagation channel.
Therefore, this work proposes a localization algorithm tailored for the integrated sensing and communications RIS-aided architecture, which is capable of uniquely positioning targets within the scene.
The algorithm is capable of detecting the number of targets along with estimating the position of targets via angles and times of arrival.
Our simulation results demonstrate the performance of the localization method in terms of different localization and detection metrics and for increasing RIS sizes.","['Index', 'Terms: \nintegrated sensing and communications (ISAC), reconfigurable intelligent surfaces (RIS), 6G, localization, passive radar']","['UAE.', 'USA']"
"Accreting supermassive black holes (SMBHs) frequently power jets that interact with the interstellar/circumgalactic medium (ISM/CGM), regulating star-formation in the galaxy. Highly supersonic jets launched by active galactic nuclei (AGN) power a cocoon that confines them and shocks the ambient medium. We build upon the models of narrow conical jets interacting with a smooth ambient medium, to include the effect of dense clouds that are an essential ingredient of a multiphase ISM. The key physical ingredient of this model is that the clouds along the supersonic jet-beam strongly decelerate the jet-head, but the subsonic cocoon easily moves around the clouds without much resistance. We propose scalings for important physical quantities – cocoon pressure, head & cocoon speed, and jet radius. We obtain, for the first time, the analytic condition on clumpiness of the ambient medium for the jet to dissipate within the cocoon and verify it with numerical simulations of conical jets interacting with a uniform ISM with embedded spherical clouds. A jet is defined to be dissipated when the cocoon speed exceeds the speed of the jet-head. We compare our models to more sophisticated numerical simulations, direct observations of jet-ISM interaction (e.g., quasar J1316+1753), and discuss implications for the Fermi/eROSITA bubbles. Our work also motivates effective subgrid models for AGN jet feedback in a clumpy ISM unresolved by the present generation of cosmological galaxy formation simulations.","['ISM: jets and outflows – galaxies: jets –', 'ISM: clouds – galaxies: clusters: intracluster medium']","['India', 'India', 'Israel', 'India', 'USA']"
"Despite recent initiatives aimed at improving accessibility, the field of digital accessibility remains markedly behind contemporary advancements in the software industry as a large number of real world software and web applications continue to fall short of accessibility requirements.
A persisting skills deficit within the existing technology workforce has been an enduring impediment, hindering organizations from delivering truly accessible software products.
This, in turn, elevates the risk of isolating and excluding a substantial portion of potential users.
In this paper, we report lessons learned from a training program for teaching digital accessibility using the Communities of Practice (CoP) framework to industry professionals.
We recruited 66 participants from a large multi-national software company and assigned them to two groups: one participating in a CoP and the other using self-paced learning.
We report experiences from designing the training program, conducting the actual training, and assessing the efficiency of the two approaches.
Based on these findings, we provide recommendations for practitioners in Learnng and Development teams and educators in designing accessibility courses for industry professionals.","['Accessibility (a11y), massive open online courses, communities of practice,', 'Computing', 'Education']","['CampusGoaIndia403726', 'CampusGoaIndia403726']"
"A UserWay study in 2021 indicates that an annual global e-commerce revenue loss of approximately $16 billion can be attributed to inaccessible websites and applications. According to the 2023 WebAIM study, only 3.7% of the world’s top one million website homepages are fully accessible. This shows that many software developers use poor coding practices that don’t adhere to the Web Content Accessibility Guidelines (WCAG). This research centers on software professionals and their role in addressing accessibility. This work seeks to understand (a) who within the software development community actively practices accessibility, (b) when and how accessibility is considered in the software development lifecycle, (c) the various challenges encountered in building accessible software, and (d) the resources required by software professionals to enhance product accessibility. Our survey of 269 software professionals from India sheds light on the pressing need for accessibility education within the software industry. A substantial majority (69.9%, N=269) of respondents express the need for training materials, workshops, and bootcamps to enhance their accessibility skills. We present a list of actionable recommendations that can be implemented within the industry to promote accessibility awareness and skills. We also open source our raw data for further research, encouraging continued exploration in this domain.","['Accessibility (a11y),', 'Indian', 'IT', 'Industry, accessibility education']","['CampusGoaIndia403726', 'CampusGoaIndia403726']"
"The industrial Internet of Things (IIoT) involves the integration of Internet of Things (IoT) technologies into industrial settings. However, given the high sensitivity of the industry to the security of industrial control system networks and IIoT, the use of software-defined networking (SDN) technology can provide improved security and automation of communication processes. Despite this, the architecture of SDN can give rise to various security threats. Therefore, it is of paramount importance to consider the impact of these threats on SDN-based IIoT environments. Unlike previous research, which focused on security in IIoT and SDN architectures separately, we propose an integrated method including two components that work together seamlessly for better detecting and preventing security threats associated with SDN-based IIoT architectures. The two components consist in a convolutional neural network-based Intrusion Detection System (IDS) implemented as an SDN application and a Blockchain-based system (BS) to empower application layer and network layer security, respectively.
A significant advantage of the proposed method lies in jointly minimizing the impact of attacks such as command injection and rule injection on SDN-based IIoT architecture layers.
The proposed IDS exhibits superior classification accuracy in both binary and multiclass categories.","['Index', 'Terms: ', 'Blockchain,', 'Industrial', 'IoT,', 'SDN,', 'Deep learning,', 'Intrusion detection system, and', 'Security.']",['tarik.taleb}@oulu.fi']
"Generative models of expressive piano performance are usually assessed by comparing their predictions to a reference human performance.
A generative algorithm is taken to be better than competing ones if it produces
performances that are closer to a human reference performance.
However, expert human performers can (and do) interpret music in different ways, making for different possible references, and quantitative closeness is not necessarily aligned with perceptual similarity, raising concerns about the validity of this evaluation approach.
In this work, we present a number of experiments that shed light on this problem.
Using precisely measured high-quality performances of classical piano music,
we carry out a listening test indicating that listeners can sometimes perceive subtle performance difference that go unnoticed under quantitative evaluation.
We further present tests that indicate that such evaluation frameworks show a lot of variability in reliability and validity across different reference performances and pieces.
We discuss these results and their implications for quantitative evaluation, and hope to foster a critical appreciation of the uncertainties involved in quantitative assessments of such performances within the wider music information retrieval (MIR) community.","['Performance,', 'Expression,', 'Evaluation,', 'Validity,', 'Listening', 'Study']","['UniversityLinzAustria', 'UniversityLinzAustria', 'UniversityLinzAustria', 'UniversityLinzAustria']"
"To improve our understanding of the quark-gluon dynamics underlying multiquark states, we systematically study their electromagnetic properties. In this study, the electromagnetic properties of the X⁢(4140)X4140\mathrm{X(4140)}roman_X ( 4140 ) and X⁢(4630)X4630\mathrm{X(4630)}roman_X ( 4630 ) states with the quantum numbers JPC=1++superscriptJPCsuperscript1absent\mathrm{J^{PC}=1^{++}}roman_J start_POSTSUPERSCRIPT roman_PC end_POSTSUPERSCRIPT = 1 start_POSTSUPERSCRIPT + + end_POSTSUPERSCRIPT and JPC=1−+superscriptJPCsuperscript1absent\mathrm{J^{PC}=1^{-+}}roman_J start_POSTSUPERSCRIPT roman_PC end_POSTSUPERSCRIPT = 1 start_POSTSUPERSCRIPT - + end_POSTSUPERSCRIPT, respectively are investigated within the framework of the QCD light-cone sum rules method by considering the diquark-antidiquark configuration of these states. We also calculate the magnetic and quadrupole moments of the theoretically predicted singly-charmed state, XAVsubscriptXAV\mathrm{X_{AV}}roman_X start_POSTSUBSCRIPT roman_AV end_POSTSUBSCRIPT, with the quantum numbers JP=1+superscriptJPsuperscript1\mathrm{J^{P}=1^{+}}roman_J start_POSTSUPERSCRIPT roman_P end_POSTSUPERSCRIPT = 1 start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT. The predicted results for the magnetic moments are as μX⁢(4140)=−1.11−0.31+0.41⁢μNsubscript𝜇X4140subscriptsuperscript1.110.410.31subscript𝜇𝑁\mu_{\mathrm{X(4140)}}=-1.11^{+0.41}_{-0.31}~{}\mu_{N}italic_μ start_POSTSUBSCRIPT roman_X ( 4140 ) end_POSTSUBSCRIPT = - 1.11 start_POSTSUPERSCRIPT + 0.41 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT - 0.31 end_POSTSUBSCRIPT italic_μ start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT, μX⁢(4630)=−0.62−0.11+0.13⁢μNsubscript𝜇X4630subscriptsuperscript0.620.130.11subscript𝜇𝑁\mu_{\mathrm{X(4630)}}=-0.62^{+0.13}_{-0.11}~{}\mu_{N}italic_μ start_POSTSUBSCRIPT roman_X ( 4630 ) end_POSTSUBSCRIPT = - 0.62 start_POSTSUPERSCRIPT + 0.13 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT - 0.11 end_POSTSUBSCRIPT italic_μ start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT, and μXAV=−0.98−0.21+0.27⁢μNsubscript𝜇subscriptXAVsubscriptsuperscript0.980.270.21subscript𝜇𝑁\mu_{\mathrm{X_{AV}}}=-0.98^{+0.27}_{-0.21}~{}\mu_{N}italic_μ start_POSTSUBSCRIPT roman_X start_POSTSUBSCRIPT roman_AV end_POSTSUBSCRIPT end_POSTSUBSCRIPT = - 0.98 start_POSTSUPERSCRIPT + 0.27 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT - 0.21 end_POSTSUBSCRIPT italic_μ start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT. The results obtained can be useful in determining the exact nature of these states. This work will hopefully stimulate experimental interest in the study of the electromagnetic properties of multiquark systems.","['Magnetic and quadrupole moments, tetraquarks, diquark-antidiquark picture,', 'QCD light-cone sum rules']",['Türkiye']
"Deep learning techniques have been applied in the context of image super-resolution (SR), achieving remarkable advances in terms of reconstruction performance. Existing techniques typically employ highly complex model structures which result in large model sizes and slow inference speeds. This often leads to high energy consumption and restricts their adoption for practical applications. To address this issue, this work employs a three-stage workflow for compressing deep SR models which significantly reduces their memory requirement. Restoration performance has been maintained through teacher-student knowledge distillation using a newly designed distillation loss. We have applied this approach to two popular image super-resolution networks, SwinIR and EDSR, to demonstrate its effectiveness. The resulting compact models, SwinIRmini and EDSRmini, attain an 89% and 96% reduction in both model size and floating-point operations (FLOPs) respectively, compared to their original versions. They also retain competitive super-resolution performance compared to their original models and other commonly used SR approaches. The source code and pre-trained models for these two lightweight SR approaches are released at https://pikapi22.github.io/CDISM/.","['Index', 'Terms: ', 'Image super-resolution, complexity reduction, model compression, knowledge distillation']",['dave.bull}@bristol.ac.uk']
"The Photometric objects Around Cosmic webs (PAC) approach developed in Xu et al. (2022b) has the advantage of making full use of spectroscopic and deeper photometric surveys. With the merits of PAC, the excess surface density n¯2⁢wpsubscript¯𝑛2subscript𝑤p\bar{n}_{2}w_{{\rm{p}}}over¯ start_ARG italic_n end_ARG start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT italic_w start_POSTSUBSCRIPT roman_p end_POSTSUBSCRIPT of neighboring galaxies can be measured down to stellar mass 1010.80⁢M⊙superscript1010.80subscript𝑀direct-product10^{10.80}\,M_{\odot}10 start_POSTSUPERSCRIPT 10.80 end_POSTSUPERSCRIPT italic_M start_POSTSUBSCRIPT ⊙ end_POSTSUBSCRIPT around quasars at redshift 0.8<zs<1.00.8subscript𝑧s1.00.8<z_{\rm{s}}<1.00.8 < italic_z start_POSTSUBSCRIPT roman_s end_POSTSUBSCRIPT < 1.0, with the data from the Sloan Digital Sky Survey IV (SDSS-IV) extended Baryon Oscillation Spectroscopic Survey (eBOSS) and the Dark Energy Spectroscopic Instrument (DESI) Legacy Imaging Surveys. We find that n¯2⁢wpsubscript¯𝑛2subscript𝑤p\bar{n}_{2}w_{{\rm{p}}}over¯ start_ARG italic_n end_ARG start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT italic_w start_POSTSUBSCRIPT roman_p end_POSTSUBSCRIPT generally increases quite steeply with the decrease of the separation. Using subhalo abundance matching method, we can accurately model the n¯2⁢wpsubscript¯𝑛2subscript𝑤p\bar{n}_{2}w_{{\rm{p}}}over¯ start_ARG italic_n end_ARG start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT italic_w start_POSTSUBSCRIPT roman_p end_POSTSUBSCRIPT both on small and large scales. We show that the steep increase of the n¯2⁢wpsubscript¯𝑛2subscript𝑤p\bar{n}_{2}w_{{\rm{p}}}over¯ start_ARG italic_n end_ARG start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT italic_w start_POSTSUBSCRIPT roman_p end_POSTSUBSCRIPT towards the quasars requires that a large fraction fsate=0.29−0.06+0.05subscript𝑓satesuperscriptsubscript0.290.060.05f_{\mathrm{sate}}=0.29_{-0.06}^{+0.05}italic_f start_POSTSUBSCRIPT roman_sate end_POSTSUBSCRIPT = 0.29 start_POSTSUBSCRIPT - 0.06 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT + 0.05 end_POSTSUPERSCRIPT of quasars should be satellites in massive halos, and find that this fraction measurement is insensitive to the assumptions of our modeling. This high satellite fraction indicates that the subhalos have nearly the same probability to host quasars as the halos for the same (infall) halo mass, and the large scale environment has negligible effect on the quasar activity. We show that even with this high satellite fraction, each massive halo on average does not host more than one satellite quasar due to the sparsity of quasars.","['AGN host galaxies(2017) —', 'Stellar mass function(1612) —', 'Quasars(1319) —', 'Active galaxies(17)']","['China', 'China', 'UK', 'China', 'China', 'China', 'China', 'China']"
"Wind is one kind of high-efficient, environmentally-friendly and cost-effective energy source.
Wind power, as one of the largest renewable energy in the world, has been playing a more and more important role in supplying electricity.
Though growing dramatically in recent years, the amount of generated wind power can be directly or latently affected by multiple uncertain factors, such as wind speed, wind direction, temperatures, etc.
More importantly, there exist very complicated dependencies of the generated power on the latent composition of these multiple time-evolving variables,
which are always ignored by existing works and thus largely hinder the prediction performances.
To this end, we propose DEWP, a novel Deep Expansion learning for Wind Power forecasting framework to carefully model the complicated dependencies with adequate expressiveness.
DEWP starts with a stack-by-stack architecture, where each stack is composed of (i) a variable expansion block that makes use of convolutional layers to capture dependencies among multiple variables;
(ii) a time expansion block that applies Fourier series and backcast/forecast mechanism to learn temporal dependencies in sequential patterns.
These two tailored blocks expand raw inputs into different latent feature spaces which can model different levels of dependencies of time-evolving sequential data.
Moreover, we propose an inference block corresponding for each stack, which applies multi-head self-attentions to acquire attentive features and maps expanded latent representations into generated wind power.
In addition, to make DEWP more expressive in handling deep neural architectures, we adapt doubly residue learning to process stack-by-stack outputs. Accurate wind power forecasting is then better achieved through fine-grained outputs by continuously removing stack residues and accumulating useful stack forecasts.
Finally, we present extensive experiments in the real-world wind power forecasting application on two datasets from two different turbines, in order to demonstrate the effectiveness of our approach.","['Wind', 'Power', 'Forecasting,', 'Time', 'Series', 'Forecasting,', 'Deep', 'Learning']","['OxfordOxfordUK', 'UniversityTempeArizonaUSA', 'ResearchBeijingChina', 'SciencesBeijingChina', 'TechnologyGuangzhouChina']"
"Smart contracts are computer programs running on blockchains to automate the transaction execution between users.
The absence of contract specifications poses a real challenge to the correctness verification of
smart contracts.
Program invariants are properties that are always preserved throughout the execution, which
characterize an important aspect of the program behaviors.
In this paper, we propose a novel invariant generation framework, InvCon+, for Solidity smart
contracts.
InvCon+ extends the existing invariant detector, InvCon, to automatically produce verified
contract invariants based on both dynamic inference and static verification.
Unlike InvCon+, InvCon only produces likely invariants, which have a high probability to hold,
yet are still not verified against the contract code.
Particularly, InvCon+ is able to infer more expressive invariants that capture richer semantic
relations of contract code.
We evaluate InvCon+ on 361 ERC20 and 10 ERC721 real-world contracts, as well as common
ERC20 vulnerability benchmarks.
The experimental results indicate that InvCon+ efficiently produces high-quality invariant
specifications, which can be used to secure smart contracts from common vulnerabilities.","['Index', 'Terms: ', 'Smart contract, invariant detection.']",['Singapore']
"Integer sorting is a fundamental problem in computer science.
This paper studies parallel integer sort both in theory and in practice.
In theory, we show tighter bounds for a class of
existing practical integer sort algorithms, which provides a solid
theoretical foundation for their widespread usage
in practice and strong performance.
In practice, we design a new integer sorting algorithm,
DovetailSort, that is theoretically-efficient and has good practical performance.
In particular, DovetailSort overcomes a common challenge in existing
parallel integer sorting algorithms, which is the difficulty of detecting and
taking advantage of duplicate keys.
The key insight in DovetailSort is to combine algorithmic ideas from
both integer- and comparison-sorting algorithms.
In our experiments, DovetailSort achieves competitive or better
performance than existing state-of-the-art parallel integer and
comparison sorting algorithms on various synthetic and real-world
datasets.","['Integer', 'Sort,', 'Radix', 'Sort,', 'Sorting', 'Algorithms,', 'Parallel', 'Algorithms']","['Riverside', 'Maryland', 'Riverside', 'Riverside']"
"Intelligent Transportation System (ITS) is vital in improving traffic congestion, reducing traffic accidents, optimizing urban planning, etc.
However, due to the complexity of the traffic network, traditional machine learning and statistical methods are relegated to the background.
With the advent of the artificial intelligence era, many deep learning frameworks have made remarkable progress in various fields and are now considered effective methods in many areas.
As a deep learning method, Graph Neural Networks (GNNs) have emerged as a highly competitive method in the ITS field since 2019 due to their strong ability to model graph-related problems. As a result, more and more scholars pay attention to the applications of GNNs in transportation domains, which have shown excellent performance.
However, most of the research in this area is still concentrated on traffic forecasting, while other ITS domains, such as autonomous vehicles and urban planning, still require more attention.
This paper aims to review the applications of GNNs in six representative and emerging ITS domains: traffic forecasting, autonomous vehicles, traffic signal control, transportation safety, demand prediction, and parking management. We have reviewed extensive graph-related studies from 2018 to 2023, summarized their methods, features, and contributions, and presented them in informative tables or lists.
Finally, we have identified the challenges of applying GNNs to ITS and suggested potential future directions.","['Traffic', 'Flow', 'Prediction,', 'Graph', 'Neural', 'Network,', 'Spatio-temporal', 'Analysis']","['UniversityBeijingChina100871', 'AngelesUSA90095', 'UniversityBeijingChina100871', 'AngelesUSA90095', 'UniversityBeijingChina100871']"
"Machine translation systems have been widely adopted in our daily life, making life easier and more convenient. Unfortunately, erroneous translations may result in severe consequences, such as financial losses.
This requires to improve the accuracy and the reliability of machine translation systems.
However, it is challenging to test machine translation systems because of the complexity and intractability of the underlying neural models.
To tackle these challenges, we propose a novel metamorphic testing approach by syntactic tree pruning (STP) to validate machine translation systems.
Our key insight is that a pruned sentence should have similar crucial semantics compared with the original sentence.
Specifically, STP
(1) proposes a core semantics-preserving pruning strategy by basic sentence structures and dependency relations on the level of syntactic tree representation;
(2) generates source sentence pairs based on the metamorphic relation;
(3) reports suspicious issues whose translations break the consistency property by a bag-of-words model.
We further evaluate STP on two state-of-the-art machine translation systems (i.e., Google Translate and Bing Microsoft Translator) with 1,200 source sentences as inputs.
The results show that STP accurately finds 5,073 unique erroneous translations in Google Translate and 5,100 unique erroneous translations in Bing Microsoft Translator (400% more than state-of-the-art techniques), with 64.5% and 65.4% precision, respectively.
The reported erroneous translations vary in types and more than 90% of them are not found by state-of-the-art techniques.
There are 9,393 erroneous translations unique to STP, which is 711.9% more than state-of-the-art techniques.
Moreover, STP is quite effective in detecting translation errors for the original sentences with a recall reaching 74.0%, improving state-of-the-art techniques by 55.1% on average.","['Software testing,', 'Machine translation,', 'Metamorphic testing']","['UniversityNanjingJiangsuChina210093', 'AmherstAmherstMAUSA01003', 'UniversityNanjingJiangsuChina210093', 'UniversityNanjingJiangsuChina210093', 'UniversityNanjingJiangsuChina210093', 'UniversityNanjingJiangsuChina210093', 'UniversityNanjingJiangsuChina210093']"
"X-ray binaries are known to launch powerful accretion disk winds that can have significant impact on the binary systems and their surroundings. To quantify the impact and determine the launching mechanisms of these outflows, we need to measure the wind plasma number density, an important ingredient in the theoretical disk wind models. While X-ray spectroscopy is a crucial tool to understanding the wind properties, such as their velocity and ionization, in nearly all cases, we lack the signal-to-noise to constrain the plasma number density, weakening the constraints on outflow location and mass outflow rate. We present a new approach to determine this number density in the X-ray binary Hercules X-1 by measuring the speed of the wind ionization response to time-variable illuminating continuum. Hercules X-1 is powered by a highly magnetized neutron star, pulsating with a period of 1.24 s. We show that the wind number density in Hercules X-1 is sufficiently high to respond to these pulsations by modeling the ionization response with the time-dependent photoionization model tpho. We then perform a pulse-resolved analysis of the best-quality XMM-Newton observation of Hercules X-1 and directly detect the wind response, confirming that the wind density is at least 1012superscript101210^{12}10 start_POSTSUPERSCRIPT 12 end_POSTSUPERSCRIPT cm−33{}^{-3}start_FLOATSUPERSCRIPT - 3 end_FLOATSUPERSCRIPT. Finally, we simulate XRISM observations of Hercules X-1 and show that they will allow us to accurately measure the number density at different locations within the outflow. With XRISM we will rule out ∼3similar-toabsent3\sim 3∼ 3 orders of magnitude in density parameter space, constraining the wind mass outflow rate, energetics, and its launching mechanism.",['Accretion (14)'],"['02139', 'USA', '60637', '02139', '02139', 'UK', 'Italy', '02139', 'Germany', 'UK']"
"Patient representation learning based on electronic health records (EHR) is a critical task for disease prediction. This task aims to effectively extract useful information on dynamic features. Although various existing works have achieved remarkable progress, the model performance can be further improved by fully extracting the trends, variations, and the correlation between the trends and variations in dynamic features. In addition, sparse visit records limit the performance of deep learning models. To address these issues, we propose the Multi-perspective Patient Representation Extractor (MPRE) for disease prediction. Specifically, we propose Frequency Transformation Module (FTM) to extract the trend and variation information of dynamic features in the time-frequency domain, which can enhance the feature representation. In the 2D Multi-Extraction Network (2D MEN), we form the 2D temporal tensor based on trend and variation. Then, the correlations between trend and variation are captured by the proposed dilated operation. Moreover, we propose the First-Order Difference Attention Mechanism (FODAM) to calculate the contributions of differences in adjacent variations to the disease diagnosis adaptively. To evaluate the performance of MPRE and baseline methods, we conduct extensive experiments on two real-world public datasets. The experiment results show that MPRE outperforms state-of-the-art baseline methods in terms of AUROC and AUPRC.","['Index', 'Terms: ', 'Disease', 'Prediction,', 'Patient', 'Representation,', 'Visit', 'Records']",['giovanni.pau@unibo.it']
"By 2050, it is predicted that there will be 9 billion people on the planet, which will call for more production, lower costs, and the preservation of natural resources. It is anticipated that atypical occurrences and climate change will pose severe risks to agricultural output. It follows that a 70% or more significant rise in food output is anticipated. Smart farming, often known as agriculture 4.0, is a tech-driven revolution in agriculture with the goal of raising industry production and efficiency. Four primary trends are responsible for it: food waste, climate change, population shifts, and resource scarcity. The agriculture industry is changing as a result of the adoption of emerging technologies. Using cutting-edge technology like IoT, AI, and other sensors, smart farming transforms traditional production methods and international agricultural policies. The objective is to establish a value chain that is optimized to facilitate enhanced monitoring and decreased labor expenses. The agricultural sector has seen tremendous transformation as a result of the fourth industrial revolution, which has combined traditional farming methods with cutting-edge technology to increase productivity, sustainability, and efficiency. To effectively utilize the potential of technology gadgets in the agriculture sector, collaboration between governments, private sector entities, and other stakeholders is necessary. This paper covers Agriculture 4.0, looks at its possible benefits and drawbacks of the implementation methodologies, compatibility, reliability, and investigates the several digital tools that are being utilized to change the agriculture industry and how to mitigate the challenges.","['Index', 'Terms: ', 'Smart', 'Farming,', 'Agriculture 4.0,', 'Precision', 'Farming,', 'Sustainable,', 'IoT,', 'Security,', 'Sensor']",['States']
"High-frequency wide-bandwidth cellular communications over mmW and sub-THz offer the opportunity for high data rates, however, it also presents high pathloss, resulting in limited coverage. To mitigate the coverage limitations, high-gain beamforming is essential. Implementation of beamforming involves a large number of antennas, which introduces analog beam constraint, i.e., only one frequency-flat beam is generated per transceiver chain (TRx). Recently introduced joint phase-time array (JPTA) architecture, which utilizes both true time delay (TTD) units and phase shifters (PSs), alleviates analog beam constraint by creating multiple frequency-dependent beams per TRx, for scheduling multiple users at different directions in a frequency-division manner. One class of previous studies offered solutions with “rainbow” beams, which tend to allocate a small bandwidth per beam direction. Another class focused on uniform linear array (ULA) antenna architecture, whose frequency-dependent beams were designed along a single axis of either azimuth or elevation direction. In this paper, we present a novel 3D beamforming codebook design aimed at maximizing beamforming gain to steer radiation toward desired azimuth and elevation directions, as well as across sub-bands partitioned according to scheduled users’ bandwidth requirements. We provide both analytical solutions and iterative algorithms to design the PSs and TTD units for a desired subband beam pattern. Through simulations of the beamforming gain, we observe that our proposed solutions outperform the state-of-the-art solutions reported elsewhere.
††This work was done in part while O. Yildiz was an intern at Samsung Research America.","['Index', 'Terms: ', 'True time delay, beamforming, millimeter wave, 3D, joint phase-time array, uniform planar array']",['USA']
"The integration of Artificial Intelligence (AI), particularly Large Language Model (LLM)-based systems, in education has shown promise in enhancing teaching and learning experiences. However, the advent of Multimodal Large Language Models (MLLMs) like GPT-4 with vision (GPT-4V), capable of processing multimodal data including text, sound, and visual inputs, opens a new era of enriched, personalized, and interactive learning landscapes in education. Grounded in theory of multimedia learning, this paper explores the transformative role of MLLMs in central aspects of science education by presenting exemplary innovative learning scenarios. Possible applications for MLLMs could range from content creation to tailored support for learning, fostering competencies in scientific practices, and providing assessment and feedback. These scenarios are not limited to text-based and uni-modal formats but can be multimodal, increasing thus personalization, accessibility, and potential learning effectiveness. Besides many opportunities, challenges such as data protection and ethical considerations become more salient, calling for robust frameworks to ensure responsible integration. This paper underscores the necessity for a balanced approach in implementing MLLMs, where the technology complements rather than supplants the educator’s role, ensuring thus an effective and ethical use of AI in science education. It calls for further research to explore the nuanced implications of MLLMs on the evolving role of educators and to extend the discourse beyond science education to other disciplines. Through the exploration of potentials, challenges, and future implications, we aim to contribute to a preliminary understanding of the transformative trajectory of MLLMs in science education and beyond.","['Artificial', 'Intelligence,', 'Large', 'Language', 'Models (LLMs),', 'ChatGPT,', 'Multimodal', 'Learning,', 'Cognitive', 'Theory of', 'Multimedia', 'Learning,', 'Science', 'Education']","['MunichMunichBYGermany', 'GeorgiaAthensGAUSA']"
"Despite significant progress in deep learning-based optical flow methods, accurately estimating large displacements and repetitive patterns remains a challenge. The limitations of local features and similarity search patterns used in these algorithms contribute to this issue. Additionally, some existing methods suffer from slow runtime and excessive graphic memory consumption. To address these problems, this paper proposes a novel approach based on the RAFT framework. The proposed Attention-based Feature Localization (AFL) approach incorporates the attention mechanism to handle global feature extraction and address repetitive patterns. It introduces an operator for matching pixels with corresponding counterparts in the second frame and assigning accurate flow values. Furthermore, an Amorphous Lookup Operator (ALO) is proposed to enhance convergence speed and improve RAFT’s ability to handle large displacements by reducing data redundancy in its search operator and expanding the search space for similarity extraction. The proposed method, Efficient RAFT (Ef-RAFT), achieves significant improvements of 10% on the Sintel dataset and 5% on the KITTI dataset over RAFT. Remarkably, these enhancements are attained with a modest 33% reduction in speed and a mere 13% increase in memory usage. The code is available at: https://github.com/n3slami/Ef-RAFT","['Index', 'Terms: ', 'Optical', 'Flow,', 'Large', 'Displacement,', 'Repetitive', 'Patterns,', 'Attention', 'Mechanism,', 'Deep', 'Neural', 'Networks']",['kasaei}@sharif.edu']
"In distributed radar systems, when several transmitters radiate simultaneously,
the reflected signals
need to be distinguished at the receivers to detect various targets. If the transmit signals are in different frequency bands, they require a large overall bandwidth.
Instead, a set of pseudo-orthogonal waveforms derived from the Zadoff-Chu (ZC) sequences could be accommodated in the same band, enabling the efficient use of available bandwidth for better range resolution.
In such a design, special care must be given to the ‘near-far’ problem, where a reflection could possibly become difficult to detect due to the presence of stronger reflections.
In this work, a scheme to detect multiple targets in such distributed radar systems is proposed.
It performs successive cancellations (SC) starting from the strong, detectable reflections in the domain of the Discrete Chirp-Fourier Transform (DCFT) after compensating for Doppler shifts, enabling the subsequent detections of weaker targets which are not trivially detectable.
Numerical
simulations corroborate the efficacy and usefulness
of the proposed method in detecting weak target reflections.","['Index', 'Terms: ', 'Distributed', 'Radar,', 'Multistatic', 'Radar,', 'Multi-Target', 'Detection,', 'Zadoff-Chu', 'Sequences,', 'Successive', 'Cancellation,', 'Discrete', 'Chirp-Fourier', 'Transform (DCFT).']",['k.giridhar}@telwise-research.com']
"The nearly continuous stream of miniature comets dominated by the
Kreutz sungrazers has been an unexpected great bonanza for cometary
science initiated by the launch of the Solar and Heliospheric Observatory
(SOHO) in 1995. Over the nearly 30 years since the time, no serious attempt
has been made to formulate a self-consistent model for the formation and
evolution of this stream of Kreutz comets — the goal of the present two-part
investigation. Part I describes historical highlights of the research
that has been relevant to the problem of SOHO sungrazers (including the major
contributions by Hubbard, Kreutz, and Marsden) and furnishes preliminaries
of diagnostic value that are intended to facilitate, and provide critical
information for, the work in Part II. Formerly noted issues, such as the
high frequency of close pairs in the SOHO database, are proposed to be
products of a broader process of swarming, seen in both the nodal longitude
and time. I present examples of tight swarms revealed by high arrival rates
of the SOHO Kreutz sungrazers, primarily from Population I.","['Subject headings: individual comets:', 'X/1106', 'C1,', 'C/1843', 'D1,', 'C/1880', 'C1,', 'C/1882', 'R1,', 'C/1963', 'R1,', 'C/1965', 'S1,', 'C/1970', 'K1,', 'C/2011', 'W3; methods: data analysis']",['ZdenSek@gmail.com']
"Light and sound waves have the fascinating property that they can move objects through the transfer of linear or angular momentum. This ability has led to the development of optical and acoustic tweezers, with applications ranging from biomedical engineering to quantum optics. Although impressive manipulation results have been achieved, the stringent requirement for a highly controlled, low-reverberant, and static environment still hinders the applicability of these techniques in many scenarios. Here, we overcome this challenge and demonstrate the manipulation of objects in disordered and dynamic media, by optimally tailoring the momentum of sound waves iteratively in the far field. The method does not require information about the object’s physical properties or the spatial structure of the surrounding medium but relies only on a real-time scattering matrix measurement and a positional guidestar. Our experiment demonstrates the possibility of optimally moving and rotating objects, extending the reach of wave-based object manipulation to complex and dynamic scattering media. We envision new opportunities for biomedical applications, sensing, or manufacturing.","['Object manipulation, wave-momentum shaping, scattering, adaptive acoustics.']","['Switzerland.', 'Kazakhstan.', 'Switzerland.', 'France.', 'Austria', 'Switzerland.']"
"The magnetic state of UO22{}_{2}start_FLOATSUBSCRIPT 2 end_FLOATSUBSCRIPT was determined experimentally to be anti-ferromagnetic. Starting from this experimental fact, researchers have calculated other properties within the Hubbard-corrected density-functional theory, DFT+U. Up to now, the Hubbard parameters for UO22{}_{2}start_FLOATSUBSCRIPT 2 end_FLOATSUBSCRIPT were usually so chosen that the calculations give good results for some experimental data.
Also, to our knowledge there exists no valid theoretical research report on the energetically stable magnetic state of this system. In present work, employing the new method which is based on density-functional perturbation theory, we have determined self-consistently the Hubbard parameters and ground-state energies for UO22{}_{2}start_FLOATSUBSCRIPT 2 end_FLOATSUBSCRIPT crystal in both ferromagnetic and anti-ferromagnetic configurations, and the calculated results show that UO22{}_{2}start_FLOATSUBSCRIPT 2 end_FLOATSUBSCRIPT crystal energetically favors an anti-ferromagnetic state with a small energy difference.
In all the calculations the PBE-sol approximation was used for the exchange-correlation energy functional.","['Uranium dioxide;', 'Ferromagnetism;', 'Anti-ferromagnetism;', 'Density-Functional', 'Theory;', 'Hubbard', 'Model;', 'Mott', 'Insulator;', 'DFT+U.']",['Iran']
"In the absence of data protection measures, software applications lead to privacy breaches, posing threats to end-users and software organisations. Privacy Enhancing Technologies (PETs) are technical measures that protect personal data, thus minimising such privacy breaches. However, for software applications to deliver data protection using PETs, software developers should actively and correctly incorporate PETs into the software they develop. Therefore, to uncover ways to encourage and support developers to embed PETs into software, this Systematic Literature Review (SLR) analyses 39 empirical studies on developers’ privacy practices. It reports the usage of six PETs in software application scenarios. Then, it discusses challenges developers face when integrating PETs into software, ranging from intrinsic challenges, such as the unawareness of PETs, to extrinsic challenges, such as the increased development cost. Next, the SLR presents the existing solutions to address these challenges, along with the limitations of the solutions. Further, it outlines future research avenues to better understand PETs from a developer perspective and minimise the challenges developers face when incorporating PETs into software.","['Privacy', 'Enhancing', 'Technologies, data protection, developers, secure computation']","['Zealand', 'CSIROCanberraAustralia', 'UniversitySydneyAustralia', 'Zealand']"
"Rush hour and sustained traffic flows in eight cities are studied using the IBM Mega
Traffic Simulator to understand the importance of road structures and vehicle
acceleration in the prevention of gridlock. Individual cars among the tens of
thousands launched are monitored at every simulation time step using live streaming
data transfer from the simulation software to analysis software on another computer.
A measure of gridlock is the fraction of cars moving at less than 30% of their
local road speed. Plots of this fraction versus the instantaneous number of cars on
the road show hysteresis during rush hour simulations, indicating that it can take
twice as long to unravel clogged roads as fill them. The area under the hysteresis
loop is used as a measure of gridlock to compare different cities normalized to the
same central areas. The differences between cities, combined with differences
between idealized models using square or triangular road grids, indicate that
gridlock tends to occur most when there are a small number of long roads that
channel large fractions of traffic. These long roads help light traffic flow but
they make heavy flows worse. Increasing the speed on these long roads makes gridlock
even worse in heavy conditions. City throughput rates are also modeled using a
smooth ramp up to a constant vehicle launch rate. Models with increasing
acceleration for the same road speeds show clear improvements in city traffic flow
as a result of faster interactions at intersections and merging points. However,
these improvements are relatively small when the gridlock is caused by long roads
having many cars waiting to exit at the same intersection. In general, gridlock in
our models begins at intersections regardless of the available road space in the
network.","['traffic, cities, simulation, agent based, gridlock']",['10598']
"We report unbiased AI measurements of the fine structure constant α𝛼\alphaitalic_α in two proximate absorption regions in the spectrum of the quasar HE0515−--4414. The data are high resolution, high signal to noise, and laser frequency comb calibrated, obtained using the ESPRESSO spectrograph on the VLT. The high quality of the data and proximity of the regions motivate a differential comparison, exploring the possibility of spatial variations of fundamental constants, as predicted in some theories. We show that if the magnesium isotopic relative abundances are terrestrial, the fine structure constants in these two systems differ at the 7σ𝜎\sigmaitalic_σ level. A 3σ𝜎\sigmaitalic_σ discrepancy between the two measurements persists even for the extreme non-terrestrial case of 100% 2424{}^{24}start_FLOATSUPERSCRIPT 24 end_FLOATSUPERSCRIPTMg, if shared by both systems. However, if Mg isotopic abundances take independent values in these two proximate systems, one terrestrial, the other with no heavy isotopes, both can be reconciled with a terrestrial α𝛼\alphaitalic_α, and the discrepancy between the two measurements falls to 2σ𝜎\sigmaitalic_σ. We discuss varying constant and varying isotope interpretations and resolutions to this conundrum for future high precision measurements.","['Cosmology: cosmological parameters –', 'Cosmology: dark energy –', 'Cosmology: dark matter –', 'Galaxies: intercluster medium –', 'Electroweak interaction']","['UK,', 'UK,', 'Australia.', 'Australia.', 'Italy,', 'Italy,', 'Italy.', 'Australia.', 'UK.']"
"The proliferation of social network data has unlocked unprecedented opportunities for extensive, data-driven exploration of human behavior. The structural intricacies of social networks offer insights into various computational social science issues, particularly concerning social influence and information diffusion. However, modeling large-scale social network data comes with computational challenges. Though large language models make it easier than ever to model textual content, any advanced network representation methods struggle with scalability and efficient deployment to out-of-sample users. In response, we introduce a novel approach tailored for modeling social network data in user detection tasks. This innovative method integrates localized social network interactions with the capabilities of large language models. Operating under the premise of social network homophily, which posits that socially connected users share similarities, our approach is designed to address these challenges. We conduct a thorough evaluation of our method across seven real-world social network datasets, spanning a diverse range of topics and detection tasks, showcasing its applicability to advance research in computational social science.","['Social', 'Network,', 'User', 'Detection,', 'User', 'Behavior,', 'Network', 'Homophily']","['USA', 'USA']"
"Using martingale theory, we compute, in very few lines, exact analytical expressions for various first-exit-time statistics associated with one-dimensional biased diffusion. Examples include the distribution for the first-exit time from an interval, moments for the first-exit site, and functionals of the position, which involve memory and time integration. As a key example, we compute analytically the mean area swept by a biased diffusion until it escapes an interval that may be asymmetric and have arbitrary length. The mean area allows us to derive the hitherto unexplored cross-correlation function between the first-exit time and the first-exit site, which vanishes only for exit problems from symmetric intervals. As a colophon, we explore connections of our results with gambling, showing that betting on the time-integrated value of a losing game it is possible to design a strategy that leads to a net average win.","['Martingale theory,', 'First-passage processes,', 'Brownian motion']","['Italy', 'Italy', 'Italy', 'Italy']"
"Sperm whales (Physeter macrocephalus) navigate underwater with a series of impulsive, click-like sounds known as echolocation clicks. These clicks are characterized by a multipulse structure (MPS) that serves as a distinctive pattern. In this work, we use the stability of the MPS as a detection metric for recognizing and classifying the presence of clicks in noisy environments. To distinguish between noise transients and to handle simultaneous emissions from multiple sperm whales, our approach clusters a time series of MPS measures while removing potential clicks that do not fulfil the limits of inter-click interval, duration and spectrum.
 As a result, our approach can handle high noise transients and low signal-to-noise ratio. The performance of our detection approach is examined using three datasets: seven months of recordings from the Mediterranean Sea containing manually verified ambient noise; several days of manually labelled data collected from the Dominica Island containing approximately 40,000 clicks from multiple sperm whales; and a dataset from the Bahamas containing 1,203 labelled clicks from a single sperm whale. Comparing with the results of two benchmark detectors, a better trade-off between precision and recall is observed as well as a significant reduction in false detection rates, especially in noisy environments. To ensure reproducibility, we provide our database of labelled clicks along with our implementation code.","['Index', 'Terms: \n', 'Sperm whale clicks, passive acoustic monitoring (PAM), real-time detection,', 'Inter-pulse interval (IPI),', 'Inter-click interval (ICI).']","['Israel', 'Croatia', 'roee.d@univ.haifa.ac.il']"
"Open Source Intelligence (OSINT) investigations, which rely entirely on publicly available data such as social media, play an increasingly important role in solving crimes and holding governments accountable. The growing volume of data and complex nature of tasks, however, means there is a pressing need to scale and speed up OSINT investigations. Expert-led crowdsourcing approaches show promise, but tend to either focus on narrow tasks or domains, or require resource-intense, long-term relationships between expert investigators and crowds. We address this gap by providing a flexible framework that enables investigators across domains to enlist crowdsourced support for discovery and verification of OSINT. We use a design-based research (DBR) approach to develop OSINT Research Studios (ORS), a sociotechnical system in which novice crowds are trained to support professional investigators with complex OSINT investigations. Through our qualitative evaluation, we found that ORS facilitates ethical and effective OSINT investigations across multiple domains.
We also discuss broader implications of expert–crowd collaboration and opportunities for future work.","['OSINT, open source intelligence, design-based research, social media investigation, collaboration, crowdsourcing']","['TechBlacksburgVAUSA', 'CollegeSwarthmorePAUSA', 'TechArlingtonVAUSA']"
"We present the UV-to-NIR size evolution of a sample of 161 quiescent galaxies with M*>1010⁢M☉subscript𝑀superscript1010subscript𝑀☉M_{*}>10^{10}M_{\sun}italic_M start_POSTSUBSCRIPT * end_POSTSUBSCRIPT > 10 start_POSTSUPERSCRIPT 10 end_POSTSUPERSCRIPT italic_M start_POSTSUBSCRIPT ☉ end_POSTSUBSCRIPT over 0.5<z<50.5𝑧50.5<z<50.5 < italic_z < 5. With deep multi-band NIRCam images in GOODS-South from JADES, we measure the effective radii (Resubscript𝑅𝑒R_{e}italic_R start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT) of the galaxies at rest-frame 0.3, 0.5 and 1µmµm\micronroman_µm. On average, we find that quiescent galaxies are 45% (15%) more compact at rest-frame 1µmµm\micronroman_µm than they are at 0.3µmµm\micronroman_µm (0.5µmµm\micronroman_µm). Regardless of wavelengths, the Resubscript𝑅𝑒R_{e}italic_R start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT of quiescent galaxies strongly evolves with redshift, and this evolution depends on stellar mass. For lower-mass quiescent galaxies with M*=1010−1010.6⁢M☉subscript𝑀superscript1010superscript1010.6subscript𝑀☉M_{*}=10^{10}-10^{10.6}M_{\sun}italic_M start_POSTSUBSCRIPT * end_POSTSUBSCRIPT = 10 start_POSTSUPERSCRIPT 10 end_POSTSUPERSCRIPT - 10 start_POSTSUPERSCRIPT 10.6 end_POSTSUPERSCRIPT italic_M start_POSTSUBSCRIPT ☉ end_POSTSUBSCRIPT, the evolution follows Re∝(1+z)−1.1proportional-tosubscript𝑅𝑒superscript1𝑧1.1R_{e}\propto(1+z)^{-1.1}italic_R start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT ∝ ( 1 + italic_z ) start_POSTSUPERSCRIPT - 1.1 end_POSTSUPERSCRIPT, whereas it becomes steeper, following Re∝(1+z)−1.7proportional-tosubscript𝑅𝑒superscript1𝑧1.7R_{e}\propto(1+z)^{-1.7}italic_R start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT ∝ ( 1 + italic_z ) start_POSTSUPERSCRIPT - 1.7 end_POSTSUPERSCRIPT, for higher-mass quiescent galaxies with M*>1010.6⁢M☉subscript𝑀superscript1010.6subscript𝑀☉M_{*}>10^{10.6}M_{\sun}italic_M start_POSTSUBSCRIPT * end_POSTSUBSCRIPT > 10 start_POSTSUPERSCRIPT 10.6 end_POSTSUPERSCRIPT italic_M start_POSTSUBSCRIPT ☉ end_POSTSUBSCRIPT. To constrain the physical mechanisms driving the apparent size evolution, we study the relationship between Resubscript𝑅𝑒R_{e}italic_R start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT and the formation redshift (zformsubscript𝑧formz_{\rm{form}}italic_z start_POSTSUBSCRIPT roman_form end_POSTSUBSCRIPT) of quiescent galaxies. For lower-mass quiescent galaxies, this relationship is broadly consistent with Re∝(1+zform)−1proportional-tosubscript𝑅𝑒superscript1subscript𝑧form1R_{e}\propto(1+z_{\rm{form}})^{-1}italic_R start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT ∝ ( 1 + italic_z start_POSTSUBSCRIPT roman_form end_POSTSUBSCRIPT ) start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT, in line with the expectation of the progenitor effect. For higher-mass quiescent galaxies, the relationship between Resubscript𝑅𝑒R_{e}italic_R start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT and zformsubscript𝑧formz_{\rm{form}}italic_z start_POSTSUBSCRIPT roman_form end_POSTSUBSCRIPT depends on stellar age. Older quiescent galaxies have a steeper relationship between Resubscript𝑅𝑒R_{e}italic_R start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT and zformsubscript𝑧formz_{\rm{form}}italic_z start_POSTSUBSCRIPT roman_form end_POSTSUBSCRIPT than that expected from the progenitor effect alone, suggesting that mergers and/or post-quenching continuous gas accretion drive additional size growth in very massive systems. We find that the z>3𝑧3z>3italic_z > 3 quiescent galaxies in our sample are very compact, with mass surface densities Σe≳1010⁢M☉/kpc2greater-than-or-equivalent-tosubscriptΣ𝑒superscript1010subscript𝑀☉superscriptkpc2\Sigma_{e}\gtrsim 10^{10}M_{\sun}/\rm{kpc}^{2}roman_Σ start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT ≳ 10 start_POSTSUPERSCRIPT 10 end_POSTSUPERSCRIPT italic_M start_POSTSUBSCRIPT ☉ end_POSTSUBSCRIPT / roman_kpc start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT, and their Resubscript𝑅𝑒R_{e}italic_R start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT are possibly even smaller than anticipated from the size evolution measured for lower-redshift quiescent galaxies. Finally, we take a close look at the structure of GS-9209, one of the earliest confirmed massive quiescent galaxies at zspec∼4.7similar-tosubscript𝑧spec4.7z_{\rm{spec}}\sim 4.7italic_z start_POSTSUBSCRIPT roman_spec end_POSTSUBSCRIPT ∼ 4.7. From UV to NIR, GS-9209 becomes increasingly compact, and its light profile becomes more spheroidal, showing that the color gradient is already present in this earliest massive quiescent galaxy.","['Galaxy formation(595);', 'Galaxy evolution(594);', 'Galaxy structure(622);', 'High-redshift galaxies(734)']","['USA', 'USA', 'USA', 'USA', 'USA', 'UK', 'UK', 'USA', 'USA', 'USA', 'UK', 'UK', 'Canada', 'Spain', 'USA', 'Australia', 'Australia', 'UK', 'Italy', 'France', 'USA', 'UK', 'UK', 'UK', 'UK', 'Germany', 'USA', 'USA', 'USA', 'USA', '21218', 'USA', 'UK', 'UK', 'USA', 'UK', 'UK', 'UK', 'USA', 'USA', 'USA', 'USA', 'Germany', 'UK', 'UK', 'USA', 'UK', 'UK', 'USA', 'Canada', 'UK', 'UK']"
"The study of cometary composition is important for understanding our solar system’s early evolutionary processes. Carbon dioxide (CO22{}_{2}start_FLOATSUBSCRIPT 2 end_FLOATSUBSCRIPT) is a common hypervolatile in comets that can drive activity but is more difficult to study than other hypervolatiles due to severe telluric absorption. CO22{}_{2}start_FLOATSUBSCRIPT 2 end_FLOATSUBSCRIPT can only be directly observed from space-borne assets. Therefore, a proxy is needed to measure CO22{}_{2}start_FLOATSUBSCRIPT 2 end_FLOATSUBSCRIPT abundances in comets using ground-based observations. The flux ratio of the [O i] 5577 Å line to the sum of the [O i] 6300 Å and [O i] 6364 Å lines (hereafter referred to as the [O i] line ratio) has, with some success, been used in the past as such a proxy. We present an [O i] line ratio analysis of comet 45P/Honda–Mrkos–Pajdušáková (HMP), using data obtained with the Tull Coudé Spectrograph on the 2.7-meter Harlan J. Smith telescope at McDonald Observatory, taken from UT February 21-23, 2017 when the comet was at heliocentric distances of 1.12-1.15 AU. HMP is a hyperactive Jupiter family comet (JFC). Icy grains driven out by CO22{}_{2}start_FLOATSUBSCRIPT 2 end_FLOATSUBSCRIPT sublimation have been proposed as a driver of hyperactivity, but the CO22{}_{2}start_FLOATSUBSCRIPT 2 end_FLOATSUBSCRIPT abundance of HMP has not been measured. From our [O i] line ratio measurements, we find a CO22{}_{2}start_FLOATSUBSCRIPT 2 end_FLOATSUBSCRIPT/H22{}_{2}start_FLOATSUBSCRIPT 2 end_FLOATSUBSCRIPTO ratio for HMP of 22.9±1.4%plus-or-minus22.9percent1.422.9\pm 1.4\%22.9 ± 1.4 %. We compare the CO22{}_{2}start_FLOATSUBSCRIPT 2 end_FLOATSUBSCRIPT/H22{}_{2}start_FLOATSUBSCRIPT 2 end_FLOATSUBSCRIPTO ratios to the active fractions of the nine comets (including HMP) in the literature that have data for both values. We find no correlation. These findings imply that CO22{}_{2}start_FLOATSUBSCRIPT 2 end_FLOATSUBSCRIPT sublimation driving out icy grains is not the only factor influencing active fractions for cometary nuclei.","['Comets (280) —', 'Comae (271) —', 'Carbon dioxide (196) —', 'Comet volatiles (2162) —', 'Short period comets (1452)']","['mikayla.huffman@colorado.edu', 'mckayaj@appstate.edu', 'USA']"
"This study investigates the integration of assistive therapeutic robotics, wearable sensors, and spatial sensors within an intelligent environment tailored for dementia care. The feasibility study aims to assess the collective impact of these technologies in enhancing care giving by seamlessly integrating supportive technology in the background. The wearable sensors track physiological data, while spatial sensors monitor geo-spatial information, integrated into a system supporting residents without necessitating technical expertise. The designed space fosters various activities, including robot interactions, medication delivery, physical exercises like walking on a treadmill (Bruce protocol), entertainment, and household tasks, promoting cognitive stimulation through puzzles. Physiological data revealed significant participant engagement during robot interactions, indicating the potential effectiveness of robot-assisted activities in enhancing the quality of life for residents.","['Index', 'Terms: ', 'Humanoid', 'Robot,', 'Pepper robot, dementia friendly living space,', 'Alzheimer’s care giving,', 'Electrodermal activity,', 'EDA;', 'Physiological data;']","['0000-0001-8779-9617', 'rjd6099@psu.edu', 'mart5877@d.umn.edu', 'dowli026@d.umn.edu', 'rana.z.imtiaz@gmail.com']"
"The recognition of human activities based on WiFi Channel State Information (CSI) enables contactless and visual privacy-preserving sensing in indoor environments. However, poor model generalization, due to varying environmental conditions and sensing hardware, is a well-known problem in this space. To address this issue, in this work, data augmentation techniques commonly used in image-based learning are applied to WiFi CSI to investigate their effects on model generalization performance in cross-scenario and cross-system settings. In particular, we focus on the generalization between line-of-sight (LOS) and non-line-of-sight (NLOS) through-wall scenarios, as well as on the generalization between different antenna systems, which remains under-explored. We collect and make publicly available a dataset of CSI amplitude spectrograms of human activities. Utilizing this data, an ablation study is conducted in which activity recognition models based on the EfficientNetV2 architecture are trained, allowing us to assess the effects of each augmentation on model generalization performance. The gathered results show that specific combinations of simple data augmentation techniques applied to CSI amplitude data can significantly improve cross-scenario and cross-system generalization.","['Index', 'Terms: ', 'Data', 'Augmentation,', 'Model', 'Generalization,', 'Human', 'Activity', 'Recognition,', 'WiFi,', 'Channel', 'State', 'Information']",['martin.kampel}@tuwien.ac.at']
"More than one hundred tidal disruption events (TDEs) have been detected at multi-bands, which can be viewed as extreme laboratories to investigate the accretion physics and gravity in the immediate vicinity of massive black holes (MBHs). Future transient surveys are expected to detect several tens of thousands of TDEs, among which a small fraction may be strongly gravitationally lensed by intervening galaxies. In this paper, we statistically etsimate the detection rate of lensed TDEs, with dependence on the limiting magnitude of the transient all-sky surveys searching for them. We find that the requisite limiting magnitude for an all-sky transient survey to observe at least 1111 yr−11{}^{-1}start_FLOATSUPERSCRIPT - 1 end_FLOATSUPERSCRIPT is ≳21.3greater-than-or-equivalent-toabsent21.3\gtrsim 21.3≳ 21.3, 21.221.221.221.2, and 21.521.521.521.5 mag in the u-, g-, and z-bands, respectively. If the limiting magnitude of the all-sky survey can reach ∼25−26similar-toabsent2526\sim 25-26∼ 25 - 26 mag in the u-, g-, and z-bands, the detection rate can be upto about several tens to hundreds per year. The discovery and identification of the first image of the lensed TDE can be taken as an early-warning of the second and other subsequent images, which may enable detailed monitoring of the pre-peak photometry and spectroscopy evolution of the TDE. The additional early-stage information may help to constrain the dynamical and radiation processes involving in the TDEs.","['Accretion (14);', 'Gravitational lensing (670);', 'Supermassive black holes (1663);', 'Tidal disruption (1696);', 'Time domain astronomy(2019);', 'Transient sources (1851)']","['China;', 'China', 'China;', 'China', 'China', 'China;']"
"Programming problems can be solved in a multitude of functionally correct ways, but the quality of these solutions (e.g. readability, maintainability) can vary immensely.
When code quality is poor, symptoms emerge in the form of ‘code smells’, which are specific negative characteristics (e.g. duplicate code) that can be resolved by applying refactoring patterns.
Many undergraduate computing curricula train students on this software engineering practice, often doing so via exercises on unfamiliar instructor-provided code.
Our observation, however, is that this makes it harder for novices to internalise refactoring as part of their own development practices.
In this paper, we propose a new approach to teaching refactoring, in which students must first complete a programming exercise constrained to ensure they will produce a code smell.
This simple intervention is based on the idea that learning refactoring is easier if students are familiar with the code (having built it), that it brings refactoring closer to their regular development practice, and that it presents a powerful opportunity to learn from a ‘mistake’.
We designed and conducted a study with 35 novice undergraduates in which they completed various refactoring exercises alternately taught using a traditional and our ‘mistake-based’ approach, finding that students were significantly more effective and confident at completing exercises using the latter.","['Refactoring, code smells, code quality, software maintenance, software engineering, mistake-based learning, undergraduate course']","['UniversitySingapore', 'UniversitySingapore']"
"Suicide is recognized as one of the most serious concerns in the modern society. Suicide causes tragedy that affects countries, communities, and families. There are many factors that lead to suicidal ideations. Early detection of suicidal ideations can help to prevent suicide occurrence by providing the victim with the required professional support, especially when the victim does not recognize the danger of having suicidal ideations. As technology usage has increased, people share and express their ideations digitally via social media, chatbots, and other digital platforms. In this paper, we proposed a novel, simple deep learning-based model to detect suicidal ideations in digital content, mainly focusing on chatbots as the primary data source. In addition, we provide a framework that employs the proposed suicide detection integration with a chatbot-based support system.","['Index', 'Terms: ', 'Suicide, deep learning, chatbot, natural language processing, detection']","['elsayeny@ucmail.uc.edu', 'elsayezs@ucmail.uc.edu', 'ozermm@ucmail.uc.edu']"
"The correctness of a compiler affects the correctness of every program written in the language, and thus must be thoroughly evaluated. Existing automatic compiler testing methods however either rely on weak oracles (e.g., a program behaves the same if only dead code is modified), or require substantial initial effort (e.g., having a complete operational language semantics).
While the former prevents a comprehensive correctness evaluation, the latter makes those methods irrelevant in practice.
In this work, we propose an axiomatic semantics based approach for testing compilers, called PTE. The idea is to incrementally develop a set of “axioms” capturing anecdotes of the language semantics in the form of (precondition, transformation, expectation) triples, which allows us to test the compiler automatically. Such axioms are written in the same language whose compiler is under test, and can be developed either based on the language specification, or by generalizing the bug reports. PTE has been applied to a newly developed compiler (i.e., Cangjie) and a mature compiler (i.e., Java), and successfully identified 42 implementation bugs and 9 potential language design issues.","['Compiler testing, language semantics, automated testing']","['UniversitySingaporeSingapore', 'UniversitySingaporeSingapore', 'UniversitySingaporeSingapore', 'UniversityBeijingChina', 'UniversityShanghaiChina']"
"The full array of the Large High Altitude Air Shower Observatory (LHAASO) has been in operation since July 2021. For its kilometer-square array (KM2A), we have optimized the selection criteria for very high and ultra-high energy γ𝛾\gammaitalic_γ-rays, using the data collected from August 2021 to August 2022, resulting in an improvement on significance of about 15%percent\%% compared with previous cuts. With the implementation of these new selection criteria, the angular resolution is also significantly improved by approximately 10%percent\%% at tens of TeV. Other aspects of the full KM2A array performance, such as the pointing error are also calibrated using the Crab Nebula. The resulting energy spectrum of the Crab Nebula in the energy range of 10-1000 TeV can be well fitted by a log-parabola model, which is consistent with the previous results from LHAASO and other experiments.","['γ𝛾\\gammaitalic_γ-ray;', 'Crab', 'Nebula; significance.']","['China', 'China', 'China', 'Ireland', 'Germany', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'Switzerland', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'France', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'Switzerland', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'Russia', 'Russia', 'Russia', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'Thailand', 'China', 'China', 'China', 'France', 'China', 'China', 'Thailand', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'Thailand', 'Thailand', 'France', 'China', 'China', 'Russia', 'Russia', 'China', 'China', 'China', 'China', 'Russia', 'Russia', 'Russia', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China']"
"The recent development on large language models makes automatically constructing small programs possible. It thus has the potential to free software engineers from low-level coding and allow us to focus on the perhaps more interesting parts of software development, such as requirement engineering and system testing. In this project, we develop a prototype named AISD (AI-aided Software Development), which is capable of taking high-level (potentially vague) user requirements as inputs, generates detailed use cases, prototype system designs, and subsequently system implementation. Different from existing attempts, AISD is designed to keep the user in the loop, i.e., by repeatedly taking user feedback on use cases, high-level system designs, and prototype implementations through system testing. AISD has been evaluated with a novel benchmark of non-trivial software projects. The experimental results suggest that it might be possible to imagine a future where software engineering is reduced to requirement engineering and system testing only.","['Requirement engineering, system testing, large language model, code generation']","['UniversityShanghaiChina', 'UniversityShanghaiChina', 'UniversitySingaporeSingapore', 'UniversitySingaporeSingapore', 'UniversityShanghaiChina', 'UniversityShanghaiChina']"
"Studying the relations between entanglement and coherence is essential in many quantum information applications. For this, we consider the concurrence, intrinsic concurrence and first-order coherence, and evaluate the proposed trade-off relations between them. In particular, we study the temporal evolution of a general two-qubit XYZ Heisenberg model with asymmetric spin-orbit interaction under decoherence and analyze the trade-off relations of quantum resource theory.
For XYZ Heisenberg model, we confirm that the trade-off relation between intrinsic concurrence and first-order coherence holds. Furthermore, we show that the lower bound of intrinsic concurrence is universally valid, but the upper bound is generally not.
These relations in Heisenberg models can provide a way to explore how quantum resources are distributed in spins, which may inspire future applications in quantum information processing.","['Trade-off relations,', 'First-order coherence,', 'Intrinsic concurrence,', 'Heisenberg models']","['Qatar', 'Qatar', 'Iran']"
"In this paper, we present Coyote C++, a fully automated white-box unit testing tool for C and C++. Whereas existing tools have struggled to realize unit test generation for C++, Coyote C++ is able to produce high coverage results from unit test generation at a testing speed of over 10,000 statements per hour. This impressive feat is made possible by the combination of a powerful concolic execution engine with sophisticated automated test harness generation. Additionally, the GUI of Coyote C++ displays detailed code coverage visualizations and provides various configuration features for users seeking to manually optimize their coverage results. Combining potent one-click automated testing with rich support for manual tweaking, Coyote C++ is the first automated testing tool that is practical enough to make automated testing of C++ code truly viable in industrial applications.","['Index', 'Terms: ', 'Software', 'Testing,', 'Test case generation,', 'Automated unit test generation,', 'Symbolic execution,', 'C++']","['rho@codemind.co.kr', 'yeoneo@codemind.co.kr', 'philipp.m@codemind.co.kr', 'shin@codemind.co.kr']"
"Poetry generation has been a challenging task in the field of Natural Language Processing, as it requires the model to understand the nuances of language, sentiment, and style. In this paper, we propose using Large Language Models to generate Vietnamese poems from natural language prompts, thereby facilitating an intuitive process with enhanced content control. Our most efficacious model, the GPT-3 Babbage variant, achieves a custom evaluation score of 0.8, specifically tailored to the ”luc bat” genre of Vietnamese poetry. Furthermore, we also explore the idea of paraphrasing poems into normal text prompts and yield a relatively high score of 0.718 in the ”luc bat” genre. This experiment presents the potential for cross-Language poem-to-poem translation with translated poems as the inputs while concurrently maintaining complete control over the generated content.","['GPT-3, poem generation,', 'BLOOM,', 'Vietnamese, quantization,', 'LoRa']","['CityVietnam', 'CityVietnam']"
"Aircraft landing time (ALT) prediction is crucial for air traffic management, especially for arrival aircraft sequencing on the runway. In this study, a trajectory image-based deep learning method is proposed to predict ALTs for the aircraft entering the research airspace that covers the Terminal Maneuvering Area (TMA). Specifically, the trajectories of all airborne arrival aircraft within the temporal capture window are used to generate an image with the target aircraft trajectory labeled as red and all background aircraft trajectory labeled as blue. The trajectory images contain various information, including the aircraft position, speed, heading, relative distances, and arrival traffic flows. It enables us to use state-of-the-art deep convolution neural networks for ALT modeling. We also use real-time runway usage obtained from the trajectory data and the external information such as aircraft types and weather conditions as additional inputs. Moreover, a convolution neural network (CNN) based module is designed for automatic holding-related featurizing, which takes the trajectory images, the leading aircraft holding status, and their time and speed gap at the research airspace boundary as its inputs. Its output is further fed into the final end-to-end ALT prediction. The proposed ALT prediction approach is applied to Singapore Changi Airport (ICAO Code: WSSS) using one-month Automatic Dependent Surveillance-Broadcast (ADS-B) data from November 1 to November 30, 2022. Experimental results show that by integrating the holding featurization, we can reduce the mean absolute error (MAE) from 82.23 seconds to 43.96 seconds, and achieve an average accuracy of 96.1%, with 79.4% of the predictions errors being less than 60 seconds.","['Air', 'Traffic', 'Management,', 'Aircraft', 'Landing', 'Time,', 'Trajectory', 'Image,', 'Convolution', 'Neural', 'Networks.']",['Singapore']
"The Gaia DR3 parallax approach was used to estimate the absolute parameters of 2375 δ𝛿\deltaitalic_δ Scuti stars from the ASAS catalog. The selected stars have a variety of observational characteristics, with a higher than 80% probability of being δ𝛿\deltaitalic_δ Scuti stars. We have displayed all the stars in the Hertzsprung-Russell (H-R) diagram along with the δ𝛿\deltaitalic_δ Scuti instability strip, the Zero Age Main Sequence (ZAMS), and the Terminal-Age Main Sequence (TAMS). Then, we determined which fundamental and overtone modes each star belongs to using pulsation constant (Q𝑄Qitalic_Q) calculations. In addition, we evaluated the parameters in the Q𝑄Qitalic_Q calculation equation using three machine learning methods, which showed that surface gravity and temperature have the greatest effect on its calculation. The Period-Luminosity (P−L𝑃𝐿P-Litalic_P - italic_L) relationship of the δ𝛿\deltaitalic_δ Scuti stars was also revisited. Eventually, using least squares linear regression, we made four linear fits for fundamental and overtone modes and updated their relationships.","['δ𝛿\\deltaitalic_δ', 'Scuti variable stars -', 'Fundamental parameters -', 'Data analysis']","['poroatila@gmail.com', 'Canada', 'USA', 'Iran', 'Iran', 'Italy', 'Iran', 'Iran', 'Iran', 'Iran', 'Iran']"
"Joint Communication and Sensing (JCAS) is taking its first shape in WLAN sensing under IEEE 802.11bf, where standardized WLAN signals and protocols are exploited to enable radar-like sensing. However, an overlooked problem in JCAS, and specifically in WLAN Sensing, is the sensitivity of the system to a deceptive jammer, which introduces phantom targets to mislead the victim radar receiver. Standardized waveforms and sensing parameters make the system vulnerable to physical layer attacks. Moreover, orthogonal frequency-division multiplexing (OFDM) makes deceptive jamming even easier as it allows digitally generated artificial range/Doppler maps. This paper studies deceptive jamming in JCAS, with a special focus on WLAN Sensing. The provided mathematical models give insights into how to design jamming signals and their impact on the sensing system. Numerical analyses illustrate various distortions caused by deceptive jamming, while the experimental results validate the need for meticulous JCAS design to protect the system against physical layer attacks in the form of deceptive jamming.","['Index', 'Terms: ', 'Joint', 'Communication and', 'Sensing,', 'WLAN', 'Sensing, deceptive jamming, physical layer security,', 'OFDM radars.']","['Belgium', 'Sweden']"
"Rust relies on its unique ownership mechanism to ensure thread and memory safety. However, numerous potential security vulnerabilities persist in practical applications. New language features in Rust pose new challenges for vulnerability detection. This paper proposes a static deadlock detection method tailored for Rust programs, aiming to identify various deadlock types, including double lock, conflict lock, and deadlock associated with conditional variables. With due consideration for Rust’s ownership and lifetimes, we first complete the pointer analysis. Then, based on the obtained points-to information, we analyze dependencies among variables to identify potential deadlocks. We develop a tool and conduct experiments based on the proposed method. The experimental results demonstrate that our method outperforms existing deadlock detection methods in precision.","['Rust', 'Programs,', 'Static', 'Analysis,', 'Deadlock', 'Detection']","['UniversityShanghaiChina', 'UniversityShanghaiChina', 'UniversityShanghaiChina']"
"Tree-based models have been successfully applied to a wide variety of tasks, including time series forecasting.
They are increasingly in demand and widely accepted because of their comparatively high level of interpretability. However, many of them suffer from the overfitting problem, which limits their application in real-world decision-making. This problem becomes even more severe in online-forecasting settings where time series observations are incrementally acquired, and the
distributions from which they are drawn may keep changing over time. In this context, we propose a novel method for the online selection of tree-based models using the TreeSHAP explainability method in the task of time series forecasting. We start with an arbitrary set of different tree-based models. Then, we outline a performance-based ranking with a coherent design to make TreeSHAP able to specialize the tree-based forecasters across different regions in the input time series. In this framework, adequate model selection is performed online, adaptively following drift detection in the time series. In addition, explainability is supported on three levels, namely online input importance, model selection, and model output explanation. An extensive empirical study on various real-world datasets demonstrates that our method achieves excellent or on-par results in comparison to the state-of-the-art approaches as well as several baselines.","['Index', 'Terms: ', 'Online', 'Model', 'Selection,', 'Tree-based', 'Models,', 'Time', 'Series', 'Forecasting,', 'TreeSHAP,', 'Explainability']","['matthias.jakobs@tu-dortmund.de', 'amal.saadallah@cs.tu-dortmund.de']"
"Deforestation, a major contributor to climate change, poses detrimental consequences such as agricultural sector disruption, global warming, flash floods, and landslides. Conventional approaches to urban street tree inventory suffer from inaccuracies and necessitate specialised equipment. To overcome these challenges, this paper proposes an innovative method that leverages deep learning techniques and mobile phone imaging for urban street tree inventory. Our approach utilises a pair of images captured by smartphone cameras to accurately segment tree trunks and compute the diameter at breast height (DBH). Compared to traditional methods, our approach exhibits several advantages, including superior accuracy, reduced dependency on specialised equipment, and applicability in hard-to-reach areas. We evaluated our method on a comprehensive dataset of 400 trees and achieved a DBH estimation accuracy with an error rate of less than 2.5%. Our method holds significant potential for substantially improving forest management practices. By enhancing the accuracy and efficiency of tree inventory, our model empowers urban management to mitigate the adverse effects of deforestation and climate change.","['Index', 'Terms: ', 'Urban deforestation, street trees inventory, deep learning, mobile phones,', 'DBH,', 'ABG,', 'CNN']","['asim.khan@ku.ac.ae', '1601005@namal.edu.pk', 'aulhaq@csu.edu.au', 'iqbal.gondal@rmit.edu.au', 'sajid.javed@ku.ac.ae']"
"The current approach to fetal anomaly screening is based on biometric
measurements derived from individually selected ultrasound images.
In this paper, we introduce a paradigm shift that attains human-level
performance in biometric measurement by aggregating automatically
extracted biometrics from every frame across an entire scan, with
no need for operator intervention. We use a convolutional neural network
to classify each frame of an ultrasound video recording. We then measure
fetal biometrics in every frame where appropriate anatomy is visible.
We use a Bayesian method to estimate the true value of each biometric
from a large number of measurements and probabilistically reject outliers.
We performed a retrospective experiment on 1457 recordings (comprising
48 million frames) of 20-week ultrasound scans, estimated fetal biometrics
in those scans and compared our estimates to the measurements sonographers
took during the scan. Our method achieves human-level performance
in estimating fetal biometrics and estimates well-calibrated credible
intervals in which the true biometric value is expected to lie.","['Index', 'Terms: \n', 'Ultrasound;', 'Fetal imaging;', 'Machine learning;', 'Bayesian estimation;', 'Biometric measurement']","['London', 'London', 'London']"
"The NASA New Horizons Venetia Burney Student Dust Counter (SDC) measures dust particle impacts along the spacecraft’s flight path for grains with mass ≥\geq≥ 10−12superscript101210^{-12}10 start_POSTSUPERSCRIPT - 12 end_POSTSUPERSCRIPT g, mapping out their spatial density distribution. We present the latest SDC dust density, size distribution, and flux measurements through 55 au and compare them to numerical model predictions. Kuiper Belt Objects (KBOs) are thought to be the dominant source of interplanetary dust particles (IDP) in the outer solar system due to both collisions between KBOs, and their continual bombardment by interstellar dust particles (ISD).
Continued measurements through 55 au show higher than model-predicted dust fluxes as New Horizons approaches the putative outer edge of the Kuiper Belt (KB). We discuss potential explanations for the growing deviation: radiation pressure stretches the dust distribution to further heliocentric distances than its parent body distribution; icy dust grains undergo photo-sputtering that rapidly increases their response to radiation pressure forces and pushes them further away from the sun; and the distribution of KBOs may extend much further than existing observations suggest. Ongoing SDC measurements at even larger heliocentric distances will continue to constrain the contributions of dust production in the KB. Continued SDC measurements remain crucial for understanding the Kuiper Belt and the interpretation of observations of dust disks around other stars.","['Kuiper', 'Belt,', 'Interplanetary', 'Dust,', 'PVDF,', 'New', 'Horizons']","['USA', 'USA', 'USA', 'USA', 'USA', 'USA', 'USA', 'USA', 'USA', 'USA', 'USA', 'USA', 'USA']"
"Graph Neural Networks (GNNs) are widely applied across various domains, yet they perform poorly in deep layers. Existing research typically attributes this problem to node over-smoothing, where node representations become indistinguishable after multiple rounds of propagation. In this paper, we delve into the neighborhood propagation mechanism of GNNs and discover that the real root cause of GNNs’ performance degradation in deep layers lies in ineffective neighborhood feature propagation. This propagation leads to an exponential growth of a node’s current representation at every propagation step, making it extremely challenging to capture valuable dependencies between long-distance nodes. To address this issue, we introduce Graph Elimination Networks (GENs), which employ a specific algorithm to eliminate redundancies during neighborhood propagation. We demonstrate that GENs can enhance nodes’ perception of distant neighborhoods and extend the depth of network propagation. Extensive experiments show that GENs outperform the state-of-the-art methods on various graph-level and node-level datasets.","['Graph', 'Neural', 'Network,', 'Deep', 'Graph', 'Neural', 'Network,', 'Graph', 'Self-Attention,', 'Subgraph', 'Self-Attention,', 'Algorithm', 'Framework']","['UniversityXiangtanChina411105', 'UniversityXiangtanChina411105', 'UniversityHunanChina410082']"
"From politicians to podcast hosts, online platforms have systematically banned (“deplatformed”) influential users for breaking platform guidelines.
Previous inquiries on the effectiveness of this intervention are inconclusive because
1) they consider only few deplatforming events;
2) they consider only overt engagement traces (e.g., likes and posts) but not passive engagement (e.g., views);
3) they do not consider all the potential places users impacted by the deplatforming event might migrate to.
We address these limitations in a longitudinal, quasi-experimental study of 165 deplatforming events targeted at 101 influencers.
We collect deplatforming events from Reddit posts and then manually curate the data, ensuring the correctness of a large dataset of deplatforming events.
Then, we link these events to Google Trends and Wikipedia page views, platform-agnostic measures of online attention that capture the general public’s interest in specific influencers.
Through a difference-in-differences approach, we find that deplatforming reduces online attention toward influencers.
After 12 months, we estimate that online attention toward deplatformed influencers is reduced by
−--63% (95% CI [−--75%,−--46%]) on Google and by
−--43% (95% CI [−--57%,−--24%]) on Wikipedia.
Further, as we study over a hundred deplatforming events, we can analyze in which cases deplatforming is more or less impactful, revealing nuances about the intervention.
Notably, we find that both permanent and temporary deplatforming reduce online attention toward influencers;
Overall, this work contributes to the ongoing effort to map the effectiveness of content moderation interventions, driving platform governance away from speculation.",['online communities; fringe online communities; content moderation; online radicalization; deplatforming; social networks'],"['EPFLSwitzerland', 'UniversityUSA', 'EPFLSwitzerland', 'EPFLSwitzerland', 'EPFLSwitzerland']"
"This paper documents a year-long experiment to “profile” the process of learning a programming language: gathering data to understand what makes a language hard to learn, and using that data to improve the learning process. We added interactive quizzes to The Rust Programming Language, the official textbook for learning Rust. Over 13 months, 62,526 readers answered questions 1,140,202 times. First, we analyze the trajectories of readers. We find that many readers drop-out of the book early when faced with difficult language concepts like Rust’s ownership types. Second, we use classical test theory and item response theory to analyze the characteristics of quiz questions. We find that better questions are more conceptual in nature, such as asking why a program does not compile vs. whether a program compiles. Third, we performed 12 interventions into the book to help readers with difficult questions. We find that on average, interventions improved quiz scores on the targeted questions by +20%. Fourth, we show that our technique can likely generalize to languages with smaller user bases by simulating our statistical inferences on small N𝑁Nitalic_N. These results demonstrate that quizzes are a simple and useful technique for understanding language learning at all scales.","['rust education, digital textbooks, item response theory']",['Island02912USA']
"At time zero, there are N𝑁Nitalic_N identical point particles in the line (1D) which are characterized by their positions and velocities. Both values are given randomly and independently from each other, with arbitrary probability densities. Each particle evolves at constant velocity until eventually they meet. When this happens, a perfectly-plastic collision is produced, resulting in a new particle composed by the sum of their masses and the weighted average velocity.
The merged particles evolve indistinguishably from the non-merged ones, i.e. they move at constant velocity until a new plastic collision eventually happens.
As in any open system, the particles are not confined to any region or reservoir, so as time progresses, they go on to infinity.
From this non-equilibrium process, the number of (now, non-identical) final particles, X~Nsubscript~𝑋𝑁\tilde{X}_{N}over~ start_ARG italic_X end_ARG start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT, the distribution of masses of these final particles and the kinetic energy loss from all plastic collisions, is studied. Counterintuitively, the way to achieve the number of final particles and each of their masses does not need to rely on evolving the particle system; this result can be obtained by simply considering the initial conditions. Moreover, they can also be used to obtain an accurate approximation of the energy loss. Finally, I will also present strong evidence for the validity of the following conjecture: ⟨X~N⟩=∑k=1𝑁⁢1kdelimited-⟨⟩subscript~𝑋𝑁𝑁𝑘11𝑘\langle\tilde{X}_{N}\rangle=\overset{N}{\underset{k=1}{\sum}}\frac{1}{k}⟨ over~ start_ARG italic_X end_ARG start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT ⟩ = overitalic_N start_ARG start_UNDERACCENT italic_k = 1 end_UNDERACCENT start_ARG ∑ end_ARG end_ARG divide start_ARG 1 end_ARG start_ARG italic_k end_ARG (which behaves as l⁢o⁢g⁢(N)+γ𝑙𝑜𝑔𝑁𝛾log(N)+\gammaitalic_l italic_o italic_g ( italic_N ) + italic_γ for large N𝑁Nitalic_N), additionally an explicit expression for the variance will also be given.","['Many-body dynamics, out-of-equilibrium systems, 1D system']","['Argentina,', 'Argentina.']"
"Given the potential for technology to inflict harm and injustice on society, it is imperative that we cultivate a sense of social responsibility among our students as they progress through the Computer Science (CS) curriculum.
Our students need to be able to examine the social complexities in which technology development and use are situated.
Also, aligning students’ personal goals and their ability to achieve them in their field of study is important for promoting motivation and a sense of belonging.
Promoting communal goals while learning computing can help broaden participation, particularly among groups who have been historically marginalized in computing.
Keeping these considerations in mind, we piloted an introductory Java programming course in which activities engaging students in ethical and socially responsible considerations were integrated across modules.
Rather than adding social on top of the technical content, our curricular approach seeks to weave them together.
The data from the class suggests that the students found the inclusion of the social context in the technical assignments to be more motivating and expressed greater agency in realizing social change.
We share our approach to designing this new introductory socially responsible computing course and the students’ reflections.
We also highlight seven considerations for educators seeking to incorporate socially responsible computing.","['SRC, responsibility, social impact, ethics, power, critical pedagogy']","['USA', 'USA', 'USA', 'USA', 'USA']"
"Channel modeling is fundamental in advancing wireless systems and has thus attracted considerable research focus. Recent trends have seen a growing reliance on data-driven techniques to facilitate the modeling process and yield accurate channel predictions. In this work, we first provide a concise overview of data-driven channel modeling methods, highlighting their limitations. Subsequently, we introduce the concept and advantages of physics-informed neural network (PINN)-based modeling and a summary of recent contributions in this area. Our findings demonstrate that PINN-based approaches in channel modeling exhibit promising attributes such as generalizability, interpretability, and robustness. We offer a comprehensive architecture for PINN methodology, designed to inform and inspire future model development. A case-study of our recent work on precise indoor channel prediction with semantic segmentation and deep learning is presented.
The study concludes by addressing the challenges faced and suggesting potential research directions in this field.","['Index', 'Terms: \nwireless channel modeling, physics-based modeling, deep learning, 3D segmentation, knowledge distillation.']",['3mingyue.ji@utah.edu']
"Grammatical inference consists in learning a language or a grammar from data. In this paper, we consider a number of models for inferring a non-deterministic finite automaton (NFA) with 3 sorts of states, that must accept some words, and reject some other words from a given sample. We then propose a transformation from this 3-sort NFA into weighted-frequency and probabilistic NFA, and we apply the latter to a classification task. The experimental evaluation of our approach shows that the probabilistic NFAs can be successfully applied for classification tasks on both real-life and superficial benchmark data sets.","['grammatical inference  and nondeterministic automata  and', 'SAT models']","['TechnologyGliwicePoland', 'AngersAngersFrance', 'AngersAngersFrance']"
"If the extraction of sensor fingerprints represents nowadays an important forensic tool for sensor attribution, it has been shown recently in [1, 2, 3] that images coming from several sensors were more prone to generate False Positives (FP) by presenting a common ”leak”. In this paper, we investigate the possible cause of this leak and after inspecting the EXIF metadata of the sources causing FP, we found out that they were related to the Adobe Lightroom or Photoshop softwares. The cross-correlation between residuals on images presenting FP reveals periodic peaks showing the presence of a periodic pattern. By developing our own images with Adobe Lightroom we are able to show that all developments from raw images (or 16 bits per channel coded) to 8 bits-coded images also embed a periodic 128×128128128128\times 128128 × 128 pattern very similar to a watermark. However, we also show that the watermark depends on both the content and the architecture used to develop the image. The rest of the paper presents two different ways of removing this watermark, one by removing it from the image noise component, and the other by removing it in the pixel domain. We show that for a camera presenting FP in [3], we were able to prevent the False Positives. A discussion with Adobe representatives informed us that the company decided to add this pattern in order to induce dithering.","['Index', 'Terms: ', 'PRNU,', 'False-Positive,', 'Watermarking,', 'Watermark', 'Removal']","['jan.butora@cnrs.fr', 'patrick.bas@cnrs.fr']"
"Recommender systems aim to recommend the most suitable items to users from a large number of candidates. Their computation cost grows as the number of user requests and the complexity of services (or models) increases.
Under the limitation of computation resources (CRs), how to make a trade-off between computation cost and business revenue becomes an essential question.
The existing studies focus on dynamically allocating CRs in queue truncation scenarios (i.e., allocating the size of candidates), and formulate the CR allocation problem as an optimization problem with constraints. Some of them focus on single-phase CR allocation, and others focus on multi-phase CR allocation but introduce some assumptions about queue truncation scenarios. However, these assumptions do not hold in other scenarios, such as retrieval channel selection and prediction model selection. Moreover, existing studies ignore the state transition process of requests between different phases, limiting the effectiveness of their approaches.
This paper proposes a Reinforcement Learning (RL) based Multi-Phase Computation Allocation approach (RL-MPCA), which aims to maximize the total business revenue under the limitation of CRs. RL-MPCA formulates the CR allocation problem as a Weakly Coupled MDP problem and solves it with an RL-based approach. Specifically, RL-MPCA designs a novel deep Q-network to adapt to various CR allocation scenarios, and calibrates the Q-value by introducing multiple adaptive Lagrange multipliers (adaptive-λ𝜆\lambdaitalic_λ) to avoid violating the global CR constraints.
Finally, experiments on the offline simulation environment and online real-world recommender system validate the effectiveness of our approach.","['Computation', 'Resource', 'Allocation,', 'Deep', 'Reinforcement', 'Learning,', 'Recommender', 'System,', 'Weakly', 'Coupled', 'MDP']","['China', 'China', 'China', 'China', 'China', 'China', 'China', 'China']"
"WiFi Channel State Information (CSI)-based human activity recognition (HAR) enables contactless, long-range sensing in spatially constrained environments while preserving visual privacy. However, despite the presence of numerous WiFi-enabled devices around us, few expose CSI to users, resulting in a lack of sensing hardware options. Variants of the Espressif ESP32 have emerged as potential low-cost and easy-to-deploy solutions for WiFi CSI-based HAR. In this work, four ESP32-S3-based 2.4GHz directional antenna systems are evaluated for their ability to facilitate long-range through-wall HAR. Two promising systems are proposed, one of which combines the ESP32-S3 with a directional biquad antenna. This combination represents, to the best of our knowledge, the first demonstration of such a system in WiFi-based HAR. The second system relies on the built-in printed inverted-F antenna (PIFA) of the ESP32-S3 and achieves directionality through a plane reflector. In a comprehensive evaluation of line-of-sight (LOS) and non-line-of-sight (NLOS) HAR performance, both systems are deployed in an office environment spanning a distance of 18 meters across five rooms. In this experimental setup, the Wallhack1.8k dataset, comprising 1806 CSI amplitude spectrograms of human activities, is collected and made publicly available. Based on Wallhack1.8k, we train activity recognition models using the EfficientNetV2 architecture to assess system performance in LOS and NLOS scenarios. For the core NLOS activity recognition problem, the biquad antenna and PIFA-based systems achieve accuracies of 92.0±plus-or-minus\pm±3.5 and 86.8±plus-or-minus\pm±4.7, respectively, demonstrating the feasibility of long-range through-wall HAR with the proposed systems.","['Index', 'Terms: ', 'Human', 'Activity', 'Recognition,', 'WiFi,', 'Channel', 'State', 'Information,', 'Through-Wall', 'Sensing,', 'ESP32']",['martin.kampel}@tuwien.ac.at']
"Most tidal disruption events (TDEs) are currently found in time-domain optical and soft X-ray surveys, both of which are prone to significant obscuration. The infrared (IR), however, is a powerful probe of dust-enshrouded environments, and hence, we recently performed a systematic search of NEOWISE mid-IR data for nearby, obscured TDEs within roughly 200 Mpc. We identified 18 TDE candidates in galactic nuclei, using difference imaging to uncover nuclear variability amongst significant host galaxy emission. These candidates were selected based on the following IR light curve properties: (1) LW2≳1042greater-than-or-equivalent-tosubscript𝐿W2superscript1042L_{\mathrm{W2}}\gtrsim 10^{42}italic_L start_POSTSUBSCRIPT W2 end_POSTSUBSCRIPT ≳ 10 start_POSTSUPERSCRIPT 42 end_POSTSUPERSCRIPT erg s−11{}^{-1}start_FLOATSUPERSCRIPT - 1 end_FLOATSUPERSCRIPT at peak, (2) fast rise, followed by a slow, monotonic decline, (3) no significant prior variability, and (4) no evidence for AGN activity in WISE colors. The majority of these sources showed no variable optical counterpart, suggesting that optical surveys indeed miss numerous obscured TDEs. Using narrow line ionization levels and variability arguments, we identified 6 sources as possible underlying AGN, yielding a total of 12 TDEs in our gold sample. This gold sample yields a lower limit on the IR-selected TDE rate of 2.0±0.3×10−5plus-or-minus2.00.3superscript1052.0\pm 0.3\times 10^{-5}2.0 ± 0.3 × 10 start_POSTSUPERSCRIPT - 5 end_POSTSUPERSCRIPT galaxy−11{}^{-1}start_FLOATSUPERSCRIPT - 1 end_FLOATSUPERSCRIPT year−11{}^{-1}start_FLOATSUPERSCRIPT - 1 end_FLOATSUPERSCRIPT (1.3±0.2×10−7plus-or-minus1.30.2superscript1071.3\pm 0.2\times 10^{-7}1.3 ± 0.2 × 10 start_POSTSUPERSCRIPT - 7 end_POSTSUPERSCRIPT Mpc−33{}^{-3}start_FLOATSUPERSCRIPT - 3 end_FLOATSUPERSCRIPT year−11{}^{-1}start_FLOATSUPERSCRIPT - 1 end_FLOATSUPERSCRIPT), which is comparable to optical and X-ray TDE rates. The IR-selected TDE host galaxies do not show a green valley overdensity nor a preference for quiescent, Balmer strong galaxies, which are both overrepresented in optical and X-ray TDE samples. This IR-selected sample represents a new population of dusty TDEs that have historically been missed by optical and X-ray surveys and helps alleviate tensions between observed and theoretical TDE rates and the so-called missing energy problem.","['Accretion (14);', 'Supermassive black holes (1663);', 'Tidal disruption (1696);', 'Transient sources (1851);', 'Time domain astronomy (2109)']","['USA', 'Fellow', 'USA', 'USA', 'USA', 'Israel', 'USA', 'USA', 'USA', 'USA', 'Germany', 'Germany', 'Germany', 'USA', 'Germany', 'USA', 'USA', 'Germany', 'USA', 'Netherlands']"
"Multi-cloud systems facilitate a cost-efficient and geographically-distributed deployment of microservice-based applications by temporary leasing virtual nodes with diverse pricing models. To preserve the cost-efficiency of multi-cloud deployments, it is essential to redeploy microservices onto the available nodes according to a dynamic resource configuration, which is often performed to better accommodate workload variations.
However, this approach leads to frequent service disruption since applications are continuously shutdown and redeployed in order to apply the new resource assignment. To overcome this issue, we propose a re-orchestration scheme that migrates microservice at runtime based on a rolling update scheduling logic. Specifically, we propose an integer linear optimization problem that minimizes the cost associated to multi-cloud virtual nodes and that ensures that delay-sensitive microservices are co-located on the same regional cluster. The resulting rescheduling order guarantees no service disruption by repacking microservices between the available nodes without the need to turn off the outdated microservice instance before redeploying the updated version. In addition, we propose a two-step heuristic scheme that effectively approximates the optimal solution at the expense of close-to-zero service disruption and QoS violation probability. Results show that proposed schemes achieve better performance in terms of cost mitigation, low service disruption and low QoS violation probability compared to baseline schemes replicating Kubernetes scheduler functionalities.","['Index', 'Terms: ', 'Microservice re-orchestration, cost minimization, resource allocation, multi-cloud systems, optimization']",['dsiracusa@fbk.eu']
"When implementing hierarchical federated learning over wireless networks, scalability assurance and the ability to handle both interference and device data heterogeneity are crucial. This work introduces a learning method designed to address these challenges, along with
a scalable transmission scheme that efficiently uses a single wireless resource
through over-the-air computation. To provide resistance against data heterogeneity, we employ gradient aggregations. Meanwhile, the impact of interference is minimized through optimized receiver normalizing factors. For this, we model a multi-cluster wireless network using stochastic geometry, and characterize the mean squared error of the aggregation estimations as a function of the network parameters.
We show that despite the interference and the data heterogeneity, the proposed scheme achieves high learning accuracy and can significantly outperform the conventional hierarchical algorithm.","['Index', 'Terms: ', 'Federated learning, hierarchical networks, over-the-air computation, interference, stochastic geometry.']","['{seyaa,vjfodor}@kth.se']"
"We present a comparative study of the molecular gas in two galaxies from the LEGUS sample: barred spiral NGC 1313 and flocculent spiral NGC 7793. These two galaxies have similar masses, metallicities, and star formation rates, but NGC 1313 is forming significantly more massive star clusters than NGC 7793, especially young massive clusters (<10absent10<10< 10 Myr, >104absentsuperscript104>10^{4}> 10 start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT M⊙direct-product{}_{\odot}start_FLOATSUBSCRIPT ⊙ end_FLOATSUBSCRIPT).
Using ALMA CO(2-1) observations of the two galaxies with the same sensitivities and resolutions of 13 pc, we directly compare the molecular gas in these two similar galaxies to determine the physical conditions responsible for their large disparity in cluster formation.
By fitting size-linewidth relations for the clouds in each galaxy, we find that NGC 1313 has a higher intercept than NGC 7793, implying that its clouds have higher kinetic energies at a given size scale. NGC 1313 also has more clouds near virial equilibrium than NGC 7793, which may be connected to its higher rate of massive cluster formation. However, these virially bound clouds do not show a stronger correlation with young clusters than that of the general cloud population. 
We find surprisingly small differences between the distributions of molecular cloud populations in the two galaxies, though the largest of those differences are that NGC 1313 has higher surface densities and lower free-fall times.","['star formation;', 'ALMA; spiral galaxies']","['USA', 'USA', 'USA', 'USA', 'USA', 'Sweden', 'USA', 'Researcher', 'USA', '82071', 'UK', 'USA', 'USA', 'USA', 'Italy', 'Italy', '53706', '55105', 'Australia', 'Australia', 'USA', 'Germany', 'USA', 'USA', 'Australia', 'Australia', 'USA', 'Switzerland', 'Sweden', 'USA', 'USA', 'USA', 'USA', 'USA', 'México']"
"We compare the molecular cloud properties in sub-galactic regions of two galaxies, barred spiral NGC 1313, which is forming many massive clusters, and flocculent spiral NGC 7793, which is forming significantly fewer massive clusters despite having a similar star formation rate to NGC 1313.
We find that there are larger variations in cloud properties between different regions within each galaxy than there are between the galaxies on a global scale, especially for NGC 1313.
There are higher masses, linewidths, pressures, and virial parameters in the arms of NGC 1313 and center of NGC 7793 than in the interarm and outer regions of the galaxies.
The massive cluster formation of NGC 1313 may be driven by its greater variation in environments, allowing more clouds with the necessary conditions to arise, although no one parameter seems primarily responsible for the difference in star formation.
Meanwhile NGC 7793 has clouds that are as massive and have as much kinetic energy as clouds in the arms of NGC 1313, but have densities and pressures more similar to the interarm regions and so are less inclined to collapse and form stars.
The cloud properties in NGC 1313 and NGC 7793 suggest that spiral arms, bars, interarm regions, and flocculent spirals each represent distinct environments with regard to molecular cloud populations.
We see surprisingly little difference in surface densities between the regions, suggesting that the differences in surface densities frequently seen between arm and interarm regions of lower-resolution studies are indicative of the sparsity of molecular clouds, rather than differences in their true surface density.","['star formation;', 'ALMA; spiral galaxies']","['USA', 'USA', 'USA', 'USA', 'USA', 'Sweden', 'USA', 'Researcher', 'USA', '82071', 'UK', 'USA', 'USA', 'USA', 'Italy', 'Italy', '53706', '55105', 'Australia', 'Australia', 'USA', 'Germany', 'USA', 'USA', 'Australia', 'Australia', 'USA', 'Switzerland', 'Sweden', 'USA', 'USA', 'USA', 'USA', 'USA', 'México']"
"DSLR cameras can achieve multiple zoom levels via shifting lens distances or swapping lens types.
However, these techniques are not possible on smartphone devices due to space constraints.
Most smartphone manufacturers adopt a hybrid zoom system: commonly a Wide (W) camera at a low zoom level and a Telephoto (T) camera at a high zoom level.
To simulate zoom levels between W and T, these systems crop and digitally upsample images from W, leading to significant detail loss.
In this paper, we propose an efficient system for hybrid zoom super-resolution on mobile devices, which captures a synchronous pair of W and T shots and leverages machine learning models to align and transfer details from T to W.
We further develop an adaptive blending method that accounts for depth-of-field mismatches, scene occlusion, flow uncertainty, and alignment errors.
To minimize the domain gap, we design a dual-phone camera rig to capture real-world inputs and ground-truths for supervised training.
Our method generates a 12-megapixel image in 500ms on a mobile platform and compares favorably against state-of-the-art methods under extensive evaluation on real-world scenarios.","['hybrid zoom, dual camera fusion, deep neural networks']",['GoogleUSA']
"We present a comprehensive analysis of the Hubble Space Telescope
observations of the atmosphere of WASP-121 b, a ultra-hot Jupiter.
After reducing the transit, eclipse, and phase-curve observations
with a uniform methodology and addressing the biases from
instrument systematics, sophisticated atmospheric retrievals are
used to extract robust constraints on the thermal structure,
chemistry, and cloud properties of the atmosphere.
Our analysis shows that the observations are consistent with a
strong thermal inversion beginning at ∼similar-to\sim∼104superscript10410^{4}10 start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT Pa on the
dayside, solar to subsolar metallicity Z𝑍Zitalic_Z
(i.e., −0.77<log⁡(Z)<0.050.77𝑍0.05-0.77<\log(Z)<0.05- 0.77 < roman_log ( italic_Z ) < 0.05), and super-solar C/O ratio
(i.e., 0.59<C/O<0.870.59C/O0.870.59<\textrm{C/O}<0.870.59 < C/O < 0.87).
More importantly, utilizing the high signal-to-noise ratio and
repeated observations of the planet, we identify the following
unambiguous time-varying signals in the data: i) a shift of
the putative hotspot offset between the two phase-curves and
ii) varying spectral signatures in the transits and eclipses.
By simulating the global dynamics of WASP-121 b
atmosphere at high-resolution, we show that the identified signals
are consistent with quasi-periodic weather patterns, hence
atmospheric variability, with signatures at the level probed by
the observations (∼similar-to\sim∼5% to ∼similar-to\sim∼10%) that change on a
timescale of ∼similar-to\sim∼5 planet days; in the simulations, the
weather patterns arise from the formation and movement of storms
and fronts, causing hot (as well as cold) patches of atmosphere
to deform, separate, and mix in time.","['Exoplanet atmospheric variability (2020),', 'Exoplanet atmospheric composition (2021),', 'Bayesian statistics (1900),', 'Astrophysical fluid dynamics (101),', 'Astronomy data analysis (1858)']","['work.', 'USA.', 'Kingdom', 'work.', 'USA', 'USA', 'USA', 'USA', 'USA', 'USA', 'Kingdom', 'Kingdom', 'France', 'Netherlands', 'Kingdom', 'Germany', 'UK', 'Sweden', 'Spain', 'USA', 'Kingdom', 'France', 'Kingdom']"
"We report the observations of two self-lensing pulses from KIC 12254688 in Transiting Exoplanet Survey Satellite (TESS) light curves. This system, containing a F2V star and white-dwarf companion, was amongst the first self-lensing binary systems discovered by the Kepler Space Telescope over the past decade. Each observed pulse occurs when the white dwarf transits in front of its companion star, gravitationally lensing the star’s surface, thus making it appear brighter to a distant observer. These two pulses are the very first self-lensing events discovered in TESS observations. We describe the methods by which the data were acquired and detrended, as well as the best-fit binary parameters deduced from our self-lensing+radial velocity model. We highlight the difficulties of finding new self-lensing systems with TESS, and we discuss the types of self-lensing systems that TESS may be more likely to discover in the future.","['Compact binary stars (283),', 'Gravitational microlensing (672),', 'White', 'Dwarf', 'Stars (1799)']","['01854', '01854', '01854', '01854', '01854', '01854', '01854']"
"PSR J1012+5307 is a millisecond pulsar with an extremely low-mass (ELM) white dwarf (WD) companion in an orbit of 14.5 hours. Magnetic braking (MB) plays an important role in influencing the orbital evolution of binary systems with a low-mass (<1−2⁢M⊙absent12subscript𝑀direct-product<1-2~{}M_{\odot}< 1 - 2 italic_M start_POSTSUBSCRIPT ⊙ end_POSTSUBSCRIPT) donor star. At present, there exist several different MB descriptions. In this paper, we investigate the formation of PSR J1012+5307 as a probe to test the plausible MB model. Employing a detailed stellar evolution model by the MESA code, we find that the Convection And Rotation Boosted MB and the ’Intermediate’ MB models can reproduce the WD mass, WD radius, WD surface gravity, neutron-star mass, and orbital period observed in PSR J1012+5307. However, our simulated WD has higher effective temperature than the observation. Other three MB mechanisms including the standard MB model are too weak to account for the observed orbital period in a Hubble time. A long cooling timescale caused by H-shell flashes of the WD may alleviate the discrepancy between the simulated effective temperature and the observed value.","['Stars: evolution:', 'Magnetic braking –', 'Stars:', 'White dwarfs –Binaries: general –', 'Pulsars:', 'PSR', 'J1012+5307']","['chenwc@pku.edu.cn', 'chenwc@pku.edu.cn', 'China', 'chenwc@pku.edu.cn', 'China', 'chenwc@pku.edu.cn', 'China']"
"In terms of human-computer interaction, it is becoming more and more important to correctly understand the user’s emotional state in a conversation, so the task of multimodal emotion recognition (MER) started to receive more attention. However, existing emotion classification methods usually perform classification only once. Sentences are likely to be misclassified in a single round of classification. Previous work usually ignores the similarities and differences between different morphological features in the fusion process. To address the above issues, we propose a two-stage emotion recognition model based on graph contrastive learning (TS-GCL). First, we encode the original dataset with different preprocessing modalities. Second, a graph contrastive learning (GCL) strategy is introduced for these three modal data with other structures to learn similarities and differences within and between modalities. Finally, we use MLP twice to achieve the final emotion classification. This staged classification method can help the model to better focus on different levels of emotional information, thereby improving the performance of the model. Extensive experiments show that TS-GCL has superior performance on IEMOCAP and MELD datasets compared with previous methods.","['Index', 'Terms: \ngraph contrastive learning, graph convolutional network, multimodal emotion recognition, two-stage classification']","['aiwei@hnu.edu.cn', 'fuchen.zhang@csuft.edu.cn', 'mengtao@hnu.edu.cn', 'yuntaoshou@csuft.edu.cn', 'hongen.shao@csuft.edu.cn', 'lik@newpaltz.edu']"
"Sequential recommenders are crucial to the success of online applications, e.g., e-commerce, video streaming, and social media. While model architectures continue to improve, for every new application domain, we still have to train a new model from scratch for high quality recommendations. On the other hand, pre-trained language and vision models have shown great success in zero-shot or few-shot adaptation to new application domains. Inspired by the success of pre-trained models in peer AI fields, we propose a novel pre-trained sequential recommendation framework: PrepRec. We learn universal item representations by modeling item popularity dynamics. Through extensive experiments on five real-world datasets, we show that PrepRec, without any auxiliary information, can not only zero-shot transfer to a new domain, but achieve competitive performance compared to state-of-the-art sequential recommender models with only a fraction of the model size. In addition, with a simple post-hoc interpolation, PrepRec can improve the performance of existing sequential recommenders on average by 13.8% in Recall@10 and 29.5% in NDCG@10. We provide an anonymized implementation of PrepRec at https://anonymous.4open.science/r/PrepRec--2F60/.","['Recommender', 'System,', 'Neural', 'Collaborative', 'Filtering,', 'Sequential', 'Recommendation,', 'Zero-shot', 'Sequential', 'Recommendation']","['AveUrbanaIllinois61801', 'AveUrbanaIllinois61801', 'AveUrbanaIllinois61801']"
"Specific emitter identification (SEI) technology is significant in device administration scenarios, such as self-organized networking and spectrum management, owing to its high security.
For nonlinear and non-stationary electromagnetic signals, SEI often employs variational modal decomposition (VMD) to decompose the signal in order to effectively characterize the distinct device fingerprint.
However, the trade-off of VMD between the robustness to noise and the ability to preserve signal information has not been investigated in the current literature.
Moreover, the existing VMD algorithm does not utilize the stability of the intrinsic distortion of emitters within a certain
temporal span, consequently constraining its practical applicability in SEI.
In this paper, we propose a joint variational modal decomposition (JVMD) algorithm, which is an improved version of VMD by simultaneously implementing modal decomposition on multi-frame signals.
The consistency of multi-frame signals in terms of the central frequencies and the inherent modal functions (IMFs) is exploited, which effectively highlights the distinctive characteristics among emitters and reduces noise.
Additionally, the complexity of JVMD is analyzed, which is proven to be more computational-friendly than VMD.
Simulations of both modal decomposition and SEI that involve real-world datasets are presented to illustrate that when compared with VMD, the JVMD algorithm improves the accuracy of device classification and the robustness towards noise.","['Index', 'Terms: ', 'Intrinsic modal functions (IMFs), radio frequency fingerprints (RFFs), specific emitter identification (SEI), variational mode decomposition (VMD)']",['yhuang24@kennesaw.edu']
"This paper discusses the limitations of evaluating Masked Language Models (MLMs) in code completion tasks. We highlight that relying on accuracy-based measurements may lead to an overestimation of models’ capabilities by neglecting the syntax rules of programming languages. To address these issues, we introduce a technique called SyntaxEval in which Syntactic Capabilities are used to enhance the evaluation of MLMs. SyntaxEval automates the process of masking elements in the model input based on their  Syntax Trees (ASTs). We conducted a case study on two popular MLMs using data from GitHub repositories. Our results showed negative causal effects between the node types and MLMs’ accuracy. We conclude that MLMs under study fail to predict some syntactic capabilities.","['deep learning, code generation, interpretability, transformers']",['MaryWilliamsburgVirginiaUSA']
"Linear RMS-flux relation has been well established in different spectral states of all accreting systems. In this work, we study the evolution of the frequency-dependent RMS-flux relation of MAXI J1820+070 during the initial decaying phase of the 2018 outburst with Insight-HXMT over a broad energy range 1–150 keV.
As the flux decreases, we first observe a linear RMS-flux relation at frequencies from 2 mHz to 10 Hz, while such a relation breaks at varying times for different energies, leading to a substantial reduction in the slope.
Moreover, we find that the low-frequency variability exhibits the highest sensitivity to the break, which occurs prior to the hard-to-hard state transition time determined through time-averaged spectroscopy, and the time deviation increases with energy. The overall evolution of the RMS-flux slope and intercept suggests the presence of a two-component Comptonization system. One component is radially extended, explaining the strong disk-corona coupling before the break, while the other component extends vertically, contributing to the reduction of the disk-corona coupling after the break. A further vertical expansion of the latter component is required to accommodate the dynamic evolution observed in the RMS-flux slope.
In conclusion, we suggest that the RMS-flux slope in 1–150 keV band can be employed as an indicator of the disk-corona coupling and the hard-to-hard state transition in MAXI J1820+070 could be partially driven by the changes in the corona geometry.","['accretion (14) – black holes (162) –', 'High energy astrophysics(739)']","['China', 'wangyn@bao.ac.cn', 'China', 'China']"
"Quantum computing offers significant acceleration capabilities over its classical counterpart in various application domains. Consequently, there has been substantial focus on improving quantum computing capabilities. However, to date, the security implications of these quantum computing platforms have been largely overlooked. With the emergence of cloud-based quantum computing services, it is critical to investigate the extension of classical computer security threats to the realm of quantum computing.
In this study, we investigated timing-based side-channel vulnerabilities within IBM’s cloud-based quantum service.
The proposed attack effectively subverts the confidentiality of the executed quantum algorithm, using a more realistic threat model compared to existing approaches. Our experimental results, conducted using IBM’s quantum cloud service, demonstrate that with just 10 measurements, it is possible to identify the underlying quantum computer that executed the circuit. Moreover, when evaluated using the popular Grover circuit, we showcase the ability to leak the quantum oracle with a mere 500 measurements. These findings underline the pressing need to address timing-based vulnerabilities in quantum computing platforms and advocate for enhanced security measures to safeguard sensitive quantum algorithms and data.","['Index', 'Terms: ', 'Quantum computing, side-channel, security, cloud computing, timing side-channel']",['USA']
"FinTech platforms facilitated by digital payments are watching growth rapidly, which enable the distribution of mutual funds personalized to individual investors via mobile Apps. As the important intermediation of financial products investment, these platforms distribute thousands of mutual funds obtaining impressions under guaranteed delivery (GD) strategy required by fund companies. Driven by the profit from fund purchases of users, the platform aims to maximize each transaction amount of customers by promoting mutual funds to these investors who will be interested in. Different from the conversions in traditional advertising or e-commerce recommendations, the investment amount in each purchase varies greatly even for the same financial product, which provides a significant challenge for the promotion recommendation of mutual funds. In addition to predicting the click-through rate (CTR) or the conversion rate (CVR) as in traditional recommendations, it is essential for FinTech platforms to estimate the customers’ purchase amount for each delivered fund and achieve an effective allocation of impressions based on the predicted results to optimize the total expected transaction value (ETV). In this paper, we propose an ETV-optimized customer allocation framework (EOCA) that aims to maximize the total ETV of recommended funds, under the constraints of GD dealt with fund companies. EOCA consists of two phases: a prediction phase of the customer purchase amount followed by a constrained allocation phase. Specifically, we propose an entire space deep probabilistic model with a novel-designed loss function to predict the purchase amount when a promotional fund is exposed to a user, which involves not only the conversion rate prediction but also the post-conversion purchase amount estimation. Based on the predicted ETV, we design a heuristic algorithm to solve the large-scale constrained combinatorial optimization problem to suggest which fund each user should be exposed to in order to maximize the total purchase amount. To the best of our knowledge, it’s the first attempt to solve the GD problem for financial product promotions based on customer purchase amount prediction. We conduct extensive experiments on large-scale real-world datasets and online tests based on LiCaiTong, Tencent’s wealth management platform, to demonstrate the effectiveness of our proposed EOCA framework.","['FinTech platform, customer allocation, purchase amount prediction']","['TencentShenzhenChina', 'FiT,TencentShenzhenChina', 'FiT,TencentShenzhenChina', '(SZ)ShenzhenChina', 'TencentShenzhenChina']"
"This paper explores opportunities and challenges of task (goal)-oriented and semantic communications for next-generation (NextG) communication networks through the integration of multi-task learning. This approach employs deep neural networks representing a dedicated encoder at the transmitter and multiple task-specific decoders at the receiver, collectively trained to handle diverse tasks including semantic information preservation, source input reconstruction, and integrated sensing and communications. To extend the applicability from point-to-point links to multi-receiver settings, we envision the deployment of decoders at various receivers, where decentralized learning addresses the challenges of communication load and privacy concerns, leveraging federated learning techniques that distribute model updates across decentralized nodes. However, the efficacy of this approach is contingent on the robustness of the employed deep learning models. We scrutinize potential vulnerabilities stemming from adversarial attacks during both training and testing phases. These attacks aim to manipulate both the inputs at the encoder at the transmitter and the signals received over the air on the receiver side, highlighting the importance of fortifying semantic communications against potential multi-domain exploits. Overall, the joint and robust design of task-oriented communications, semantic communications, and integrated sensing and communications in a multi-task learning framework emerges as the key enabler for context-aware, resource-efficient, and secure communications ultimately needed in NextG network systems.","['Index', 'Terms: ', 'Task-oriented communications, semantic communications, integrated sensing and communications, deep learning, multi-task learning, distributed learning, security.']","['USA', 'USA', 'USA', 'USA']"
"Multi-modal Learning has attracted widespread attention in medical image analysis. Using multi-modal data, whole slide images (WSIs) and clinical information, can improve the performance of deep learning models in the diagnosis of axillary lymph node metastasis. However, clinical information is not easy to collect in clinical practice due to privacy concerns, limited resources, lack of interoperability, etc.
Although patient selection can ensure the training set to have multi-modal data for model development, missing modality of clinical information can appear during test. This normally leads to performance degradation, which limits the use of multi-modal models in the clinic. To alleviate this problem, we propose a bidirectional distillation framework consisting of a multi-modal branch and a single-modal branch. The single-modal branch acquires the complete multi-modal knowledge from the multi-modal branch, while the multi-modal learns the robust features of WSI from the single-modal. We conduct experiments on a public dataset of Lymph Node Metastasis in Early Breast Cancer to validate the method. Our approach not only achieves state-of-the-art performance with an AUC of 0.861 on the test set without missing data, but also yields an AUC of 0.842 when the rate of missing modality is 80%. This shows the effectiveness of the approach in dealing with multi-modal data and missing modality. Such a model has the potential to improve treatment decision-making for early breast cancer patients who have axillary lymph node metastatic status.","['Index', 'Terms: ', 'Missing modality,', 'Whole slide image,', 'Clinical data.']",['yanglin@westlake.edu.cn']
"There is growing evidence that high-mass star formation (HMSF) is a multiscale, dynamical process in molecular clouds, where filaments transport gas material between larger and smaller scales. We analyze here multiscale gas dynamics in an HMSF filamentary cloud, G034.43+00.24 (G34), using APEX observations of C1818{}^{18}start_FLOATSUPERSCRIPT 18 end_FLOATSUPERSCRIPTO (2-1), HCO+{}^{+}start_FLOATSUPERSCRIPT + end_FLOATSUPERSCRIPT/H1313{}^{13}start_FLOATSUPERSCRIPT 13 end_FLOATSUPERSCRIPTCO+{}^{+}start_FLOATSUPERSCRIPT + end_FLOATSUPERSCRIPT (3-2), and HCN/H1313{}^{13}start_FLOATSUPERSCRIPT 13 end_FLOATSUPERSCRIPTCN (3-2) lines. We find large-scale, filament-aligned velocity gradients from C1818{}^{18}start_FLOATSUPERSCRIPT 18 end_FLOATSUPERSCRIPTO emission, which drive filamentary gas inflows onto dense clumps in the middle ridge of G34. The nature of these inflows is gravity-driven.
We also find clump-scale gas infall in the middle ridge of MM2, MM4, and MM5 clumps from other lines. Their gas infall rates could depend on large-scale filamentary gas inflows since the infall/inflow rates on these two scales are comparable.
We confirm that the multiscale, dynamical HMSF scenario is at work in G34.
It could be driven by gravity up to the filament scale, beyond which turbulence originating from several sources including gravity could be in effect in G34.","['Star forming regions (1565);', 'Molecular clouds (1072);', 'Infrared dark clouds (787);', 'High-mass stars (1834);', 'Molecular gas (1073)']","['China', 'China', 'China']"
"Communication protocols form the bedrock of our interconnected world, yet vulnerabilities within their implementations pose significant security threats.
Recent developments have seen a surge in fuzzing-based research dedicated to uncovering these vulnerabilities within protocol implementations.
However, there still lacks a systematic overview of protocol fuzzing for answering the essential questions such as what the unique challenges are, how existing works solve them, etc.
To bridge this gap, we conducted a comprehensive investigation of related works from both academia and industry.
Our study includes a detailed summary of the specific challenges in protocol fuzzing, and provides a systematic categorization and overview of existing research efforts. Furthermore, we explore and discuss potential future research directions in protocol fuzzing. This survey serves as a foundational guideline for researchers and practitioners in the field.","['Protocol,', 'Fuzz', 'Testing,', 'Security']","['AnShaanxiChina710071', 'AveSingaporeSingaporeSingapore639798', 'AnShaanxiChina710071', 'ProvinceChina210023', 'AveSingaporeSingaporeSingapore639798', 'AveSingaporeSingaporeSingapore639798', 'DistrictBeijingBeijingChina100085', 'AveSingaporeSingaporeSingapore639798', 'StSingaporeSingaporeSingapore188065']"
"An important aspect of solar energetic particle (SEP) events is their source populations.
Elemental abundance enhancements of impulsive SEP events, originating in presumed coronal
reconnection episodes, can be fitted to steep power laws of A/Q,
where A and Q are the atomic mass and ionic charge.
Since thermal electron energies are enhanced and
nonthermal electron distributions arise in the reconnection process, we might expect that ionic
charge states Q would be increased through ionization interactions with those electron populations
during the acceleration process. The temperature estimated from the SEPs corresponds to the charge state during the acceleration process, while the actual charge state measured in situ may be modified as the SEPs pass through the corona.
We examine whether the temperature estimation from the A/Q would differ with various kappa values
in a kappa function representing high-energy tail deviating from a Maxwellian velocity distribution.
We find that the differences in the A/Q between a Maxwellian and an extreme kappa distribution are only about 10-30%. We fit power-law enhancement of element abundances as a function of their A/Q with various kappa values. Then, we find that the derived source region temperature is not significantly affected by whether or not the electron velocity distribution deviates from a Maxwellian, i.e., thermal, distribution. Assuming that electrons are heated in the acceleration region, the agreement of the SEP charge state during acceleration with typical active region temperatures suggests that SEPs are accelerated and leave the acceleration region in a shorter time than the ionization time scale.","['The', 'Sun (1693) —', 'Solar', 'Energetic', 'Particles (1491) —', 'Non-thermal radiation sources (1119)']","['Korea', 'USA', 'USA', 'USA']"
"Consumer-resource dynamics is central in determining biomass transport across ecosystems. The assumptions of mass action, chemostatic conditions and stationarity in stochastic feeding dynamics lead to Holling type II functional responses, whose use is widespread in macroscopic models of population dynamics. However, to be useful for parameter inference, stochastic population models need to be identifiable, this meaning that model parameters can be uniquely inferred from a large number of model observations. In this contribution we study parameter identifiability in a multi-resource consumer-resource model, for which we can obtain the steady-state and out-of-equilibrium probability distributions of predator’s abundances by analytically solving the master equation. Based on these analytical results, we can conduct in silico experiments by tracking the abundance of consumers that are either searching for or handling prey, data then used for maximum likelihood parameter estimation. We show that, when model observations are recorded out of equilibrium, feeding parameters are truly identifiable, whereas if sampling is done at stationarity, only ratios of rates can be inferred from data. We discuss the implications of our results when inferring parameters of general dynamical models.","['stochastic consumer-resource models, master equation, out-of-equilibrium distribution, parameter inference, multi-resource', 'Holling type', 'II feeding dynamics']","['Spain.', 'Spain.']"
"Machine learning models underpin many modern financial systems for use cases such as fraud detection and churn prediction. Most are based on supervised learning with hand-engineered features, which relies heavily on the availability of labelled data. Large self-supervised generative models have shown tremendous success in natural language processing and computer vision, yet so far they haven’t been adapted to multivariate time series of financial transactions. In this paper, we present a generative pretraining method that can be used to obtain contextualised embeddings of financial transactions. Benchmarks on public datasets demonstrate that it outperforms state-of-the-art self-supervised methods on a range of downstream tasks. We additionally perform large-scale pretraining of an embedding model using a corpus of data from 180 issuing banks containing 5.1 billion transactions and apply it to the card fraud detection problem on hold-out datasets. The embedding model significantly improves value detection rate at high precision thresholds and transfers well to out-of-domain distributions.","['transaction embeddings, self-supervised learning, generative modelling, multivariate time series, fraud detection']","['LabFeaturespaceCambridgeUK', 'LabFeaturespaceCambridgeUK', 'LabFeaturespaceCambridgeUK', 'LabFeaturespaceCambridgeUK', 'LabFeaturespaceCambridgeUK']"
"E-commerce platforms usually present an ordered list, mixed with several organic items and an advertisement, in response to each user’s page view request. This list, the outcome of ad auction and allocation processes, directly impacts the platform’s ad revenue and gross merchandise volume (GMV). Specifically, the ad auction determines which ad is displayed and the corresponding payment, while the ad allocation decides the display positions of the advertisement and organic items. The prevalent methods of segregating the ad auction and allocation into two distinct stages face two problems: 1) Ad auction does not consider externalities, such as the influence of actual display position and context on ad Click-Through Rate (CTR); 2) The ad allocation, which utilizes the auction-winning ad’s payment to determine the display position dynamically, fails to maintain incentive compatibility (IC) for the advertisement. For instance, in the auction stage employing the traditional Generalized Second Price (GSP) , even if the winning ad increases its bid, its payment remains unchanged. This implies that the advertisement cannot secure a better position and thus loses the opportunity to achieve higher utility in the subsequent ad allocation stage. Previous research often focused on one of the two stages, neglecting the two-stage problem, which may result in suboptimal outcomes.

Therefore, this paper proposes a deep automated mechanism that integrates ad auction and allocation, ensuring both IC and Individual Rationality (IR) in the presence of externalities while maximizing revenue and GMV. The mechanism takes candidate ads and the ordered list of organic items as input. For each candidate ad, several candidate allocations are generated by inserting the ad in different positions of the ordered list of organic items. For each candidate allocation, a list-wise model takes the entire allocation as input and outputs the predicted result for each ad and organic item to model the global externalities. Finally, an automated auction mechanism, modeled by deep neural networks, is executed to select the optimal allocation. Consequently, this mechanism simultaneously decides the ranking, payment, and display position of the ad. Furthermore, the proposed mechanism results in higher revenue and GMV than state-of-the-art baselines in offline experiments and online A/B tests.","['Automated', 'Mechanism', 'Design,', 'Ad', 'Auction,', 'Externalities,', 'Ad', 'Allocation']","['MeituanBeijingChina', 'MeituanBeijingChina', 'MeituanBeijingChina', 'MeituanBeijingChina', 'MeituanBeijingChina', 'MeituanBeijingChina']"
"In the last years, social media has gained an unprecedented amount of attention, playing a pivotal role in shaping the contemporary landscape of communication and connection. However, Coordinated Inhautentic Behaviour (CIB), defined as orchestrated efforts by entities to deceive or mislead users about their identity and intentions, has emerged as a tactic to exploit the online discourse.
In this study, we quantify the efficacy of CIB tactics by defining a general framework for evaluating the influence of a subset of nodes in a directed tree.
We design two algorithms that provide optimal and greedy post-hoc placement strategies that lead to maximising the configuration influence.
We then consider cascades from information spreading on Twitter to compare the observed behaviour with our algorithms.
The results show that, according to our model, coordinated accounts are quite inefficient in terms of their network influence, thus suggesting that they may play a less pivotal role than expected.
Moreover, the causes of these poor results may be found in two separate aspects: a bad placement strategy and a scarcity of resources.","['Trees,', 'Influence,', 'Coordinated', 'Inhautentic', 'Behaviour']","['295RomaItaly', 'SienaSienaItaly', '295RomaItaly', '295RomaItaly']"
"I identify a point-symmetric morphology of the supernova remnant (SNR) G352.7-0.1 and propose that the outer axially-symmetric structure is the remnant of a common envelope evolution (CEE) of the progenitor system, while the inner structure is the ejecta of a thermonuclear explosion triggered by the merger of a white dwarf (WD) and the core of an asymptotic giant branch (AGB) star. The main radio structure of SNR G352.7-0.1 forms an outer (large) ellipse. The bright X-ray emitting gas forms a smaller ellipse with a symmetry axis inclined to the symmetry axis of the large radio ellipse. The high abundance of iron and the energy of its X-ray lines suggest a type Ia supernova (SN Ia). The massive swept-up gas suggests a relatively massive progenitor system. I propose a scenario with progenitors of initial masses of MZAMS,1≃5−7⁢M⊙similar-to-or-equalssubscript𝑀ZAMS157subscript𝑀direct-productM_{\rm ZAMS,1}\simeq 5-7M_{\odot}italic_M start_POSTSUBSCRIPT roman_ZAMS , 1 end_POSTSUBSCRIPT ≃ 5 - 7 italic_M start_POSTSUBSCRIPT ⊙ end_POSTSUBSCRIPT and MZAMS,2≃4−5⁢M⊙similar-to-or-equalssubscript𝑀ZAMS245subscript𝑀direct-productM_{\rm ZAMS,2}\simeq 4-5M_{\odot}italic_M start_POSTSUBSCRIPT roman_ZAMS , 2 end_POSTSUBSCRIPT ≃ 4 - 5 italic_M start_POSTSUBSCRIPT ⊙ end_POSTSUBSCRIPT. At a later phase, the WD remnant of the primary star and the AGB secondary star experience a CEE that ejects the circumstellar material that swept up more ISM to form the large elliptical radio structure. An explosion during the merger of the WD with the core of the AGB star triggered a super-Chandrasekhar thermonuclear explosion that formed the inner structure that is bright in X-ray. A tertiary star in the system caused the misalignment of the two symmetry axes. This study adds to the rich variety of evolutionary routes within the different scenarios of normal and peculiar SNe Ia.","['Type', 'Ia supernovae –', 'Supernova remnants –', 'Common envelope binary stars –', 'Planetary nebulae –', 'Stellar jets']",['soker@physics.technion.ac.il']
"Monitoring cameras are extensively utilized in industrial production to monitor equipment running. With advancements in computer vision, device recognition using image features is viable. This paper presents a vision-assisted identification system that implements real-time automatic equipment labeling through image matching in surveillance videos. The system deploys the ORB algorithm to extract image features and the GMS algorithm to remove incorrect matching points. According to the principles of clustering and template locality, a method known as Local Adaptive Clustering (LAC) has been established to enhance label positioning. This method segments matching templates using the cluster center, which improves the efficiency and stability of labels. The experimental results demonstrate that LAC effectively curtails the label drift.","['Index', 'Terms: ', 'Image', 'Matching,', 'Local', 'Clustering,', 'Automatic', 'Identification']",['2023200830@mail.buct.edu.cn']
"We introduce a new challenge to the software development community: 1) leveraging AI to accurately detect and flag up secrets in code and on popular document sharing platforms that frequently used by developers, such as Confluence and 2) automatically remediating the detections (e.g. by suggesting password vault functionality). This is a challenging, and mostly unaddressed task. Existing methods leverage heuristics and regular expressions, that can be very noisy, and therefore increase toil on developers. The next step - modifying code itself - to automatically remediate a detection, is a complex task. We introduce two baseline AI models that have good detection performance and propose an automatic mechanism for remediating secrets found in code, opening up the study of this task to the wider community.","['artificial intelligence, software engineering, cybersecurity']","['Chase', 'Chase', 'Chase', 'Chase', 'Chase']"
"With the development of social media, rumors have been spread broadly on social media platforms, causing great harm to society. Beside textual information, many rumors also use manipulated images or conceal textual information within images to deceive people and avoid being detected, making multimodal rumor detection be a critical problem. The majority of multimodal rumor detection methods mainly concentrate on extracting features of source claims and their corresponding images, while ignoring the comments of rumors and their propagation structures. These comments and structures imply the wisdom of crowds and are proved to be crucial to debunk rumors. Moreover, these methods usually only extract visual features in a basic manner, seldom consider tampering or textual information in images. Therefore, in this study, we propose a novel Vision and Graph Fused Attention Network (VGA) for rumor detection to utilize propagation structures among posts so as to obtain the crowd opinions and further explore visual tampering features, as well as the textual information hidden in images. We conduct extensive experiments on three datasets, demonstrating that VGA can effectively detect multimodal rumors and outperform state-of-the-art methods significantly.","['rumor detection, multimodal fusion, propagation structure, social media']","['UniversityHaidianBeijingChina', 'UniversityHaidianBeijingChina', 'UniversityHaidianBeijingChina', 'UniversityHaidianBeijingChina']"
"The proliferation of low-quality online information in today’s era has underscored the need for robust and automatic mechanisms to evaluate the trustworthiness of online news publishers. In this paper, we analyse the trustworthiness of online news media outlets by leveraging a dataset of 4033 news stories from 40 different sources. We aim to infer the trustworthiness level of the source based on the classification of individual articles’ content. The trust labels are obtained from NewsGuard, a journalistic organization that evaluates news sources using well-established editorial and publishing criteria. The results indicate that the classification model is highly effective in classifying the trustworthiness levels of the news articles. This research has practical applications in alerting readers to potentially untrustworthy news sources, assisting journalistic organizations in evaluating new or unfamiliar media outlets and supporting the selection of articles for their trustworthiness assessment.","['Online', 'News,', 'Transparency and', 'Reputability of', 'Online', 'News', 'Sources,', 'Multiclass', 'Classification,', 'Data', 'Science for', 'Social', 'Good']","['LuccaItaly55100', 'IIT-CNRItaly55100', 'LuccaItaly', 'LuccaItaly']"
"Much debate nowadays is devoted to the impacts of modern information and communication technology on global carbon emissions. Green information and communication technology is a paradigm creating a sustainable and environmentally friendly computing field that tries to minimize the adverse effects on the environment. Green information and communication technology are under constant development nowadays. Thus, in this paper, we undertake the problem of performance bugs that, until recently, have never been studied so profoundly. We assume that inappropriate software implementations can have a crucial influence on global carbon emissions. Here, we classify those performance bugs and develop inappropriate implementations of four programs written in C++. To mitigate these simulated performance bugs, measuring software and hardware methods that can estimate the increased carbon footprint properly were proposed.","['Index', 'Terms: \n\ncarbon footprint, green computing, performance bugs, software engineering']","['iztok.fister1@um.si', 'dusan.fister@um.si', 'vili.podgorelec@um.si', 'iztok.fister@um.si']"
"It was proposed that the tensor product structure of the Hilbert space is uniquely determined by the Hamiltonian’s spectrum, for most finite-dimensional cases satisfying certain conditions.
I show that, for more than three qudits, any such method can only lead to infinitely many tensor product structures. The number of additional continuous parameters needed to find a unique solution is exponential in the number of qudits. In addition, even if the result were unique, such a Hamiltonian would not entangle subsystems.
These results affect some proposals to recover the 3d space from the Hamiltonian.","['tensor product structure; entanglement; emergent spacetime;', 'Hilbert space fundamentalism.']",['holotronix@gmail.com']
"Connected and automated vehicles (CAVs) have become a transformative technology that can change our daily life.
Currently, millimeter-wave (mmWave) bands are identified as the promising CAV connectivity solution. While it can provide high data rate, their realization faces many challenges such as high attenuation during mmWave signal propagation and mobility management. Existing solution has to initiate pilot signal to measure channel information, then apply signal processing to calculate the best narrow beam towards the receiver end to guarantee sufficient signal power. This process takes significant overhead and time, hence not suitable for vehicles. In this study, we propose an autonomous and low-cost testbed to collect extensive co-located mmWave signal and other sensors data such as LiDAR (Light Detection and Ranging), cameras, ultrasonic, etc, traditionally for “automated”, to facilitate mmWave vehicular communications. Intuitively, these sensors can build a 3D map around the vehicle and signal propagation path can be estimated, eliminating iterative the process via pilot signals. This multimodal data fusion, together with AI, is expected to bring significant advances in “connected” research.","['Index', 'Terms: ', 'Vehicular communication, mmWave communication, sensor fusion, machine learning.']",['2hsun@uga.edu']
"Energy disaggregation is a promising solution to access detailed information on energy consumption in a household, by itemizing its total energy consumption. However, in real-world applications, overfitting remains a challenging problem for data-driven disaggregation methods. First, the available real-world datasets are biased towards the most frequently used appliances. Second, both real and synthetic publicly-available datasets are limited in number of appliances, which may not be sufficient for a disaggregation algorithm to learn complex relations among different types of appliances and their states. To address the lack of appliance data, we propose two physics-informed data generators: one for high sampling rate signals (kHz) and another for low sampling rate signals (Hz). These generators rely on prior knowledge of the physics of appliance energy consumption, and are capable of simulating a virtually unlimited number of different appliances and their corresponding signatures for any time period. Both methods involve defining a mathematical model, selecting centroids corresponding to individual appliances, sampling model parameters around each centroid, and finally substituting the obtained parameters into the mathematical model. Additionally, by using Principal Component Analysis and Kullback-Leibler divergence, we demonstrate that our methods significantly outperform the previous approaches.","['Index', 'Terms: \nenergy disaggregation, non-intrusive load monitoring, synthetic data, physics-informed methods']",['H.Ouerdane@skoltech.ru']
"Photodissociated gas bears the signature of the dynamical evolution of the
ambient interstellar medium impacted by the mechanical and radiative feedback
from an expanding H ii region. Here we present an analysis of the kinematics of
the young Trifid nebula, based on velocity-resolved observations of the far-infrared fine-structure lines of [C ii] at 158 µm and [O i] at 63 µm. The distribution of the photodissociated regions (PDRs) surrounding the nebula is consistent with a shell-like structure created by the H ii region expanding with a velocity of 5 km s−11{}^{-1}start_FLOATSUPERSCRIPT - 1 end_FLOATSUPERSCRIPT. Comparison of ratios of [C ii] and [O i]63 µm intensities for identical velocity components with PDR models indicate a density of 1044{}^{4}start_FLOATSUPERSCRIPT 4 end_FLOATSUPERSCRIPT cm−33{}^{-3}start_FLOATSUPERSCRIPT - 3 end_FLOATSUPERSCRIPT. The red- and blue-shifted PDR shells with a combined mass of 516 M⊙subscriptMdirect-product\mathrm{M}_{\odot}roman_M start_POSTSUBSCRIPT ⊙ end_POSTSUBSCRIPT have a kinetic energy of ∼1047similar-toabsentsuperscript1047\sim 10^{47}∼ 10 start_POSTSUPERSCRIPT 47 end_POSTSUPERSCRIPT erg. This is consistent with the thermal energy of the H ii region as well as with the energy deposited by the stellar wind luminosity from HD 169442A, an O7 V star, over the 0.5 Myr lifetime of the star. The observed momentum of the PDR shell is lower than what theoretical calculations predict for the radial momentum due to the shell being swept up by an expanding H ii region, which suggests that significant mass loss has occurred in M20 due to the dispersal of the surrounding gas by the advancing ionization front.","['ISM: clouds –', 'ISM: kinematics and dynamics – submillimetre:', 'ISM –', 'ISM: structure\n– stars: formation –', 'ISM:individual (M20)']","['India', 'USA']"
"Using large training datasets enhances the generalization capabilities of neural networks.
Semi-supervised learning (SSL) is useful when there are few labeled data and a lot of unlabeled data.
SSL methods that use data augmentation are most successful for image datasets.
In contrast, texts do not have consistent augmentation methods as images.
Consequently, methods that use augmentation are not as effective in text data as they are in image data.
In this study, we compared SSL algorithms that do not require augmentation; these are self-training, co-training, tri-training, and tri-training with disagreement.
In the experiments, we used 4 different text datasets for different tasks.
We examined the algorithms from a variety of perspectives by asking experiment questions and suggested several improvements.
Among the algorithms, tri-training with disagreement showed the closest performance to the Oracle; however, performance gap shows that new semi-supervised algorithms or improvements in existing methods are needed.","['Index', 'Terms: \nsemi supervised learning, self-training, co-training, tri-training, tri-training with disagreement']","['tkesgin@yildiz.edu.tr', 'amasyali@yildiz.edu.tr']"
"We use the VERITAS imaging air Cherenkov Telescope (IACT) array to obtain the first measured angular diameter of β𝛽\betaitalic_β UMa at visual wavelengths using stellar intensity interferometry (SII) and independently constrain the limb-darkened angular diameter.
The age of the Ursa Major moving group has been assessed from the ages of its members, including nuclear member Merak (β𝛽\betaitalic_β UMa), an A1-type subgiant, by comparing effective temperature and luminosity constraints to model stellar evolution tracks.
Previous interferometric limb-darkened angular-diameter measurements of β𝛽\betaitalic_β UMa in the near-infrared (CHARA Array, 1.149 ±plus-or-minus\pm± 0.014 mas) and mid-infrared (Keck Nuller, 1.08 ±plus-or-minus\pm± 0.07 mas), together with the measured parallax and bolometric flux, have constrained the effective temperature.
This paper presents current VERITAS-SII observation and analysis procedures to derive squared visibilities from correlation functions.
We fit the resulting squared visibilities to find a limb-darkened angular diameter of 1.07±0.04⁢(stat)±0.05plus-or-minus1.070.04stat0.051.07\pm 0.04{\rm~{}(stat)}\pm 0.051.07 ± 0.04 ( roman_stat ) ± 0.05 (sys) mas, using synthetic visibilities from a stellar atmosphere model that provides a good match to the spectrum of β𝛽\betaitalic_β UMa in the optical wave band. The VERITAS-SII limb-darkened angular diameter yields an effective temperature of 9700±200±200plus-or-minus97002002009700\pm 200\pm 2009700 ± 200 ± 200 K,
consistent with ultraviolet spectrophotometry, and an age of 390±29±32plus-or-minus3902932390\pm 29\pm 32390 ± 29 ± 32 Myr, using MESA Isochrones and Stellar Tracks (MIST). This age is consistent with 408 ±plus-or-minus\pm± 6 Myr from the CHARA Array angular diameter.","['Long baseline interferometry (932),', 'Fundamental parameters of stars (555),', 'Astronomy data modeling (1859)']","['USA', 'USA', 'USA', 'USA', 'Germany', 'USA', 'USA', 'USA', 'USA', 'USA', 'USA', 'USA', 'USA', 'USA', 'USA', 'USA', 'USA', 'USA', 'Germany', 'USA', 'USA', 'USA', 'Canada', 'France', 'USA', 'Ireland', 'USA', 'Ireland', 'USA', '3N6', 'USA', 'Germany', 'Germany', 'Ireland', 'USA', 'Canada', 'USA', 'USA', 'USA', 'Germany', 'USA', 'USA', 'USA', 'Germany', 'USA', 'USA', 'Canada']"
"Context: Cybercrime groups, driven by financial and geopolitical motives, launch advanced persistent threat (APT) attacks. The attacks consist of adversarial techniques, which adversaries perform step-by-step by during cyberattacks. Cybersecurity vendors often publish cyber threat intelligence (CTI) reports, referring to the written artifacts on technical and forensic analysis of the techniques used by the malware in APT attacks. To defend organizations, prevalent techniques used by malware in APT attacks and the association among the techniques need to be identified. Objective: The goal of this research is to inform cybersecurity practitioners about how adversaries form cyberattacks through an analysis of adversarial techniques documented in cyberthreat intelligence reports. Dataset: We use 594 adversarial techniques cataloged in MITRE ATT&CK. We systematically construct a set of 667 CTI reports that MITRE ATT&CK used as citations in the descriptions of the cataloged adversarial techniques. Methodology: We analyze the frequency and trend of adversarial techniques, followed by a qualitative analysis of the implementation of techniques. Next, we perform association rule mining to identify pairs of techniques recurring in APT attacks. We then perform qualitative analysis to identify the underlying relations among the techniques in the recurring pairs. Findings: The set of 667 CTI reports documents 10,370 techniques in total, and we identify 19 prevalent techniques accounting for 37.3% of documented techniques. We also identify 425 statistically significant recurring pairs and seven types of relations among the techniques in these pairs. The top three among the seven relationships suggest that techniques used by the malware inter-relate with one another in terms of (a) abusing or affecting the same system assets, (b) executing in sequences, and (c) overlapping in their implementations. We identify that obtaining information on the operating and network system of the victim environment is the most prevalent technique and appears in the highest number of recurring pairs. We identify that spear-phishing is the most prevalent way of initial infection. We also identify three prevalent misuses of system functionalities: macro in office documents, registry in Windows, and task scheduler. We also identify that mimicking legitimate users through compromised credentials is the most prevalent persistence-related technique used by malware. Overall, the study quantifies how adversaries leverage techniques through malware in APT attacks based on publicly reported documents. We advocate organizations prioritize their defense against the identified prevalent techniques and actively hunt for potential malicious intrusion based on the identified pairs of techniques.","['Tactics, techniques, and procedures,', 'ATT&CK,', 'APT attacks,', 'Multi-stage attacks, malware, cyber-criminal groups, cyber-threat actors,', 'TTPs,', 'Advanced persistent threats,', 'Threat hunting,', 'Cyberattack']","['UniversityRaleighNCUSA', 'UniversityRaleighNCUSA', 'UniversityRaleighNCUSA', 'UniversityRaleighNCUSA']"
"On 2022 February 15, an impressive filament eruption was observed off the solar eastern limb from three remote-sensing viewpoints, namely Earth, STEREO-A, and Solar Orbiter. In addition to representing the most-distant observed filament at extreme ultraviolet wavelengths—captured by Solar Orbiter’s field of view extending to above 6 R⊙subscript𝑅direct-productR_{\odot}italic_R start_POSTSUBSCRIPT ⊙ end_POSTSUBSCRIPT—this event was also associated with the release of a fast (∼similar-to\sim∼2200 km⋅⋅\cdot⋅s−11{}^{-1}start_FLOATSUPERSCRIPT - 1 end_FLOATSUPERSCRIPT) coronal mass ejection (CME) that was directed towards BepiColombo and Parker Solar Probe. These two probes were separated by 2∘{}^{\circ}start_FLOATSUPERSCRIPT ∘ end_FLOATSUPERSCRIPT in latitude, 4∘{}^{\circ}start_FLOATSUPERSCRIPT ∘ end_FLOATSUPERSCRIPT in longitude, and 0.03 au in radial distance around the time of the CME-driven shock arrival in situ. The relative proximity of the two probes to each other and to the Sun (∼similar-to\sim∼0.35 au) allows us to study the mesoscale structure of CMEs at Mercury’s orbit for the first time. We analyse similarities and differences in the main CME-related structures measured at the two locations, namely the interplanetary shock, the sheath region, and the magnetic ejecta. We find that, despite the separation between the two spacecraft being well within the typical uncertainties associated with determination of CME geometric parameters from remote-sensing observations, the two sets of in-situ measurements display some profound differences that make understanding of the overall 3D CME structure particularly challenging. Finally, we discuss our findings within the context of space weather at Mercury’s distances and in terms of the need to investigate solar transients via spacecraft constellations with small separations, which has been gaining significant attention during recent years.","['Solar filament eruptions (1981);', 'Solar coronal mass ejections (310);', 'Interplanetary magnetic fields (824);', 'Interplanetary shocks (829)']","['USA', 'USA', 'USA', 'USA', 'USA', 'UK', 'USA', 'USA', 'USA', 'USA', 'USA', 'USA', 'USA', 'Germany', 'Germany', 'USA', 'USA', 'USA', 'Belgium', 'Russia', 'Belgium', 'USA', 'USA', 'Spain', 'Spain', 'USA', 'Finland', 'Finland', 'Finland', 'Austria', 'Austria', 'Germany', 'Germany', 'USA', 'USA', 'Belgium', 'Romania']"
"Relativistic jets accompany the collapse of massive stars, the merger of compact objects, or the accretion of gas in active galactic nuclei. They carry information about the central engine and generate electromagnetic radiation. No self-consistent simulations have been able to follow these jets from their birth at the black hole scale to the Newtonian dissipation phase, making the inference of central engine property through astronomical observations undetermined. We present the general relativistic moving-mesh framework to achieve the continuity of jet simulations throughout space-time. We implement the general relativistic extension for the moving-mesh relativistic hydrodynamic code-JET, and develop a tetrad formulation to utilize the HLLC Riemann solver in the general relativistic moving mesh code. The new framework is able to trace the radial movement of the relativistic jets from the central region where strong gravity holds all the way to distances of jet dissipation.","['General', 'Relativistic,', 'Relativistic jet,', 'HLLC']","['Germany', 'Germany']"
"As time progresses, the need for more secure applications grows exponentially. The different types of sensitive information that is being transferred virtually has sparked a rise in systems that leverage blockchain. Different sectors are beginning to use this disruptive technology to evaluate the risks and benefits. Sectors like finance, medicine, higher education, and wireless communication have research regarding blockchain. Futhermore, the need for security standards in this area of research is pivotal. In recent past, several attacks on blockchain infrastructures have resulted in hundreds of millions dollars lost and sensitive information compromised. Some of these attacks include DAO attacks, bZx attacks, and Parity Multisignature Wallet Double Attacks which targeted vulnerabilities within smart contracts on the Ethereum network. These attacks exposed the weaknesses of current smart contract development practices which has led to the increase in distrust and adoption of systems that leverage blockchain for its functionality. In this paper, I identify common software vulnerabilities and attacks on blockchain infrastructures, thoroughly detail the smart contract development process and propose a model for ensuring a stronger security standard for future systems leveraging smart contracts. The purpose for proposing a model is to promote trust among end users in the system which is a foundational element for blockchain adoption in the future.","['Index', 'Terms: ', 'Smart', 'Contract,', 'Blockchain,', 'Software', 'Development,', 'Cybersecurity']",['crawford@cs.ua.edu']
"In recent years, foundation models (FMs) have solidified their role as cornerstone advancements in the deep learning domain. By extracting intricate patterns from vast datasets, these models consistently achieve state-of-the-art results across a spectrum of downstream tasks, all without necessitating extensive computational resources [1].
Notably, MedCLIP [2], a vision-language contrastive learning-based medical FM, has been designed using unpaired image-text training. While the medical domain has often adopted unpaired training to amplify data [3], the exploration of potential security concerns linked to this approach hasn’t kept pace with its practical usage. Notably, the augmentation capabilities inherent in unpaired training also indicate that minor label discrepancies can result in significant model deviations. In this study, we frame this label discrepancy as a backdoor attack problem. We further analyze its impact on medical FMs throughout the FM supply chain. Our evaluation primarily revolves around MedCLIP, emblematic of medical FM employing the unpaired strategy. We begin with an exploration of vulnerabilities in MedCLIP stemming from unpaired image-text matching, termed BadMatch. BadMatch is achieved using a modest set of wrongly labeled data. Subsequently, we disrupt MedCLIP’s contrastive learning through BadDist-assisted BadMatch by introducing a Bad-Distance between the embeddings of clean and poisoned data. Intriguingly, when BadMatch and BadDist are combined, a slight 0.05 percent of misaligned image-text data can yield a staggering 99 percent attack success rate, all the while maintaining MedCLIP’s efficacy on untainted data. Additionally, combined with BadMatch and BadDist, the attacking pipeline consistently fends off backdoor assaults across diverse model designs, datasets, and triggers. Also, our findings reveal that current defense strategies are insufficient in detecting these latent threats in medical FMs’ supply chains. Code and pre-trained models can be found at https://github.com/ubc-tea/Backdoor_Multimodal_Foundation_Model.","['Index', 'Terms: ', 'Backdoor', 'Attack,', 'Foundation', 'Models,', 'Vision-Text', 'Models.', 'Contrastive', 'Learning']","['Columbia', 'Institute', 'University']"
"“Changing-look” Active Galactic Nuclei (CL-AGNs) are challenging our basic ideas about the physics of accretion flows and of circumnuclear gas around supermassive black holes (SMBHs).
Using first-year Sloan Digital Sky Survey V (SDSS-V) repeated spectroscopy of nearly 29,000 previously-known AGNs, combined with dedicated follow-up spectroscopic observations, and publicly available optical light curves, we have identified 116 CL-AGNs where (at least) one broad emission line has essentially (dis-)appeared, as well as 88 other extremely variable systems.
Our CL-AGN sample, with 107 newly identified cases, is among the largest reported to date, and includes ∼0.4%similar-toabsentpercent0.4\sim 0.4\%∼ 0.4 % of the AGNs re-observed in the first year of SDSS-V operations.
Among our CL-AGNs, 67% exhibit dimming while 33% exhibit brightening.
Our data and sample probe extreme AGN spectral variability on timescales of months to decades, including some cases of recurring transitions on surprisingly short timescales (≲2less-than-or-similar-toabsent2\lesssim 2≲ 2 months in the rest frame).
We find that CL events are preferentially found in lower Eddington ratio (fEddsubscript𝑓Eddf_{\mathrm{Edd}}italic_f start_POSTSUBSCRIPT roman_Edd end_POSTSUBSCRIPT) systems: Our CL-AGNs have a fEddsubscript𝑓Eddf_{\mathrm{Edd}}italic_f start_POSTSUBSCRIPT roman_Edd end_POSTSUBSCRIPT distribution that significantly differs from that of a redshift- and a carefully constructed, luminosity-matched control sample (pKS≲2×10−4less-than-or-similar-tosubscript𝑝KS2superscript104p_{\rm KS}\lesssim 2\times 10^{-4}italic_p start_POSTSUBSCRIPT roman_KS end_POSTSUBSCRIPT ≲ 2 × 10 start_POSTSUPERSCRIPT - 4 end_POSTSUPERSCRIPT; median fEdd≈0.025subscript𝑓Edd0.025f_{\mathrm{Edd}}\approx 0.025italic_f start_POSTSUBSCRIPT roman_Edd end_POSTSUBSCRIPT ≈ 0.025 vs. 0.0430.0430.0430.043).
This preference for low fEddsubscript𝑓Eddf_{\mathrm{Edd}}italic_f start_POSTSUBSCRIPT roman_Edd end_POSTSUBSCRIPT strengthens previous findings of higher CL-AGN incidence at lower Eddington ratios, found in much smaller samples of spectroscopically confirmed CL-AGNs.
Finally, we show that the broad Mg ii emission line in our CL-AGN sample tends to vary significantly less than the broad Hβ𝛽\betaitalic_β emission line.
Our large CL-AGN sample demonstrates the advantages and challenges in using multi-epoch spectroscopy from large surveys to study extreme AGN variability, SMBH fueling, and AGN physics.","['Supermassive black holes (1663),', 'Quasars (1319),', 'Active galactic nuclei (16)']","['Israel', 'Israel', 'USA', 'USA', 'USA', 'USA', 'USA', 'USA', 'Chile', 'Chile', 'Chile', 'USA', 'USA', 'USA', 'USA', 'USA', 'USA', 'USA', 'USA', 'USA', 'Chile', 'USA', 'Germany', 'USA', 'USA', 'USA', 'Chile', 'Chile', 'China', 'USA', 'USA', 'USA', 'USA', 'Israel', 'Chile', 'USA', 'China', 'USA', 'Germany', 'USA', 'USA', 'Moscow', 'USA', 'USA', 'Canada']"
"We present a methodology for modeling and removing light from cluster galaxies and intracluster light (ICL) from James Webb Space Telescope (JWST ) images of gravitational lensing clusters. We apply our method to Webb’s First Deep Field the SMACS 0723 Early Release Observations and use the ICL subtracted images to select a sample of globular clusters (GCs) and dwarf galaxies within the cluster. We compare the spatial distributions of these two samples with our models of the galaxy and ICL light, finding significant similarity. Specifically we find that GCs trace the diffuse ICL, while dwarf galaxies are centrally concentrated near the cluster center We quantify the relationship between the surface density of compact sources and total cluster light, demonstrating a significant, tight correlation. We repeat our methodology and compare distributions of GCs with dark matter surface density and find a comparable result. Our findings suggest a common origin for GCs and diffuse ICL, with stripping from massive galaxies as they merge with the cluster being a plausible scenario.",['JWST\xa0; galaxies'],"['Canada', 'Canada', 'Slovenia', 'Canada', 'Canada', 'Canada', 'Canada', 'Japan', 'Slovenia', 'USA', 'Denmark', 'Denmark', 'Slovenia', 'Canada', 'Canada', 'Canada', 'Denmark', 'Slovenia']"
"The increased demand of cyber security professionals has also increased the development of new platforms and tools that help those professionals to improve their offensive skills. One of these platforms is HackTheBox, an online cyber security training platform that delivers a controlled and safe environment for those professionals to explore virtual machines in a Capture the Flag (CTF) competition style.
Most of the tools used in a CTF, or even on real-world Penetration Testing (Pentest), were developed for specific reasons so each tool usually has different input and output formats. These different formats make it hard for cyber security professionals and CTF competitors to develop an attack graph. In order to help cyber security professionals and CTF competitors to discover, select and exploit an attack vector, this paper presents Shadow Blade, a tool to aid users to interact with their attack vectors.","['Index', 'Terms: \n\ncapture the flag, cyber security,', 'HackTheBox, attack graph']",['avelino.zorzo@pucrs.br3']
"In the ever-changing and dynamic realm of high-end fashion marketplaces, providing accurate and personalized size recommendations has become a critical aspect. Meeting customer expectations in this regard is not only crucial for ensuring their satisfaction but also plays a pivotal role in driving customer retention, which is a key metric for the success of any fashion retailer. 
We propose a novel sequence classification approach to address this problem, integrating implicit (Add2Bag) and explicit (ReturnReason) user signals.
Our approach comprises two distinct models: one employs LSTMs to encode the user signals, while the other leverages an Attention mechanism.
Our best model outperforms SFNet, improving accuracy by 45.7%.
By using Add2Bag interactions we increase the user coverage by 24.5% when compared with only using Orders.
Moreover, we evaluate the models’ usability in real-time recommendation scenarios by conducting experiments to measure their latency performance.","['Size', 'Recommendation,', 'Recommendations', 'Systems,', 'Size and', 'Fit,', 'Personalization,', 'E-commerce']","['FARFETCHLisbonPortugal', 'FARFETCHPortoPortugal', 'FARFETCHPortoPortugal', 'Kingdom']"
"We present the detection of a magnetized dust ring (M0.8-0.2) in the Central Molecular Zone (CMZ) of the Galactic Center. The results presented in this paper utilize the first data release (DR1) of the Far-Infrared Polarimetric Large Area CMZ Exploration (FIREPLACE) survey (i.e., FIREPLACE I; Butterfield et al., 2023). The FIREPLACE survey is a 214 µm polarimetic survey of the Galactic Center using the SOFIA/HAWC+ telescope. The M0.8–0.2 ring is a region of gas and dust that has a circular morphology with a central depression. The dust polarization in the M0.8–0.2 ring implies a curved magnetic field that traces the ring-like structure of the cloud. We posit an interpretation in which an expanding shell compresses and concentrates the ambient gas and magnetic field. We argue that this compression results in the strengthening of the magnetic field, as we infer from the observations toward the interior of the ring.","['Galactic', 'Center,', 'Interstellar', 'Medium,', 'Dust', 'Continuum', 'Emission,', 'Polarimetry']","['USA', 'USA', 'USA', 'USA', 'USA', 'USA', 'USA', 'USA', 'USA', 'USA', 'USA']"
"We present our sixth set of results from our mid-infrared imaging survey of Milky Way Giant H ii regions with our detailed analysis of NGC 3603, the most luminous GH ii region in the Galaxy. We used imaging data from the FORCAST instrument on the Stratospheric Observatory For Infrared Astronomy (SOFIA) at 20 and 37 μ𝜇\muitalic_μm which mapped the central ∼similar-to\sim∼8.5′×8.5⁢′′8.5′\arcmin\times 8.5\arcmin′ × 8.5 ′ infrared-emitting area of NGC 3603 at a spatial resolution of ≲less-than-or-similar-to\lesssim≲3″″\arcsec″. Utilizing these SOFIA data in conjunction with multi-wavelength observations from the near-infrared to radio, including Spitzer-IRAC and Herschel-PACS archival data, we investigate the physical nature of individual infrared sources and sub-components within NGC 3603. For individual compact sources we used the multi-wavelength photometry data to construct spectral energy distributions (SEDs) and fit them with massive young stellar object (MYSO) SED models, and find 14 sources that are likely to be MYSOs. We also detect dust emission from the 3 massive proplyd candidates, as well as from the disk and outflow of the evolved blue supergiant, Sher 25. Utilizing multi-wavelength data, we derived luminosity-to-mass ratio and virial parameters for the star-forming clumps within NGC 3603, estimating their relative ages and finding that NGC 3603 is an older GH ii region overall, compared to our previously studied GH ii regions. We discuss how NGC 3603, which we categorize as a ‘cavity-type’ GH ii region, exhibits a more modest number of MYSOs and molecular clumps when compared to the ‘distributed-type’ GH ii regions that share similar Lyman continuum photon rates.","['ISM:', 'H ii regions — infrared: stars —– stars: formation —– infrared:', 'ISM: continuum —–', 'ISM: individual(W49A)']","['USA', 'USA', 'USA', 'USA']"
"An inner component misaligned from an outer component in a protoplanetary disk can result in the former casting shadows on the latter.
We present a new instance of shadowing on the outer disk around a very low mass star, ZZ Tau IRS. Through the analysis of near-infrared (NIR) archival data at λ=1.6𝜆1.6\lambda=1.6italic_λ = 1.6 μ𝜇\muitalic_μm acquired with the Wide Field Camera 3 on the Hubble Space Telescope, we identified brightness asymmetries in the top and bottom halves of the highly inclined outer disk, separated by a dark lane. The brighter sides in the top and bottom halves are on the opposite sides, which we attributed to shadows cast by a misaligned inner disk. Radiative transfer modeling of the system with a misaligned angle of 15 deg between the inner and outer disks well reproduced the observations.
Additionally, we found an elevated brightness temperature of 1212{}^{12}start_FLOATSUPERSCRIPT 12 end_FLOATSUPERSCRIPTCO (3-2) at r∼30similar-to𝑟30r\sim 30italic_r ∼ 30 au on the brighter side in NIR wavelengths in the top half disk, which can be explained by the shadowing effect too.
While the origin of the misaligned inner disk remains unclear, future monitoring observations to search for temporal variations in brightness asymmetries will likely provide useful clues.","['Protoplanetary disks (1300);', 'Planetary-disk interactions (2204);', 'Exoplanet formation (492);', 'M stars (985);', 'Near infrared astronomy (1093);', 'Hubble', 'Space', 'Telescope (756); stars: individual (ZZ', 'Tau', 'IRS)']","['Japan', 'Japan', 'Japan', 'Canada', 'Japan', 'Netherlands', 'Japan', 'R.O.C.', 'R.O.C.', 'R.O.C', 'R.O.C.']"
"Utilizing astrometric parameters sourced from Gaia Data Release 3 and radial velocities obtained from various spectroscopic surveys, we identify 519 high-velocity stars (HiVels) with a total velocity in the Galactocentric restframe greater than 70% of their local escape velocity under the Gala MilkyWayPotential. Our analysis reveals that the majority of these HiVels are metal-poor late-type giants, and we show 9 HiVels that are unbound candidates to the Galaxy with escape probabilities of 50%. To investigate the origins of these HiVels, we classify them into four categories and consider the impact of the Large Magellanic Cloud (LMC) potential on their backward-integration trajectories. Specifically, we find that one of the HiVels can track back to the Galactic Center, and three HiVels may originate from the Sagittarius dwarf spheroidal galaxy (Sgr dSph). Furthermore, some HiVels appear to be ejected from the Galactic disk, while others formed within the Milky Way or have an extragalactic origin. Given that the LMC has a significant impact on the orbits of Sgr dSph, we examine the reported HiVels that originate from the Sgr dSph, with a few of them passing within the half-light radius of the Sgr dSph.","['High-velocity stars(736) —', 'Stellar kinematics(1608) —', 'Stellar dynamics(1596) —', 'Sagittarius dwarf spheroidal galaxy(1423) —Large', 'Magellanic', 'Cloud(903)']","['China', 'China', 'China', 'China', 'P.R.China', 'China', 'P.R.China', 'P.R.China', 'China', 'P.R.China', 'China']"
"The Habitable Worlds Observatory will attempt to image Earth-sized planets in Habitable Zone orbits around nearby Sun-like stars. In this work we explore approximate analytic yield calculations for a future flagship direct imaging mission for a survey sample of uniformly distributed set of identical Sun-like stars. We consider the dependence of this exoplanet detection yield on factors such as η⊕subscript𝜂direct-sum\eta_{\oplus}italic_η start_POSTSUBSCRIPT ⊕ end_POSTSUBSCRIPT, telescope diameter, total on-sky time, orbital phase and separation, inner working angle, flux contrast, desired signal-to-noise ratio, spectral resolution, and other factors. We consider the impact on yield and survey efficiency in the absence of and with precursor knowledge of the Earth-size analog exoplanets. In particular, for precursor knowledge we assume the exoplanet orbital phase at the time of observation can be optimized so as to only image the Earth-size analog exoplanet when it is outside the inner working angle. We find that the yield of flagship direct imaging missions such as Habitable Worlds Observatory will be inner-working angle limited for the estimated exoplanet yields, and will not be impacted by precursor knowledge given our assumptions presented herein. However, we find that the survey efficiency will be enhanced by precursor knowledge. We benchmark our analytic approximations against detailed simulations for coronagraphs and starshades carried out for the HabEx and LUVOIR missions concept studies, and find consistent conclusions. Our analytic relations thus provide quick estimates and derivatives of the impact of key mission parameter choices on exo-Earth yield when considering design trades that can supplement existing computational simulations.",['planetary systems – techniques: direct imaging'],"['USA', 'USA', 'USA', 'USA', 'USA', 'USA', 'USA', '32901', '21218']"
"C/2019 Y4 (ATLAS) is an Oort cloud comet
with an orbital period of ∼similar-to\sim\,∼5895yryr\,{\rm yr}roman_yr.
Starting in March 2020, its nucleus underwent disintegration.
In order to investigate the gas and dust properties
of C/2019 Y4 (ATLAS) during its disintegration,
we obtained long-slit spectra at 3600–8700ÅÅ\,{\rm\AA}roman_Å
and B⁢V⁢R⁢I𝐵𝑉𝑅𝐼BVRIitalic_B italic_V italic_R italic_I multi-band images
with the Xinglong 2.16-Meter Telescope in April 2020.
Our observations revealed that C/2019 Y4 (ATLAS)
exhibited strong emission bands of CN, C22{}_{2}start_FLOATSUBSCRIPT 2 end_FLOATSUBSCRIPT, C33{}_{3}start_FLOATSUBSCRIPT 3 end_FLOATSUBSCRIPT,
and NH22{}_{2}start_FLOATSUBSCRIPT 2 end_FLOATSUBSCRIPT which are superimposed on
a dust scattering continuum,
typical of cometary spectra in the optical.
The production rates of CN, C22{}_{2}start_FLOATSUBSCRIPT 2 end_FLOATSUBSCRIPT, and C33{}_{3}start_FLOATSUBSCRIPT 3 end_FLOATSUBSCRIPT
derived using the Haser model
and the corresponding C22{}_{2}start_FLOATSUBSCRIPT 2 end_FLOATSUBSCRIPT/CN and C33{}_{3}start_FLOATSUBSCRIPT 3 end_FLOATSUBSCRIPT/CN ratios
suggest that C/2019 Y4 (ATLAS) is a “typical”
Oort cloud comet under the A’Hearn classification, although
it appears less dusty as revealed by the A⁢f⁢ρ𝐴𝑓𝜌Af\rhoitalic_A italic_f italic_ρ quantities.
Its dust-scattering reflectivity is slightly red,
with a gradient of ∼similar-to\sim\,∼5% per 103⁢Åsuperscript103Å10^{3}\,{\rm\AA}10 start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT roman_Å.
We model the reflectivity gradient in terms of porous dust
and find that the red color is accounted for by porous dust.","['Long period comets (933);', 'Comets (280);', 'Small solar system bodies (1469)']","['jfliu@nao.cas.cn', 'China', 'lia@missouri.edu', 'bin.yang@mail.udp.cl', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China']"
"Wireless powered communication (WPC) involves the integration of energy harvesting and data transmission. This allows devices to communicate without constant battery replacements or wired power sources. Reconfigurable intelligent surfaces (RISs) can dynamically manipulate radio signals. In this paper, we explore the use of active elements to mitigate double-fading challenges inherent in RIS-aided links. We enhance the reliability performance for an energy-constrained user by combining active RIS and WPC. The theoretical closed-form analysis, which includes transmission rate, harvested energy, and outage probability, provides valuable insights that inform parameter selection.","['Index', 'Terms: ', 'Wireless powered communication (WPC),', 'Active', 'RIS,', 'Outage probability.']","['gmail.com}', 'heejungyu@korea.ac.kr', 'al.boulogeorgos@ieee.org']"
"Generating explanations for
graph neural networks (GNNs) has been studied to
understand their behavior
in analytical tasks such
as graph classification.
Existing approaches
aim to understand
the overall results of 𝖦𝖭𝖭𝗌𝖦𝖭𝖭𝗌\mathsf{GNNs}sansserif_GNNs rather
than providing
explanations for specific class labels of interest,
and may return explanation structures that are hard to access, nor directly queryable. We propose 𝖦𝖵𝖤𝖷𝖦𝖵𝖤𝖷\mathsf{GVEX}sansserif_GVEX, a novel paradigm that
generates Graph Views for 𝖦𝖭𝖭𝖦𝖭𝖭\mathsf{GNN}sansserif_GNN EXplanation. (1) We design a two-tier explanation
structure called explanation
views. An explanation view
consists of a set of graph
patterns and a set of induced
explanation subgraphs.
Given a database 𝒢𝒢{\mathcal{G}}caligraphic_G of multiple graphs and a specific
class label l𝑙litalic_l assigned by a
GNN-based classifier ℳℳ{\mathcal{M}}caligraphic_M, it concisely describes
the fraction of
𝒢𝒢{\mathcal{G}}caligraphic_G that best explains
why l𝑙litalic_l is assigned
by ℳℳ{\mathcal{M}}caligraphic_M.
(2) We propose quality measures
and formulate an optimization
problem to compute optimal explanation views
for 𝖦𝖭𝖭𝖦𝖭𝖭\mathsf{GNN}sansserif_GNN explanation.
We show that the problem is
ΣP2subscriptsuperscriptΣ2𝑃\Sigma^{2}_{P}roman_Σ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_P end_POSTSUBSCRIPT-hard.
(3) We present two algorithms.
The first one follows an
explain-and-summarize strategy
that first
generates high-quality explanation subgraphs
which best explain
𝖦𝖭𝖭𝗌𝖦𝖭𝖭𝗌\mathsf{GNNs}sansserif_GNNs in terms of feature influence maximization, and
then performs a summarization
step to generate patterns.
We show that this strategy provides an approximation ratio of 1212\frac{1}{2}divide start_ARG 1 end_ARG start_ARG 2 end_ARG. Our second algorithm
performs a single-pass to
an input node stream in batches to
incrementally maintain explanation
views, having an anytime
quality guarantee of
1414\frac{1}{4}divide start_ARG 1 end_ARG start_ARG 4 end_ARG-approximation. Using
real-world benchmark data,
we experimentally demonstrate
the effectiveness, efficiency,
and
scalability of 𝖦𝖵𝖤𝖷𝖦𝖵𝖤𝖷\mathsf{GVEX}sansserif_GVEX. Through case studies, we showcase the practical applications of 𝖦𝖵𝖤𝖷𝖦𝖵𝖤𝖷\mathsf{GVEX}sansserif_GVEX.","['deep learning, graph neural networks, explainable', 'AI, graph views, data mining']","['UniversityHangzhouChina', 'UniversityAalborgDenmark', 'UniversityClevelandOhioUSA', 'UniversityAalborgDenmark', 'UniversityHangzhouChina', 'UniversityHangzhouChina']"
"Python has become one of the most popular programming languages for software development due to its simplicity, readability, and versatility. As the Python ecosystem grows, developers face increasing challenges in avoiding module conflicts, which occur when different packages have the same namespace modules. Unfortunately, existing work has neither investigated the module conflict comprehensively nor provided tools to detect the conflict.
Therefore, this paper systematically investigates the module conflict problem and its impact on the Python ecosystem.
We propose a novel technique called InstSimulator, which leverages semantics and installation simulation to achieve accurate and efficient module extraction. Based on this, we implement a tool called ModuleGuard to detect module conflicts for the Python ecosystem.
For the study, we first collect 97 MC issues, classify the characteristics and causes of these MC issues, summarize three different conflict patterns, and analyze their potential threats. Then, we conducted a large-scale analysis of the whole PyPI ecosystem (4.2 million packages) and GitHub popular projects (3,711 projects) to detect each MC pattern and analyze their potential impact. We discovered that module conflicts still impact numerous TPLs and GitHub projects.
This is primarily due to developers’ lack of understanding of the modules within their direct dependencies, not to mention the modules of the transitive dependencies.
Our work reveals Python’s shortcomings in handling naming conflicts and provides a tool and guidelines for developers to detect conflicts.","['Module', 'Conflict,', 'PyPI', 'Ecosystem,', 'Dependency', 'Graphs,', 'Namespace', 'Conflict,', 'Dependency', 'Resolution']","['UniversityHangzhouChina310000', 'UniversityHangzhouChina310000', 'UniversitySingapore', 'UniversitySingapore', 'UniversityHangzhouChina310000', 'UniversityHangzhouChina310000', 'UniversitySingapore']"
"High-dimensional vector similarity search (HVSS) is receiving a spotlight as a powerful tool for various data science and AI applications. As vector data grows larger, in-memory indexes become extremely expensive because they necessitate substantial expansion of main memory resources. One possible solution is to use disk-based implementation, which stores and searches vector data in high-performance devices like NVMe SSDs. However, HVSS for data segments is still challenging in vector databases, where one machine has multiple segments for system features (like scaling) purposes. In this setting, each segment has limited memory and disk space, so HVSS on the data segment needs to balance accuracy, efficiency, and space cost. Existing disk-based methods are sub-optimal because they do not consider all these requirements together.
In this paper, we present 𝚂𝚝𝚊𝚛𝚕𝚒𝚗𝚐𝚂𝚝𝚊𝚛𝚕𝚒𝚗𝚐\mathtt{Starling}typewriter_Starling, an I/O-efficient disk-resident graph index framework that optimizes data layout and search strategy in the segment. It has two main components: (1) a data layout that includes an in-memory navigation graph and a reordered disk-based graph with locality enhancement, which reduces the search path length and disk bandwidth wastage; and (2) a block search strategy that minimizes expensive disk I/Os when executing a vector query. We conduct extensive experiments to verify 𝚂𝚝𝚊𝚛𝚕𝚒𝚗𝚐𝚂𝚝𝚊𝚛𝚕𝚒𝚗𝚐\mathtt{Starling}typewriter_Starling’s effectiveness, efficiency, and scalability. On a data segment with 2GB memory and 10GB disk capacity, 𝚂𝚝𝚊𝚛𝚕𝚒𝚗𝚐𝚂𝚝𝚊𝚛𝚕𝚒𝚗𝚐\mathtt{Starling}typewriter_Starling can maintain up to 33 million vectors in 128 dimensions, and serve HVSS with more than 0.9 average precision and top-10 recall rate, and latency of under 1 millisecond. The results show that 𝚂𝚝𝚊𝚛𝚕𝚒𝚗𝚐𝚂𝚝𝚊𝚛𝚕𝚒𝚗𝚐\mathtt{Starling}typewriter_Starling exhibits 43.9×\times× higher throughput with 98% lower query latency than state-of-the-art methods under the same accuracy.","['high-dimensional vector, approximate nearest neighbor search, range search, disk-based graph index, block shuffling']","['UniversityChina', 'ZillizChina', 'LabChina', 'UniversityChina', 'UniversityChina', 'UniversityChina', 'UniversityChina', 'UniversityChina', 'ZillizChina', 'ZillizChina']"
"Machine learning models are increasingly used in critical decision-making applications. However, these models are susceptible to replicating or even amplifying bias present in real-world data. While there are various bias mitigation methods and base estimators in the literature, selecting the optimal model for a specific application remains challenging.
This paper focuses on binary classification and proposes FairGridSearch, a novel framework for comparing fairness-enhancing models. FairGridSearch enables experimentation with different model parameter combinations and recommends the best one.
The study applies FairGridSearch to three popular datasets (Adult, COMPAS, and German Credit) and analyzes the impacts of metric selection, base estimator choice, and classification threshold on model fairness.
The results highlight the significance of selecting appropriate accuracy and fairness metrics for model evaluation. Additionally, different base estimators and classification threshold values affect the effectiveness of bias mitigation methods and fairness stability respectively, but the effects are not consistent across all datasets.
Based on these findings, future research on fairness in machine learning should consider a broader range of factors when building fair models, going beyond bias mitigation methods alone.","['Index', 'Terms: ', 'Algorithmic fairness, algorithmic bias, bias mitigation, fairness in machine learning,', 'AI ethics']","['shih-chi.ma@th-wildau.de', 'tatiana.ermakova@htw-berlin.de', 'benjamin.fabian@th-wildau.de']"
"Digital twins are becoming increasingly popular across many industries for real-time data streaming, processing, and visualization. They allow stakeholders to monitor, diagnose, and optimize assets. Emerging technologies used for immersive visualization, such as virtual reality, open many new possibilities for intuitive access and monitoring of remote assets through digital twins. This is specifically relevant for floating wind farms, where access is often limited. However, the integration of data from multiple sources and access through different devices including virtual reality headsets can be challenging. In this work, a data integration framework for static and real-time data from various sources on the assets and their environment is presented that allows collecting and processing of data in Python and deploying the data in real-time through Unity on different devices, including virtual reality headsets. The integration of data from terrain, weather, and asset geometry is explained in detail. A real-time data stream from the asset to the clients is implemented and reviewed, and instructions are given on the code required to connect Python scripts to any Unity application across devices. The data integration framework is implemented for a digital twin of a floating wind turbine and an onshore wind farm, and the potential for future research is discussed.","['Index', 'Terms: \ndata integration framework, digital twin, wind energy']","['florian.stadtmann@ntnu.no', 'harypm@stud.ntnu.no', 'adil.rasheed@ntnu.no']"
"Recent astrophysical mass inferences of compact stars (CSs) HESS J1731-347
and PSR J0952-0607, with extremely small and large masses respectively,
as well as the measurement of the neutron skin of Ca in the CREX experiment
challenge and constrain the models of dense matter. In this work, we examine
the concept of hybrid stars - objects containing quark cores surrounded by
nucleonic envelopes - as models that account for these new data along with
other (multimessenger) inferences. We employ a family of 81 nucleonic equations
of state (EoSs) based on covariant density functional with variable skewness
and slope of symmetry energy at saturation density and a constant speed-of-sound
EoS for quark matter. For each nucleonic EoS, a family of hybrid star EoS is
generated by varying the transition density from nucleonic to quark matter,
the density jump at the transition, and the speed-of-sound. These models are
tested against the data from GW170817 and J1731-347, which favor low-density
soft EoS and PSR J0592-0607 and J0740+6620, which require high-density stiff EoS.
We then examine the occurrence of twin configurations and quantify the ranges of
masses and radii that they can possess. It is shown that including J1731-347 data
favors EoS models which predict low-mass twin stars with masses
M≲1.3⁢M⊙less-than-or-similar-to𝑀1.3subscript𝑀direct-productM\lesssim 1.3\,M_{\odot}italic_M ≲ 1.3 italic_M start_POSTSUBSCRIPT ⊙ end_POSTSUBSCRIPT that can be realized if the deconfinement transition
density is low. If combined with a large speed-of-sound in quark matter such models
allow for maximum masses of hybrid stars in the range 2.02.02.02.0-2.6⁢M⊙2.6subscript𝑀direct-product2.6\,M_{\odot}2.6 italic_M start_POSTSUBSCRIPT ⊙ end_POSTSUBSCRIPT.","['Subject headings:', 'Compact objects (228);', 'Neutron stars (1108);', 'Nuclear astrophysics (1129);', 'High energy astrophysics (739)']",['alford@physics.wustl.edu']
"As the horizon of intelligent transportation expands with the evolution of Automated Driving Systems (ADS), ensuring paramount safety becomes more imperative than ever. Traditional risk assessment methodologies, primarily crafted for human-driven vehicles, grapple to adequately adapt to the multifaceted, evolving environments of ADS. This paper introduces a framework for real-time Dynamic Risk Assessment (DRA) in ADS, harnessing the potency of Artificial Neural Networks (ANNs).
Our proposed solution transcends these limitations, drawing upon ANNs, a cornerstone of deep learning, to meticulously analyze and categorize risk dimensions using real-time On-board Sensor (OBS) data. This learning-centric approach not only elevates the ADS’s situational awareness but also enriches its understanding of immediate operational contexts. By dissecting OBS data, the system is empowered to pinpoint its current risk profile, thereby enhancing safety prospects for onboard passengers and the broader traffic ecosystem.
Through this framework, we chart a direction in risk assessment, bridging the conventional voids and enhancing the proficiency of ADS.
By utilizing ANNs, our methodology offers a perspective, allowing ADS to adeptly navigate and react to potential risk factors, ensuring safer and more informed autonomous journeys.","['Index', 'Terms: ', 'Dynamic', 'Risk', 'Assessment,', 'Automated', 'Driving', 'System,', 'Artificial', 'Neural', 'Network']","['apatel@rptu.de', 'peter.liggesmeyer@rptu.de']"
"In this research, we delve into the localization patterns of fermionic fields within a braneworld setting, employing a modified gravity model denoted as f⁢(Q)𝑓𝑄f(Q)italic_f ( italic_Q ). Our investigation revolves around two specific models, f1⁢(Q)=Q+k⁢Qnsubscript𝑓1𝑄𝑄𝑘superscript𝑄𝑛f_{1}(Q)=Q+kQ^{n}italic_f start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( italic_Q ) = italic_Q + italic_k italic_Q start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT and f2⁢(Q)=Q+k1⁢Q2+k2⁢Q3subscript𝑓2𝑄𝑄subscript𝑘1superscript𝑄2subscript𝑘2superscript𝑄3f_{2}(Q)=Q+k_{1}Q^{2}+k_{2}Q^{3}italic_f start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( italic_Q ) = italic_Q + italic_k start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT italic_Q start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + italic_k start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT italic_Q start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT
, where we systematically vary the parameters n𝑛nitalic_n and k1,2subscript𝑘12k_{1,2}italic_k start_POSTSUBSCRIPT 1 , 2 end_POSTSUBSCRIPT.
Through an in-depth analysis encompassing the effective potential, massless, and massive modes, we elucidate how deviations from the conventional symmetric teleparallel equivalent of general relativity (STEGR) gravity impact the localization of fermionic fields. To ensure greater precision, our methodology integrates probabilistic measures such as Shannon entropy and relative probability.
Moreover, we gauge the stability of these models employing differential configurational entropy (DCE), revealing a compelling correlation between the most stable configurations and the emergence of novel structures within the background scalar field.
This work significantly contributes to our understanding of the gravitational modifications’ intricate influence on fermionic field localization within braneworld scenarios. By shedding light on these dynamics, it advances the broader comprehension of the interplay between gravity modifications and fermionic field behaviors in these theoretical frameworks.","['f\u2062(Q)𝑓𝑄f(Q)italic_f ( italic_Q ) gravity;', 'Braneworld model;', 'Configurational entropy;', 'Fermion localization;', 'Shannon entropy.']","['China.', 'Brazil.', 'China.', 'Mexico.']"
"Deep reinforcement learning (DRL) remains challenging in tasks with sparse rewards. These sparse rewards often only indicate whether the task is partially or fully completed, meaning that many exploration actions must be performed before the agent obtains useful feedback. Hence, most existing DRL algorithms fail to learn feasible policies within a reasonable time frame. To overcome this problem, we develop an approach that exploits offline demonstration trajectories for faster and more efficient online RL in sparse reward settings. Our key insight is that by regarding offline demonstration trajectories as guidance, instead of imitating them, our method learns a policy whose state-action visitation marginal distribution matches that of offline demonstrations. Specifically, we introduce a novel trajectory distance based on maximum mean discrepancy (MMD) and formulate policy optimization as a distance-constrained optimization problem. Then, we show that this distance-constrained optimization problem can be reduced into a policy-gradient algorithm with shaped rewards learned from offline demonstrations. The proposed algorithm is evaluated on extensive discrete and continuous control tasks with sparse and deceptive rewards. The experimental results indicate that our proposed algorithm is significantly better than the baseline methods regarding diverse exploration and learning the optimal policy.","['Index', 'Terms: \ndeep reinforcement learning, sparse rewards, efficient exploration, offline demonstrations']","['wgj@buaa.edu.cn', 'faguo@buaa.edu.cn', 'xiao.zh@buaa.edu.cn']"
"An article usually includes an abstract, a concise summary of the work
covered at length in the main body of the article. It is used for
secondary publications and for information retrieval purposes.",['Suggested keywords'],"['[', 'address', 'address']"
"During solar flares, spectral lines formed in the photosphere have been shown to exhibit changes to their profiles despite the challenges of energy transfer to these depths. Recent work has shown that deep-forming spectral lines are subject to significant contributions from regions above the photosphere throughout the flaring period, resulting in a composite emergent intensity profile from multiple layers of the atmosphere. We employ radiative-hydrodynamic and radiative transfer calculations to simulate the response of the solar/stellar atmosphere to electron beam heating and synthesize spectral lines of Fe I to investigate the line-of-sight velocity fields information available from Doppler shifts of the emergent intensity profile. By utilizing the contribution function to deconstruct the line profile shape into its constituent sources, we show that variations in the line profiles are primarily caused by changes in the chromosphere. Up-flows in this region were found to create blueshifts or ”false” redshifts in the line core dependent on the relative contribution of the chromosphere compared to the photosphere. In extreme solar and stellar flare scenarios featuring explosive chromospheric condensations, red-shifted transient components can dominate the temporal evolution of the profile shape, requiring a tertiary component consideration to fully characterize. We conclude that deep-forming lines require a multi-component understanding and treatment, with different regions of the spectral line being useful for probing individual regions of the atmosphere’s velocity flows.","['Solar flares,', 'Solar photosphere,', 'Solar activity,', 'Radiative transfer simulations']","['UK', 'USA', 'USA', 'USA']"
"Help-seeking is a critical way that students learn new concepts, acquire new skills, and get unstuck when problem-solving in their computing courses. The recent proliferation of generative AI tools, such as ChatGPT, offers students a new source of help that is always available on-demand. However, it is unclear how this new resource compares to existing help-seeking resources along dimensions of perceived quality, latency, and trustworthiness. In this paper, we investigate the help-seeking preferences and experiences of computing students now that generative AI tools are available to them. We collected survey data (n=47) and conducted interviews (n=8) with computing students. Our results suggest that although these models are being rapidly adopted, they have not yet fully eclipsed traditional help resources. The help-seeking resources that students rely on continue to vary depending on the task and other factors. Finally, we observed preliminary evidence about how help-seeking with generative AI is a skill that needs to be developed, with disproportionate benefits for those who are better able to harness the capabilities of LLMs. We discuss potential implications for integrating generative AI into computing classrooms and the future of help-seeking in the era of generative AI.","['Generative', 'AI,', 'ChatGPT, computing education, help-seeking']","['PAUSA', 'PAUSA', 'PAUSA', 'PAUSA', 'PAUSA', 'States']"
"LinkedIn is the largest professional network in the world. As such, it can serve to build bridges between practitioners, whose daily work is software engineering (SE), and researchers, who work to advance the field of software engineering. We know that such a metaphorical bridge exists: SE research findings are sometimes shared on LinkedIn and commented on by software practitioners. Yet, we do not know what state the bridge is in. Therefore, we quantitatively and qualitatively investigate how SE practitioners and researchers approach each other via public LinkedIn discussions and what both sides can contribute to effective science communication. We found that a considerable proportion of LinkedIn posts on SE research are written by people who are not the paper authors (39%). Further, 71% of all comments in our dataset are from people in the industry, but only every second post receives at least one comment at all. Based on our findings, we formulate concrete advice for researchers and practitioners to make sharing new research findings on LinkedIn more fruitful.","['Science', 'Communication,', 'LinkedIn,', 'Software', 'Engineering,', 'Outreach,', 'Knowledge', 'Sharing,', 'Research', 'Impact,', 'Community', 'Engagement']","['UniversitySaarbrückenGermany', 'Netherlands']"
"Serverless computing has emerged as an attractive paradigm due to the efficiency of development and the ease of deployment without managing any underlying infrastructure. Nevertheless, serverless computing approaches face numerous challenges to unlock their full potential in hybrid environments. To gain a deeper understanding and firsthand knowledge of serverless computing in edge-cloud deployments, we review the current state of open-source serverless platforms and compare them based on predefined requirements. We then design and implement a serverless computing platform with a novel edge orchestration technique that seamlessly deploys serverless functions across the edge and cloud environments on top of the Knative serverless platform. Moreover, we propose an offloading strategy for edge environments and four different functions for experimentation and showcase the performance benefits of our solution. Our results demonstrate that such an approach can efficiently utilize both cloud and edge resources by dynamically offloading functions from the edge to the cloud during high activity, while reducing the overall application latency and increasing request throughput compared to an edge-only deployment.","['Serverless', 'Computing,', 'Edge-Cloud', 'Continuum,', 'Knative,', 'Offloading']","['Netherlands', 'Netherlands', 'Netherlands', 'Netherlands', 'Netherlands', 'Netherlands']"
"Recent X-ray studies of starburst galaxies have found that Charge eXchange (CX) commonly occurs between the outflowing hot plasma and cold gas, possibly from swept-up clouds.
However, the total CX flux and the regions where CX occurs have been poorly understood.
We present an analysis of the XMM-Newton observations of M82, a prototype starburst galaxy, aiming to investigate these key properties of the CX emisssion.
We have used a blind source separation method in the image analysis with the CCD data which identified a component with the enhanced O-K lines expected from the CX process.
Analyzing the RGS spectra from the region identified by the image analysis, we have detected a high forbidden-to-resonance ratio in the O vii Heα𝛼\alphaitalic_α triplet as well as several emission lines from K-shell transitions of C, N, and O enhanced in the CX process.
The CX is less responsible for the emission line of Ne and Mg and the accurate estimation of the CX contribution is confirmed to be crucial in measuring chemical abundances.
The temperature of the plasma as electron receiver in the CX process is significantly lower compared to that of the plasma components responsible for most of the X-rays.
From the low temperature and an estimation of the CX emitting volume, we find that the CX primarily occurs in a limited region at the interface of the plasma and gas whose temperature rapidly decreases due to thermal conduction.","['galaxies: individual (M82) – galaxies: starburst –', 'X-rays: galaxies']","['USA', 'USA', 'USA', 'USA', 'USA', 'USA']"
"Graph Neural Networks (GNNs) have achieved great success in Knowledge Graph Completion (KGC) by modelling how entities and relations interact in recent years. However, the explanation of the predicted facts has not caught the necessary attention. Proper explanations for the results of GNN-based KGC models increase model transparency and help researchers develop more reliable models.  Existing practices for explaining KGC tasks rely on instance/subgraph-based approaches, while in some scenarios, paths can provide more user-friendly and interpretable explanations. Nonetheless, the methods for generating path-based explanations for KGs have not been well-explored. To address this gap, we propose Power-Link, the first path-based KGC explainer that explores GNN-based models. We design a novel simplified graph-powering technique, which enables the generation of path-based explanations with a fully parallelisable and memory-efficient training scheme. We further introduce three new metrics for quantitative evaluation of the explanations, together with a qualitative human evaluation. Extensive experiments demonstrate that Power-Link outperforms the SOTA baselines in interpretability, efficiency, and scalability.","['Graph', 'Neural', 'Networks,', 'Knowledge', 'Graph', 'Completion,', 'Model', 'Transparency,', 'Model', 'Explanation']","['Ltd.BeijingChina', 'Kingdom', 'Kingdom', 'Kingdom', '(Guangzhou)GuangzhouChina']"
"This paper concerns the training of a single-layer morphological perceptron using disciplined convex-concave programming (DCCP). We introduce an algorithm referred to as K𝐾Kitalic_K-DDCCP, which combines the existing single-layer morphological perceptron (SLMP) model proposed by Ritter and Urcid with the weighted disciplined convex-concave programming (WDCCP) algorithm by Charisopoulos and Maragos. The proposed training algorithm leverages the disciplined convex-concave procedure (DCCP) and formulates a non-convex optimization problem for binary classification. To tackle this problem, the constraints are expressed as differences of convex functions, enabling the application of the DCCP package. The experimental results confirm the effectiveness of the K𝐾Kitalic_K-DDCCP algorithm in solving binary classification problems. Overall, this work contributes to the field of morphological neural networks by proposing an algorithm that extends the capabilities of the SLMP model.","['Index', 'Terms: ', 'Single-Layer', 'Morphological', 'Perceptron,', 'Disciplined', 'Convex-Concave', 'Programming,', 'Dendritic', 'Structures,', 'Binary', 'Classification,', 'Non-Convex', 'Optimization.']","['iarasilva@utfpr.edu.br', 'valle@ime.unicamp.br']"
"We report on studies of Classical Nova (CN) explosions where we follow the
evolution of thermonuclear runaways (TNRs) on oxygen-neon (ONe) white dwarfs (WDs).
Using NOVA, a one-dimensional hydrodynamic computer code, we accrete
Solar matter until the TNR is ongoing and then switch to a mixed composition. This approach is guided by the results of multi-dimensional studies of TNRs in WDs which find that sufficient mixing with WD core material occurs after the TNR is well underway, and levels of enrichment of the CNONeMg elements are reached that
agree with observations of CN ejecta abundances. Because the amount of accreted material is inversely proportional to the oxygen abundance,
by first accreting Solar matter, the amount of accreted material is larger than in those simulations with an initially enriched composition.
We vary the mass of the WD (from 0.6 M⊙direct-product{}_{\odot}start_FLOATSUBSCRIPT ⊙ end_FLOATSUBSCRIPT to 1.35 M⊙direct-product{}_{\odot}start_FLOATSUBSCRIPT ⊙ end_FLOATSUBSCRIPT) and the composition of the mixed materials. Our results show large enrichments of 77{}^{7}start_FLOATSUPERSCRIPT 7 end_FLOATSUPERSCRIPTBe in the ejected gases implying that ONe CNe and CO CNe  (Starrfield et al., 2020) may be responsible for a significant fraction (∼similar-to\sim∼ 100 M⊙direct-product{}_{\odot}start_FLOATSUBSCRIPT ⊙ end_FLOATSUBSCRIPT) of the galactic 77{}^{7}start_FLOATSUPERSCRIPT 7 end_FLOATSUPERSCRIPTLi (∼similar-to\sim∼1000 M⊙direct-product{}_{\odot}start_FLOATSUBSCRIPT ⊙ end_FLOATSUBSCRIPT). The production of 2222{}^{22}start_FLOATSUPERSCRIPT 22 end_FLOATSUPERSCRIPTNa and 2626{}^{26}start_FLOATSUPERSCRIPT 26 end_FLOATSUPERSCRIPTAl in CN explosions and the γ𝛾\gammaitalic_γ-ray emission predicted by our simulations is discussed. The WDs in all our simulations eject less material than they accrete and we predict that the WD is growing in mass as a consequence of the CN outburst. ONe CNe, therefore, may be an important channel for accretion induced collapse (AIC) events.","['Cataclysmic variable stars (203) –', 'Novae (1127) –', 'Recurrent', 'Novae (1366) –', 'Galaxy chemical evolution (580) –', 'Galaxy abundances (574) –', 'Milky', 'Way', 'Galaxy (1054)']","['starrfield@asu.edu', 'USA', 'USA', '27599-3255', 'USA', '37831-6354', '37996', '55455', '85721', '43210', '2023']"
"We investigate the properties of voids and void galaxies in the TNG300 simulation. Using a luminous galaxy catalog and a spherical void finding algorithm,
we identify 5,078 voids at redshift z=0𝑧0z=0italic_z = 0.
The voids cover 83% of the simulation volume and have a median radius of 4.4⁢h−1⁢Mpc4.4superscriptℎ1Mpc4.4h^{-1}\rm{Mpc}4.4 italic_h start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT roman_Mpc.
We identify two populations of field galaxies based on whether the galaxies reside within a void (“void galaxies”; 75,220 objects) or outside a void (“non-void galaxies”; 527,454 objects).
Within the voids, mass does not directly trace light. Instead, the mean radial underdensity profile as defined by the locations of void galaxies is systematically lower than the mean radial underdensity profile as defined by the dark matter (i.e., the voids are more “devoid” of galaxies than they are of mass).
Within the voids, the integrated underdensity profiles of the dark matter and the galaxies are independent of the local background density (i.e., voids-in-voids vs. voids-in-clouds). Beyond the void radii, however, the integrated underdensity profiles of both the dark matter and the galaxies exhibit strong dependencies on the local background density.
Compared to non-void galaxies, void galaxies are on average younger, less massive, bluer in color, less metal enriched, and have smaller radii.
In addition, the specific star formation rates of void galaxies are ∼20similar-toabsent20\sim 20∼ 20% higher than non-void galaxies and, in the case of galaxies with central supermassive black holes with MBH≳3×106⁢h−1⁢M⊙greater-than-or-equivalent-tosubscript𝑀BH3superscript106superscriptℎ1subscript𝑀direct-productM_{\rm BH}\gtrsim 3\times 10^{6}h^{-1}M_{\odot}italic_M start_POSTSUBSCRIPT roman_BH end_POSTSUBSCRIPT ≳ 3 × 10 start_POSTSUPERSCRIPT 6 end_POSTSUPERSCRIPT italic_h start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT italic_M start_POSTSUBSCRIPT ⊙ end_POSTSUBSCRIPT, the fraction of active void galaxies is ∼25similar-toabsent25\sim 25∼ 25% higher than active non-void galaxies.","['Large-scale structure of the', 'Universe (902) –', 'Voids (1779) –', 'Magnetohydrodynamical simulations (1966) –', 'Galaxy evolution (594)']","['USA', 'USA', 'USA']"
"Millimeter wave (mmWave) has been recognized as one of key technologies for 5G and beyond networks due to its potential to enhance channel bandwidth and network capacity.
The use of mmWave for various applications including vehicular communications has been extensively discussed. However, applying mmWave to vehicular communications
faces challenges of high mobility nodes and narrow coverage along the mmWave beams. Due to high mobility in dense networks, overlapping beams can cause strong interference which leads to performance degradation. As a remedy, beam switching capability in mmWave can be utilized.
Then,
frequent beam switching and cell change become inevitable to manage interference, which increase computational and signalling complexity.
In order to deal with the complexity in interference control, we develop a new strategy
called Multi-Agent Context Learning (MACOL), which utilizes Contextual Bandit
to manage interference while allocating mmWave beams to serve vehicles in the network.
Our approach demonstrates that by leveraging knowledge of neighbouring beam status,
the machine learning agent can
identify and avoid potential interfering transmissions to other ongoing transmissions.
Furthermore, we show that even under
heavy traffic loads, our
proposed MACOL
strategy
is able to maintain
low interference levels
at around 10%.","['Index', 'Terms: ', 'Vehicular networks, mmWave, beam management, machine learning, multi-armed bandit.']",['m.shojafar}@surrey.ac.uk']
"A fraction of core-collapse supernovae (SNe) with signs of interaction with a dense circumstellar matter are preceded by bright precursor emission. While the precursors are likely caused by a mass ejection before core-collapse, their mechanism to power energetic bursts, sometimes reaching 1048superscript104810^{48}10 start_POSTSUPERSCRIPT 48 end_POSTSUPERSCRIPT–1049⁢ergsuperscript1049erg10^{49}\ {\rm erg}10 start_POSTSUPERSCRIPT 49 end_POSTSUPERSCRIPT roman_erg that are larger than the binding energies of red supergiant envelopes, is still under debate. Remarkably, such a huge energy-deposition should result in an almost complete envelope ejection and hence a strong sign of interaction, but the observed SNe with precursors show in fact typical properties among the interacting SNe. More generally, the observed luminosity of 1040−42⁢erg⁢s−1superscript104042ergsuperscripts110^{40-42}\,\rm erg\,s^{-1}10 start_POSTSUPERSCRIPT 40 - 42 end_POSTSUPERSCRIPT roman_erg roman_s start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT is shown to be challenging for a single SN progenitor. To resolve these tensions, we propose a scenario where the progenitor is in a binary system with a compact object (CO), and an outburst from the star leads to a super-Eddington accretion onto the CO. We show that for sufficiently short separations, outbursts with moderate initial kinetic energies of 1046superscript104610^{46}10 start_POSTSUPERSCRIPT 46 end_POSTSUPERSCRIPT–1047superscript104710^{47}10 start_POSTSUPERSCRIPT 47 end_POSTSUPERSCRIPT erg can be energized by the accreting CO so that their radiative output can be consistent with the observed precursors. We discuss the implications of our model in relation to CO binaries detectable with Gaia and gravitational wave detectors.","['Eruptive phenomena;', 'Stellar mass-loss;', 'Circumstellar matter;', 'Core-collapse supernovae']","['USA', 'Japan', 'Japan', 'Japan', 'USA', 'USA', 'USA']"
"Panspermia is the hypothesis that life originated on Earth from the bombardment of foreign interstellar ejecta harboring polyextremophile microorganisms. Since the 2017 discovery of the comet-like body ‘Oumuamua (1I/2017 U1) by the Pans-STARRS telescope, various studies have re-examined panspermia based on updated number density models that accommodate for ‘Oumuamua’s properties. By utilizing ‘Oumuamua’s properties as an anchor, we estimate the mass and number density of ejecta in the ISM (ρmsubscript𝜌𝑚\rho_{m}italic_ρ start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT [kg⋅⋅\cdot⋅au]−3{}^{-3}]start_FLOATSUPERSCRIPT - 3 end_FLOATSUPERSCRIPT ] and ρnsubscript𝜌𝑛\rho_{n}italic_ρ start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT [au]−3{}^{-3}]start_FLOATSUPERSCRIPT - 3 end_FLOATSUPERSCRIPT ]). We build upon prior work by first accounting for the minimum ejecta size to shield microbes from supernova radiation. Second, we estimate the total number of impact events Cnsubscript𝐶𝑛C_{n}italic_C start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT on Earth after its formation and prior to the emergence of life (≈\approx≈ 0.80.80.80.8 Gyr). We derive a conditional probability relation for the likelihood of panspermia for Earth specifically of <10−5absentsuperscript105<10^{-5}< 10 start_POSTSUPERSCRIPT - 5 end_POSTSUPERSCRIPT, given a number of factors including fBsubscript𝑓𝐵f_{B}italic_f start_POSTSUBSCRIPT italic_B end_POSTSUBSCRIPT, the fraction of ejecta harboring extremophiles and other factors that are poorly constrained. However, we find that panspermia is a plausible potential life-seeding mechanism for (optimistically) up to ∼105similar-toabsentsuperscript105\sim 10^{5}∼ 10 start_POSTSUPERSCRIPT 5 end_POSTSUPERSCRIPT of the ∼109similar-toabsentsuperscript109\sim 10^{9}∼ 10 start_POSTSUPERSCRIPT 9 end_POSTSUPERSCRIPT Earth-sized habitable zone worlds in our Galaxy.","['‘Oumuamua;', 'Panspermia;', 'Extremophiles;', 'Supernova radiation;', 'ISM ejecta number density;', 'Impact events']","['USA', 'USA', 'USA']"
"We study a relativistic collisionless electron-positron shock propagating
into an unmagnetized ambient medium using 2D particle-in-cell
simulations of unprecedented duration and size. The shock generates
intermittent magnetic structures of increasingly larger size as the simulation progresses.
Toward the end of our simulation, at around 26,000 plasma times,
the magnetic coherence scale approaches λ∼100similar-to𝜆100\lambda\sim 100italic_λ ∼ 100 plasma skin depths, both
ahead and behind the shock front. We anticipate a continued growth of λ𝜆\lambdaitalic_λ beyond
the time span of our simulation, as long as the shock accelerates particles to increasingly higher energies.
The post-shock field is concentrated in localized patches, which maintain a
local magnetic energy fraction εB∼0.1similar-tosubscript𝜀𝐵0.1\varepsilon_{B}\sim 0.1italic_ε start_POSTSUBSCRIPT italic_B end_POSTSUBSCRIPT ∼ 0.1.
Particles randomly sampling the downstream fields
spend most of their time in low field regions (εB≪0.1much-less-thansubscript𝜀𝐵0.1\varepsilon_{B}\ll 0.1italic_ε start_POSTSUBSCRIPT italic_B end_POSTSUBSCRIPT ≪ 0.1),
but emit a large fraction of the synchrotron power in the localized patches
with strong fields (εB∼0.1similar-tosubscript𝜀𝐵0.1\varepsilon_{B}\sim 0.1italic_ε start_POSTSUBSCRIPT italic_B end_POSTSUBSCRIPT ∼ 0.1).
Our results have important implications for models of gamma-ray burst afterglows.","['High energy astrophysics (739);', 'Shocks (2086);', 'Non-thermal radiation\nsources (1119);', 'Plasma astrophysics (1261);', 'Gamma-ray bursts (629)']","['Belgium', 'USA', 'USA', 'USA', 'USA']"
"In this paper we propose to quantify execution time variability of programs using statistical dispersion parameters.
We show how the execution time variability can be exploited in mixed criticality real-time systems.
We propose a heuristic to compute the execution time budget to be allocated to each low criticality real-time task according to its execution time variability. We show using experiments and simulations
that the proposed heuristic reduces the probability of exceeding the allocated budget compared to algorithms which do not take into account the execution time variability parameter.","['Index', 'Terms: ', 'Mixed criticality systems, execution time variability, statistical dispersion parameters.']",['France']
"Our understanding of the Universe breaks down for very small spacetime intervals, corresponding to an extremely high level of granularity (and energy), commonly referred to as the “Planck scale”. At this fundamental level, there are attempts of describing physics in terms of interacting automata that perform classical, deterministic computation. On one hand, various mathematical arguments have already illustrated how quantum laws (which describe elementary particles and interactions) could in principle arise as low-granularity approximations of automata-based systems. On the other hand, understanding how such systems might give rise to relativistic laws (which describe spacetime and gravity) remains a major problem. I explain here a few ideas that seem crucial for overcoming this problem, along with related algorithmic challenges that need to be addressed. Giving emphasis to meaningful computational counterparts of locality and general covariance, I outline basic ingredients of a distributed communication-rewiring protocol that would allow us to construct multi-automaton models that are viable from a relativistic perspective. I also explain how viable models can be evaluated using a variety of criteria, and discuss related aspects pertaining to the falsifiability and plausibility of the automata paradigm.","['Strict locality, distributed computation, causal sets, asynchronous communication, pregeometry, multi-agent systems, superdeterminism, automata models.']",['University']
"Global financial crime activity is driving demand for machine learning solutions in fraud prevention. However, prevention systems are commonly serviced to financial institutions in isolation, and few provisions exist for data sharing due to fears of unintentional leaks and adversarial attacks. Collaborative learning advances in finance are rare, and it is hard to find real-world insights derived from privacy-preserving data processing systems. In this paper, we present a collaborative deep learning framework for fraud prevention, designed from a privacy standpoint, and awarded at the recent PETs Prize Challenges. We leverage latent embedded representations of varied-length transaction sequences, along with local differential privacy, in order to construct a data release mechanism which can securely inform externally hosted fraud and anomaly detection models. We assess our contribution on two distributed data sets donated by large payment networks, and demonstrate robustness to popular inference-time attacks, along with utility-privacy trade-offs analogous to published work in alternative application domains.","['Index', 'Terms: \ndeep learning, privacy, financial systems']","['Kingdom', 'Kingdom', 'Kingdom', 'derek.mcauley@nottingham.ac.uk']"
"Federated learning (FL) as one of the novel branches of distributed machine learning (ML), develops global models through a private procedure without direct access to local datasets. However, access to model updates (e.g. gradient updates in deep neural networks) transferred between clients and servers can reveal sensitive information to adversaries. Differential privacy (DP) offers a framework that gives a privacy guarantee by adding certain amounts of noise to parameters. This approach, although being effective in terms of privacy, adversely affects model performance due to noise involvement. Hence, it is always needed to find a balance between noise injection and the sacrificed accuracy. To address this challenge, we propose adaptive noise addition in FL which decides the value of injected noise based on features’ relative importance. Here, we first propose two effective methods for prioritizing features in deep neural network models and then perturb models’ weights based on this information. Specifically, we try to figure out whether the idea of adding more noise to less important parameters and less noise to more important parameters can effectively save the model accuracy while preserving privacy. Our experiments confirm this statement under some conditions. The amount of noise injected, the proportion of parameters involved, and the number of global iterations can significantly change the output. While a careful choice of parameters by considering the properties of datasets can improve privacy without intense loss of accuracy, a bad choice can make the model performance worse.","['Index', 'Terms: ', 'Federated', 'Learning,', 'Differential privacy,', 'Feature importance,', 'Deep neural networks']","['mtalaei@bu.edu1', 'iman.izadi@iut.ac.ir']"
"The advent of foundation models (FMs) as an emerging suite of AI techniques has struck a wave of opportunities in computational healthcare. The interactive nature of these models, guided by pre-training data and human instructions, has ignited a data-centric AI paradigm that emphasizes better data characterization, quality, and scale. In healthcare AI, obtaining and processing high-quality clinical data records has been a longstanding challenge, ranging from data quantity, annotation, patient privacy, and ethics. In this survey, we investigate a wide range of data-centric approaches in the FM era (from model pre-training to inference) towards improving the healthcare workflow. We discuss key perspectives in AI security, assessment, and alignment with human values. Finally, we offer a promising outlook of FM-based analytics to enhance the performance of patient outcome and clinical workflow in the evolving landscape of healthcare and medicine. We provide an up-to-date list of healthcare-related foundation models and datasets at https://github.com/Yunkun-Zhang/Data-Centric-FM-Healthcare.","['foundation models, large language models, data-centric', 'AI, healthcare,', 'AI alignment']","['DistrictShanghaiChina', 'UniversityChina', 'UniversityChina', 'UniversityChina', 'CarolinaUSA', 'JersyUSA', 'LaboratoryChina', 'UniversityChina']"
"Sparse graphs are ubiquitous in real and virtual worlds. With the phenomenal growth in semi-structured and unstructured data, sizes of the underlying graphs have witnessed a rapid growth over the years. Analyzing such large structures necessitates parallel processing, which is challenged by the intrinsic irregularity of sparse computation, memory access, and communication. It would be ideal if programmers and domain-experts get to focus only on the sequential computation and a compiler takes care of auto-generating the parallel code. On the other side, there is a variety in the number of target hardware devices, and achieving optimal performance often demands coding in specific languages or frameworks. Our goal in this work is to focus on a graph DSL which allows the domain-experts to write almost-sequential code, and generate parallel code for different accelerators from the same algorithmic specification. In particular, we illustrate code generation from the StarPlat graph DSL for NVIDIA, AMD, and Intel GPUs using CUDA, OpenCL, SYCL, and OpenACC programming languages. Using a suite of ten large graphs and four popular algorithms, we present the efficacy of StarPlat’s versatile code generator.","['Graphs,', 'DSL, code generation,', 'OpenCL,', 'CUDA,', 'SYCL,', 'OpenACC']","['MadrasIndia', 'TechIndia', 'MadrasIndia', 'MadrasIndia', 'MadrasIndia', 'MadrasIndia', 'MadrasIndia']"
"\par\LTX@newpageAbstract
We present initial results from a JWST survey of the youngest Galactic core-collapse supernova remnant Cassiopeia A (Cas A), made up of NIRCam and MIRI imaging mosaics that map emission from the main shell, interior, and surrounding circumstellar/interstellar material (CSM/ISM). We also present four exploratory positions of MIRI/MRS IFU spectroscopy that sample ejecta, CSM, and associated dust from representative shocked and unshocked regions. Surprising discoveries include: 1) a web-like network of unshocked ejecta filaments resolved to ∼similar-to\sim∼ 0.01 pc scales exhibiting an overall morphology consistent with turbulent mixing of cool, low-entropy matter from the progenitor’s oxygen layer with hot, neutrino and radioactively heated high-entropy matter, 2) a thick sheet of dust-dominated emission from shocked CSM seen in projection toward the remnant’s interior pockmarked with small (∼1′′similar-toabsentsuperscript1′′\sim 1^{\prime\prime}∼ 1 start_POSTSUPERSCRIPT ′ ′ end_POSTSUPERSCRIPT) round holes formed by ≲less-than-or-similar-to\lesssim≲ 0.′′arcsecond\farcsstart_ID start_POSTFIX SUPERSCRIPTOP . ′ ′ end_POSTFIX end_ID1 knots of high-velocity ejecta that have pierced through the CSM and driven expanding tangential shocks, 3) dozens of light echoes with angular sizes between ∼0⁢.′′⁢1similar-toabsent0arcsecond1\sim 0\farcs 1∼ 0 start_ID start_POSTFIX SUPERSCRIPTOP . ′ ′ end_POSTFIX end_ID 1 to 1′superscript1′1^{\prime}1 start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT reflecting previously unseen fine-scale structure in the ISM. NIRCam observations place new upper limits on infrared emission (≲20less-than-or-similar-toabsent20\lesssim 20≲ 20 nJy at 3 μ𝜇\muitalic_μm) from the neutron star in Cas A’s center and tightly constrain scenarios involving a possible fallback disk. These JWST survey data and initial findings help address unresolved questions about massive star explosions that have broad implications for the formation and evolution of stellar populations, the metal and dust enrichment of galaxies, and the origin of compact remnant objects.",['supernovae – supernova remnants'],"['47907', 'USA', 'USA', 'Belgium', '47907', 'USA', 'USA', 'USA', 'USA', 'USA', 'USA', 'Netherlands', 'Netherlands', 'UK', 'USA', 'USA', 'USA', '47907', '47907', 'USA', 'USA', 'USA', 'USA', 'USA', 'Korea', 'USA', 'Korea', 'Italy', 'Germany', '47907', 'Kingdom', 'USA', 'USA', 'USA', 'Sweden', 'USA', 'USA', 'USA', 'USA', 'UK', 'Belgium', 'Korea', 'UK', 'UK', 'USA', 'USA', 'UK', 'USA', 'Belgium', 'Kingdom', 'USA', 'USA', 'USA', 'USA', '47907', 'UK', 'USA']"
"Insights from JWST observations suggest
that AGN feedback evolved from a short-lived, high redshift phase in which radiatively cooled turbulence and/or momentum-conserving outflows
stimulated vigorous early star formation (“positive” feedback), to late, energy-conserving outflows that depleted halo gas reservoirs and quenched star formation. The transition between these two regimes occurred at z∼6similar-to𝑧6z\sim 6italic_z ∼ 6, independently of galaxy mass, for simple assumptions about the outflows and star formation process. Observational predictions provide circumstantial evidence for the prevalence of massive black holes at the highest redshifts hitherto observed, and we discuss their origins.",['galaxies:formation; galaxies:nuclei; quasars:general; galaxies:jets; stars:formation'],"['France', 'USA', 'UK', 'USA', 'USA', 'USA', 'Israel', 'USA']"
"Henize 2–10 is a dwarf starburst galaxy hosting a ∼106⁢M⊙similar-toabsentsuperscript106subscript𝑀direct-product\sim 10^{6}~{}M_{\odot}∼ 10 start_POSTSUPERSCRIPT 6 end_POSTSUPERSCRIPT italic_M start_POSTSUBSCRIPT ⊙ end_POSTSUBSCRIPT black hole (BH) that is driving an ionized outflow and triggering star formation within the central ∼100similar-toabsent100\sim 100∼ 100 pc of the galaxy. Here we present ALMA continuum observations from 99 to 340 GHz, as well as spectral line observations of the molecules CO (1–0, 3–2), HCN (1–0, 3–2), and HCO+++ (1–0, 3–2), with a focus on the BH and its vicinity. Incorporating cm-wave radio measurements from the literature, we show that the spectral energy distribution of the BH is dominated by synchrotron emission from 1.4 to 340 GHz with a spectral index of α≈−0.5𝛼0.5\alpha\approx-0.5italic_α ≈ - 0.5. We analyze the spectral line data and identify an elongated molecular gas structure around the BH with a velocity distinct from the surrounding regions. The physical extent of this molecular gas structure is ≈130⁢pc×30absent130pc30\approx 130~{}{\rm pc}\times 30≈ 130 roman_pc × 30 pc and the molecular gas mass is ∼106⁢M⊙similar-toabsentsuperscript106subscript𝑀direct-product\sim 10^{6}~{}M_{\odot}∼ 10 start_POSTSUPERSCRIPT 6 end_POSTSUPERSCRIPT italic_M start_POSTSUBSCRIPT ⊙ end_POSTSUBSCRIPT. Despite an abundance of molecular gas in this general region, the position of the BH is significantly offset from the peak intensity, which may explain why the BH is radiating at a very low Eddington ratio. Our analysis of the spatially-resolved line ratio between CO J=3–2 and J=1–0 implies that the CO gas in the vicinity of the BH is highly excited, particularly at the interface between the BH outflow and the regions of triggered star formation. This suggests that the cold molecular gas is being shocked by the bipolar outflow from the BH, supporting the case for positive BH feedback.","['galaxies:active – galaxies: dwarf –galaxies: individual (He 2–10) -', 'ISM: molecules']","['USA', 'USA', 'USA']"
"The Hobby-Eberly Telescope Dark Energy Experiment (HETDEX) is designed to detect and measure the redshifts of more than one million Lyα𝛼\alphaitalic_α emitting galaxies (LAEs) between 1.88<z<3.521.88𝑧3.521.88<z<3.521.88 < italic_z < 3.52. In addition to its cosmological measurements, these data enable studies of Lyα𝛼\alphaitalic_α spectral profiles and the underlying radiative transfer. Using the roughly half a million LAEs in the HETDEX Data Release 3, we stack various subsets to obtain the typical Lyα𝛼\alphaitalic_α profile for the z∼2−3similar-to𝑧23z\sim 2-3italic_z ∼ 2 - 3 epoch and to understand their physical properties. We find clear absorption wings around Lyα𝛼\alphaitalic_α emission, which extend ∼2000similar-toabsent2000\sim 2000∼ 2000 km s−1superscripts1\mathrm{s}^{-1}roman_s start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT both redward and blueward of the central line. Using far-UV spectra of nearby (0.002<z<0.1820.002𝑧0.1820.002<z<0.1820.002 < italic_z < 0.182) LAEs in the CLASSY treasury and optical/near-IR spectra of 2.8<z<6.72.8𝑧6.72.8<z<6.72.8 < italic_z < 6.7 LAEs in the MUSE-Wide survey, we observe absorption profiles in both redshift regimes. Dividing the sample by volume density shows that the troughs increase in higher density regions. This trend suggests that the depth of the absorption is dependent on the local density of objects near the LAE, a geometry that is similar to damped Lyman-α𝛼\alphaitalic_α systems. Simple simulations of Lyα𝛼\alphaitalic_α radiative transfer can produce similar troughs due to absorption of light from background sources by H i gas surrounding the LAEs.",['cosmology: observations – galaxies: evolution – galaxies: formation – galaxies: high redshift'],"['USA', 'USA', 'USA', 'USA', 'USA', 'USA', 'USA', 'USA', 'USA', 'Germany', 'USA', 'USA', 'Germany', 'Germany', 'UK', 'UK', 'USA', 'USA', 'USA', 'USA', 'Fellow', 'USA', 'USA', 'USA', 'USA', 'Germany', 'Germany', 'Germany', 'Japan', 'Germany', 'USA', 'Japan', 'USA', 'USA', 'USA']"
"The recent survey of the core-collapse supernova remnant Cassiopeia A (Cas A) with the MIRI instrument on board the James Webb Space Telescope (JWST) revealed a large structure in the interior region, referred to as the
“Green Monster”. Although its location suggests that it is an ejecta structure, the infrared properties of the “Green Monster” hint at a circumstellar medium (CSM) origin.
In this companion paper to the JWST Cas A paper, we investigate the filamentary X-ray structures associated with the “Green Monster” using Chandra X-ray Observatory data.
We extracted spectra along the “Green Monster” as well as from shocked CSM regions.
Both the extracted spectra and a principal component analysis show that the “Green Monster” emission properties are similar to those of the shocked CSM.
The spectra are well-fit by a model consisting of a combination of a non-equilibrium ionization model and a power-law component, modified by Galactic absorption.
All the “Green Monster” spectra show a blueshift of around -2500 km⁢s−1kmsuperscripts1{\rm km\ s^{-1}}roman_km roman_s start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT, suggesting that the structure is on the near side of Cas A.
The ionization age is around ne⁢t≈1.4×1011⁢cm−3⁢ssubscript𝑛e𝑡1.4superscript1011superscriptcm3sn_{\rm e}t\approx 1.4\times 10^{11}~{}{\rm cm^{-3}s}italic_n start_POSTSUBSCRIPT roman_e end_POSTSUBSCRIPT italic_t ≈ 1.4 × 10 start_POSTSUPERSCRIPT 11 end_POSTSUPERSCRIPT roman_cm start_POSTSUPERSCRIPT - 3 end_POSTSUPERSCRIPT roman_s. This translates into a pre-shock density of ∼11⁢cm−3similar-toabsent11superscriptcm3\sim 11~{}{\rm cm^{-3}}∼ 11 roman_cm start_POSTSUPERSCRIPT - 3 end_POSTSUPERSCRIPT, higher than previous estimates of the unshocked CSM.
The relatively high ne⁢tsubscript𝑛e𝑡n_{\rm e}titalic_n start_POSTSUBSCRIPT roman_e end_POSTSUBSCRIPT italic_t
and relatively low radial velocity suggest that this structure has a relatively high density compared to other shocked CSM plasma.
This analysis provides yet another piece of evidence that the CSM around Cas A’s progenitor was not that of a smooth steady wind profile.","['Supernova remnants (1667) —', 'X-ray astronomy (1810) —', 'Stellar mass loss (1613) —', 'Circumstellar dust(236)']","['Netherlands', 'Netherlands', 'Netherlands', 'USA', 'Belgium', '47907', 'USA', 'USA', 'USA']"
"We report on the detection of radio bursts from the Galactic bulge using the real-time transient detection and localization system, realfast. The pulses were detected commensally on the Karl G. Jansky Very Large Array during a survey of unidentified Fermi γ𝛾\gammaitalic_γ-ray sources. The bursts were localized to subarcsecond precision using realfast fast-sampled imaging. Follow-up observations with the Green Bank Telescope detected additional bursts from the same source. The bursts do not exhibit periodicity in a search up to periods of 480s, assuming a duty cycle of <<< 20%. The pulses are nearly 100% linearly polarized, show circular polarization up to 12%, have a steep radio spectral index of –2.7, and exhibit variable scattering on timescales of months. The arcsecond-level realfast localization links the source confidently with the Fermi γ𝛾\gammaitalic_γ-ray source and places it nearby (though not coincident with) an XMM-Newton X-ray source. Based on the source’s overall properties, we discuss various options for the nature of this object and propose that it could be a young pulsar, magnetar, or a binary pulsar system.","['Radio transient sources,', 'Time domain astronomy,', 'High energy astrophysics']","['USA', 'USA', 'USA', 'USA', 'Fellow', 'USA', 'USA', 'Mexico.', 'USA', 'USA', 'USA', 'USA', 'USA', 'USA', 'USA']"
"Smart Digital twins (SDTs) are being increasingly used to virtually replicate and predict the behaviors of complex physical systems through continual data assimilation enabling the optimization of the performance of these systems by controlling the actions of systems. Recently, deep learning (DL) models have significantly enhanced the capabilities of SDTs, particularly for tasks such as predictive maintenance, anomaly detection, and optimization. In many domains, including medicine, engineering, and education, SDTs use image data (image-based SDTs) to observe and learn system behaviors and control their behaviors. This paper focuses on various approaches and associated challenges in developing image-based SDTs by continually assimilating image data from physical systems. The paper also discusses the challenges involved in designing and implementing DL models for SDTs, including data acquisition, processing, and interpretation. In addition, insights into the future directions and opportunities for developing new image-based DL approaches to develop robust SDTs are provided. This includes the potential for using generative models for data augmentation, developing multi-modal DL models, and exploring the integration of DL with other technologies, including 5G, edge computing, and IoT. In this paper, we describe the image-based SDTs, which enable broader adoption of the digital twin DT paradigms across a broad spectrum of areas and the development of new methods to improve the abilities of SDTs in replicating, predicting, and optimizing the behavior of complex systems.","['Index', 'Terms: ', 'Artificial intelligence;', 'Deep learning;', 'Digital twins;', 'Cyber-physical systems;']","['1mdrumanislam,2msubramaniam,3phuang@unomaha.edu']"
"Observed protostellar outflows exhibit a variety of asymmetrical
features, including remarkable unipolar outflows and bending
outflows. Revealing the formation and early evolution of such
asymmetrical protostellar outflows, especially the unipolar
outflows, is essential for a better understanding of the star and
planet formation because they can dramatically change the mass
accretion and angular momentum transport to the protostars and
protoplanetary disks. Here, we perform the three-dimensional
non-ideal magnetohydrodynamics simulations to investigate the
formation and early evolution of the asymmetrical protostellar
outflows in magnetized turbulent isolated molecular cloud cores. We
find, for the first time to our knowledge, that the unipolar outflow
forms even in the single low-mass protostellar system. The results
show that the unipolar outflow is driven in the weakly magnetized
cloud cores with the dimensionless mass-to-flux ratios of μ=8𝜇8\mu=8italic_μ = 8
and 16161616. Furthermore, we find the protostellar rocket
effect of the unipolar outflow, which is similar to the launch
and propulsion of a rocket. The unipolar outflow ejects the
protostellar system from the central dense region to the outer
region of the parent cloud core, and the ram pressure caused by its
ejection suppresses the driving of additional new outflows. In
contrast, the bending bipolar outflow is driven in the moderately
magnetized cloud core with μ=4𝜇4\mu=4italic_μ = 4. The ratio of the magnetic to
turbulent energies of a parent cloud core may play a key role in the
formation of asymmetrical protostellar outflows.","['Magnetic fields (994) —', 'Protoplanetary disks (1300) —', 'Protostars (1302) —', 'Star formation (1569) —', 'Stellar outflows\n(1636) —', 'Young stellar objects (1834)']","['Japan', 'Japan', 'Japan', 'Japan', 'Taiwan', 'Japan', 'Japan', 'Japan', 'Japan', 'Japan']"
"This paper explores core-collapse supernovae as crucial targets for neutrino telescopes, addressing uncertainties in their simulation results. We comprehensively analyze eighteen modern simulations and discriminate among supernova models using realistic detectors and interactions. A significant correlation between the total neutrino energy and cumulative counts, driven by massive lepton neutrinos and oscillations, is identified, particularly noticeable with the DUNE detector. Bayesian techniques indicate strong potential for model differentiation during a Galactic supernova event, with HK excelling in distinguishing models based on equation of state, progenitor mass, and mixing scheme.","['Core-collapse supernovae (304) —', 'Supernova neutrinos (1666) —', 'Neutrino telescopes (1105) —', 'Bayesian statistics (1900)']","['Japan', '94720', 'Japan', 'USA', 'USA', 'Japan', 'Japan', 'Japan', 'Japan', 'U.S.A.']"
"Crystalline materials, such as metals and semiconductors, nearly always contain a special defect type called dislocation.
This defect decisively determines many important material properties, e.g., strength, fracture toughness, or ductility.
Over the past years, significant effort has been put into understanding dislocation behavior across different length scales via experimental characterization techniques and simulations.
This paper introduces the dislocation ontology (DISO), which defines the concepts and relationships related to linear defects in crystalline materials.
We developed DISO using a top-down approach in which we start defining the most general concepts in the dislocation domain and subsequent specialization of them.
DISO is published through a persistent URL following W3C best practices for publishing Linked Data.
Two potential use cases for DISO are presented to illustrate its usefulness in the dislocation dynamics domain.
The evaluation of the ontology is performed in two directions, evaluating the success of the
ontology in modeling a real-world domain and the richness of the ontology.","['Ontology,', 'Dislocation,', 'Crystallographic', 'Defects,', 'Linked', 'Data,', 'Materials', 'Science and', 'Engineering']","['Germany', 'Egypt', 'Germany']"
"More than 36 years have passed since the discovery of the infrared excess from circumstellar dust orbiting the white dwarf G29-38, which at 17.5 pc it is the nearest and brightest of its class. The precise morphology of the orbiting dust remains only marginally constrained by existing data, subject to model-dependent inferences, and thus fundamental questions of its dynamical origin and evolution persist. This study presents a means to constrain the geometric distribution of the emitting dust using stellar pulsations measured at optical wavelengths as a variable illumination source of the dust, which re-radiates primarily in the infrared. By combining optical photometry from the Whole Earth Telescope with 0.7–2.5 \upmu\upmu\upmum spectroscopy obtained with SpeX at NASA’s Infrared Telescope Facility, we detect luminosity variations at all observed wavelengths, with variations at most wavelengths corresponding to the behavior of the pulsating stellar photosphere, but towards the longest wavelengths the light curves probe the corresponding time-variability of the circumstellar dust. In addition to developing methodology, we find pulsation amplitudes decrease with increasing wavelength for principal pulsation modes, yet increase beyond ≈\approx≈2 \upmu\upmu\upmum for nonlinear combination frequencies. We interpret these results as combination modes deriving from principal modes of identical ℓℓ\ellroman_ℓ values and discuss the implications for the morphology of the warm dust. We also draw attention to some discrepancies between our findings and theoretical expectations for the results of the non-linearity imposed by the surface convection zone on mode–mode interactions and on the behavior of the first harmonic of the highest-amplitude pulsation mode.",['circumstellar matter — planetary systems — stars: individual (G29-38) — white dwarfs'],"['Administration.', 'USA', 'Administration.', 'UK', 'USA', 'USA', 'UK', 'Administration.', 'UK', 'UK', 'Deceased.', 'Canada', 'USA', 'USA', 'USA', 'USA', 'Kazakhstan', 'Russia', 'Kazakhstan', 'Kazakhstan', 'USA', 'Poland', 'Lithuania', 'Kazakhstan', 'Turkey', 'USA', 'Poland', 'Poland']"
"Modern RISC-V platforms control and monitor security-critical systems such as industrial controllers and autonomous vehicles.
While these platforms feature a Root-of-Trust (RoT) to store authentication secrets and enable secure boot technologies, they often lack Control-Flow Integrity (CFI) enforcement and are vulnerable to cyber-attacks which divert the control flow of an application to trigger malicious behaviours.
Recent techniques to enforce CFI in RISC-V systems include ISA modifications or custom hardware IPs, all requiring ad-hoc binary toolchains or design of CFI primitives in hardware.
This paper proposes TitanCFI, a novel approach to enforce CFI in the RoT.
TitanCFI modifies the commit stage of the protected core to stream control flow instructions to the RoT and it integrates the CFI enforcement policy in the RoT firmware.
Our approach enables maximum reuse of the hardware resource present in the System-on-Chip (SoC), and it avoids the design of custom IPs and the modification of the compilation toolchain, while exploiting the RoT tamper-proof storage and cryptographic accelerators to secure CFI metadata.
We implemented the proposed architecture on a modern RISC-V SoC along with a return address protection policy in the RoT, and benchmarked area and runtime overhead.
Experimental results show that TitanCFI achieves overhead comparable to SoA hardware CFI solutions for most benchmarks, with lower area overhead, resulting in 1% of additional area occupation.","['Index', 'Terms: ', 'Control-Flow', 'Integrity,', 'OpenTitan,', 'RISC-V']",['emanuele.parisi@unibo.it']
"The lifetime of mm size dust grains, such as chondrules, in the nominal solar nebula model is limited to ∼105similar-toabsentsuperscript105\sim 10^{5}∼ 10 start_POSTSUPERSCRIPT 5 end_POSTSUPERSCRIPT yr due to an inward drift driven by gas drag.
However, isotopic and petrological studies on primitive meteorites indicate a discrepancy of ≳106greater-than-or-equivalent-toabsentsuperscript106\gtrsim 10^{6}≳ 10 start_POSTSUPERSCRIPT 6 end_POSTSUPERSCRIPT yr between the formation time of chondrules and that of chondritic parent bodies.
Therefore chondrules should survive for ≳106greater-than-or-equivalent-toabsentsuperscript106\gtrsim 10^{6}≳ 10 start_POSTSUPERSCRIPT 6 end_POSTSUPERSCRIPT yr in the solar nebula against the inward drift without subsequent growth (i.e., planetesimal formation).
Here we investigate the conditions of the solar nebula that are suitable for the long lifetime of chondrule-sized dust particles.
We take the turbulent strength, the radial pressure gradient force, and the disk metallicity of the solar nebula as free parameters.
For 1 mm-radius-chondrules to survive and keep their size for ≳106greater-than-or-equivalent-toabsentsuperscript106\gtrsim 10^{6}≳ 10 start_POSTSUPERSCRIPT 6 end_POSTSUPERSCRIPT yr, the suitable condition is a weak turbulence (α∼10−6similar-to𝛼superscript106\alpha\sim 10^{-6}italic_α ∼ 10 start_POSTSUPERSCRIPT - 6 end_POSTSUPERSCRIPT), a flat radial profile (η≲10−3less-than-or-similar-to𝜂superscript103\eta\lesssim 10^{-3}italic_η ≲ 10 start_POSTSUPERSCRIPT - 3 end_POSTSUPERSCRIPT), and a high metallicity (Z∼0.1similar-to𝑍0.1Z\sim 0.1italic_Z ∼ 0.1).
This condition is qualitatively consistent with the characteristics of protoplanetary disks suggested by recent observations.
We eventually propose that planetesimal formation may be induced by the disk evolution, e.g., the inside-out dispersal of the gas component due to the disk wind.","['Chondrules;', 'Solar nebulae;', 'Planetesimals;', 'Planetary system formation;', 'Protoplanetary disks;', 'Carbonaceous chondrites']","['Japan', 'Japan', 'USA']"
"In the forthcoming era of big astronomical data, it is a burden to find out target sources from ground-based and space-based telescopes.
Although Machine Learning (ML) methods have been extensively utilized to address this issue, the incorporation of in-depth data analysis can significantly enhance the efficiency of identifying target sources when dealing with massive volumes of astronomical data.
In this work, we focused on the task of finding AGN candidates and identifying BL Lac/FSRQ candidates from the 4FGL_DR3 uncertain sources.
We studied the correlations among the attributes of the 4FGL_DR3 catalogue and proposed a novel method, named FDIDWT, to transform the original data.
The transformed dataset is characterized as low-dimensional and feature-highlighted, with the estimation of correlation features by Fractal Dimension (FD) theory and the multi-resolution analysis by Inverse Discrete Wavelet Transform (IDWT).
Combining the FDIDWT method with an improved lightweight MatchboxConv1D model, we accomplished two missions:
(1) to distinguish the Active Galactic Nuclei (AGNs) from others (Non-AGNs) in the 4FGL_DR3 uncertain sources with an accuracy of 96.65%±1.32%plus-or-minuspercent96.65percent1.3296.65\%\pm 1.32\%96.65 % ± 1.32 %, namely, Mission A;
(2) to classify blazar candidates of uncertain type (BCUs) into BL Lacertae objects (BL Lacs) or Flat Spectrum Radio Quasars (FSRQs) with an accuracy of 92.03%±2.2%plus-or-minuspercent92.03percent2.292.03\%\pm 2.2\%92.03 % ± 2.2 %, namely, Mission B. There are 1354 AGN candidates in Mission A, 482 BL Lacs candidates and 128 FSRQ candidates in Mission B were found.
The results show a high consistency of greater than 98% with the results in previous works.
In addition, our method has the advantage of finding less variable and relatively faint sources than ordinary methods.","['machine learning — attribute analysis — fractal dimension — wavelet transform —', 'Fermi source']","['China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China']"
"The corona is a structure possessed by stars, including the sun. The abnormal heating of the solar corona and chromosphere is one of the greatest mysteries in modern astronomy. While state-of-the-art observations have identified some candidates of magnetic activity events that could be responsible for this abnormal heating, and theoretical studies have proposed various heating modes, a complete physical picture of how they are heated as a whole remains elusive. In this study, the characteristics of the heated corona and chromosphere are investigated, and for the first time, the question of how they are abnormally heated is explicitly answered by analyzing the long-term observations of the global chromosphere in the Ca II K line and the global corona in the coronal green line.
The findings reveal that both the quiet chromosphere and corona are in anti-phase with the solar cycle, whereas the active chromosphere and corona are in phase with it.
Different parts of the solar corona and chromosphere exhibit significantly different variation characteristics, and are found to be heated by different magnetic categories and probably in different modes.
This study posits that unraveling the heating mystery is best approached through the lens of magnetic categories, rather than magnetic activity events.","['Sun: corona –', 'Sun: chromosphere –', 'Sun: magnetic fields']","['China', 'China', 'China', 'Science', 'China']"
"Verifying safety and liveness over array systems is a highly challenging problem. Array systems naturally capture parameterized systems such as distributed protocols with an unbounded number of processes. Such distributed protocols often exploit process IDs during their computation, resulting in array systems whose element values range over an infinite domain. In this paper, we develop a novel framework for proving safety and liveness over array systems. The crux of the framework is to overapproximate an array system as a string rewriting system (i.e. over a finite alphabet) by means of a new predicate abstraction that exploits the so-called indexed predicates. This allows us to tap into powerful verification methods for string rewriting systems that have been heavily developed in the last two decades or so (e.g. regular model checking).
We demonstrate how our method yields simple, automatically verifiable proofs of safety and liveness properties for challenging examples, including Dijkstra’s self-stabilizing protocol and the Chang-Roberts leader election protocol.","['Array theory,', 'Regular model checking,', 'Infinite-state model checking,', 'Abstract interpretation,', 'Predicate abstraction,', 'Distributed protocol verification']","['UniversityTaipeiTaiwan', 'Kaiserslautern-LandauKaiserslauternGermany', 'MPI-SWSKaiserslauternGermany']"
"Aerial-ground person re-identification (Re-ID) presents unique challenges in computer vision, stemming from the distinct differences in viewpoints, poses, and resolutions between high-altitude aerial and ground-based cameras. Existing research predominantly focuses on ground-to-ground matching, with aerial matching less explored due to a dearth of comprehensive datasets. To address this, we introduce AG-ReID.v2, a dataset specifically designed for person Re-ID in mixed aerial and ground scenarios. This dataset comprises 100,502 images of 1,615 unique individuals, each annotated with matching IDs and 15 soft attribute labels. Data were collected from diverse perspectives using a UAV, stationary CCTV, and smart glasses-integrated camera, providing a rich variety of intra-identity variations. Additionally, we have developed an explainable attention network tailored for this dataset. This network features a three-stream architecture that efficiently processes pairwise image distances, emphasizes key top-down features, and adapts to variations in appearance due to altitude differences. Comparative evaluations demonstrate the superiority of our approach over existing baselines. We plan to release the dataset and algorithm source code publicly, aiming to advance research in this specialized field of computer vision. For access, please visit https://github.com/huynguyen792/AG-ReID.v2.","['Index', 'Terms: ', 'Person re-identification, aerial-ground imagery,', 'UAV,', 'CCTV, smart glasses, video surveillance, attribute-guided, three-stream network']",['c.fookes}@qut.edu.au']
"Bloom Filters are a space-efficient data structure used for the testing of membership in a set that errs only in the False Positive direction. However, the standard analysis that measures this False Positive rate provides a form of worst case bound that is both overly conservative for the majority of network applications that utilize Bloom Filters, and reduces accuracy by not taking into account the actual state (number of bits set) of the Bloom Filter after each arrival. In this paper, we more accurately characterize the False Positive dynamics of Bloom Filters as they are commonly used in networking applications. In particular, network applications often utilize a Bloom Filter that “recycles”: it repeatedly fills, and upon reaching a certain level of saturation, empties and fills again. In this context, it makes more sense to evaluate performance using the average False Positive rate instead of the worst case bound. We show how to efficiently compute the average False Positive rate of recycling Bloom Filter variants via renewal and Markov models. We apply our models to both the standard Bloom Filter and a “two-phase” variant, verify the accuracy of our model with simulations, and find that the previous analysis’ worst-case formulation leads to up to a 30% reduction in the efficiency of Bloom Filter when applied in network applications, while two-phase overhead diminishes as the needed False Positive rate is tightened.","['Index', 'Terms: ', 'Bloom', 'Filter,', 'False', 'Positives']","['US', 'US', 'US']"
"Mobile malware has become one of the most critical security threats in the era of ubiquitous mobile computing.
Despite the intensive efforts from security experts to counteract it, recent years have still witnessed a rapid growth of identified malware samples.
This could be partly attributed to the newly-emerged technologies that may constantly open up under-studied attack surfaces for the adversaries.
One typical example is the recently-developed mobile machine learning (ML) framework that enables storing and running deep learning (DL) models on mobile devices. Despite obvious advantages, this new feature also inadvertently introduces potential vulnerabilities (e.g., on-device models may be modified for malicious purposes).
In this work, we propose a method to generate or transform mobile malware by hiding the malicious payloads inside the parameters of deep learning models, based on a strategy that considers four factors (layer type, layer number, layer coverage and the number of bytes to replace).
Utilizing the proposed method, we can run malware in DL mobile applications covertly with little impact on the model performance (i.e., as little as 0.4% drop in accuracy and at most 39ms latency overhead). We test our method on seven different models and seven malware samples, and we can successfully trigger the malicious functions such as downloading files, getting SMS records and getting screenshots in a real-world application.
In terms of the practical effectiveness, the generated malware can evade state-of-the-art detection techniques (i.e., none detected by VirusTotal), and the malware-based attack exhibits a high practical feasibility (i.e., successfully attack 41% of the apps with on-device DL models).
Our work should alert the security experts on malware injection attacks on mobile devices, and further raise more awareness towards the deep learning assisted attacks in the mobile ecosystem.","['malware injection, neural networks, deep learning, mobile application, backdoor attack']","['KongChina', 'TechnologyWuhanChina', 'TechnologyWuhanChina', 'QueenslandBrisbaneAustralia', 'KongChina', 'TechnologyWuhanChina']"
": Vulnerability detectors based on deep learning (DL) models have proven their effectiveness in recent years.
However, the shroud of opacity surrounding the decision-making process of these detectors makes it difficult for security analysts to comprehend.
To address this, various explanation approaches have been proposed to explain the predictions by highlighting important features, which have been demonstrated effective in other domains such as computer vision and natural language processing.
Unfortunately, an in-depth evaluation of vulnerability-critical features, such as fine-grained vulnerability-related code lines, learned and understood by these explanation approaches remains lacking.
In this study, we first evaluate the performance of ten explanation approaches for vulnerability detectors based on graph and sequence representations, measured by two quantitative metrics including fidelity and vulnerability line coverage rate.
Our results show that fidelity alone is not sufficient for evaluating these approaches, as fidelity incurs significant fluctuations across different datasets and detectors. We subsequently check the precision of the vulnerability-related code lines reported by the explanation approaches, and find poor accuracy in this task among all of them.
This can be attributed to the inefficiency of explainers in selecting important features and the presence of irrelevant artifacts learned by DL-based detectors.","['Vulnerability', 'Detection,', 'Explanation', 'Approaches,', 'Fidelity,', 'Coverage', 'Rate']","['UniversityChina', 'AlbertaCanada', 'TechnologyChina', 'TechnologyChina', 'QueenslandAustralia', 'WalesAustralia', 'UniversityChina', 'AlbertaCanada', 'TechnologyChina']"
"In past years, we have been dedicated to automating user acceptance testing (UAT) process of WeChat Pay, one of the most influential mobile payment
applications in China. A system titled XUAT has been developed for this purpose. However, there is still a human-labor-intensive stage, i.e, test scripts generation, in the current system. Therefore, in this paper, we concentrate on methods of boosting the automation level of the current system, particularly the stage of test scripts generation. With recent notable successes, large language models (LLMs) demonstrate significant potential in attaining human-like intelligence and there has been a growing research area that employs LLMs as autonomous agents to obtain human-like decision-making capabilities. Inspired by these works, we propose an LLM-powered multi-agent collaborative system, named XUAT-Copilot, for automated UAT. The proposed system mainly consists of three LLM-based agents responsible for action planning, state checking and parameter selecting, respectively, and two additional modules for state sensing and case rewriting. The agents interact with testing device, make human-like decision and generate action command in a collaborative way. The proposed multi-agent system achieves a close effectiveness to human testers in our experimental studies and gains a significant improvement of P⁢a⁢s⁢s⁢@⁢1𝑃𝑎𝑠𝑠@1Pass@1italic_P italic_a italic_s italic_s @ 1 accuracy compared with single-agent architecture. More importantly, the proposed system has launched in the formal testing environment of WeChat Pay mobile app, which saves a considerable amount of manpower in the daily development work.","['Automated', 'UAT,', 'AI', 'Agents,', 'Large', 'Language', 'Model']",['TencentShenzhenChina']
"GitHub Sponsors was launched in 2019, enabling donations to open-source software developers to provide financial support, as per GitHub’s slogan: “Invest in the projects you depend on”. However, a 2022 study on GitHub Sponsors found that only two-fifths of developers who were seeking sponsorship received a donation. The study found that, other than internal actions (such as offering perks to sponsors), developers had advertised their GitHub Sponsors profiles on social media, such as Twitter (also known as X). Therefore, in this work, we investigate the impact of tweets that contain links to GitHub Sponsors profiles on sponsorship, as well as their reception on Twitter/X. We further characterize these tweets to understand their context and find that (1) such tweets have the impact of increasing the number of sponsors acquired, (2) compared to other donation platforms such as Open Collective and Patreon, GitHub Sponsors has significantly fewer interactions but is more visible on Twitter/X, and (3) developers tend to contribute more to open-source software during the week of posting such tweets. Our findings are the first step toward investigating the impact of social media on obtaining funding to sustain open-source software.","['Open-source', 'Software,', 'Sponsorship,', 'Social', 'Media']","['Japan', 'Japan', 'UniversityJapan', 'MelbourneAustralia', 'Japan']"
"This review presents various image segmentation methods using complex networks.
Image segmentation is one of the important steps in image analysis as it helps analyze and understand complex images. At first, it has been tried to classify complex networks based on how it being used in image segmentation.
In computer vision and image processing applications, image segmentation is essential for analyzing complex images with irregular shapes, textures, or overlapping boundaries. Advanced algorithms make use of machine learning, clustering, edge detection, and region-growing techniques. Graph theory principles combined with community detection-based methods allow for more precise analysis and interpretation of complex images. Hybrid approaches combine multiple techniques for comprehensive, robust segmentation, improving results in computer vision and image processing tasks.","['Index', 'Terms: ', 'Image segmentation,', 'Complex networks,', 'Community detection']","['amin.rezaei@ieee.org', 'fatemeh.asadi@stu.yazd.ac.ir']"
"One of the celebrated tools in explaining the Hydrogen atom is Born-Oppenheimer approximation. The resemblance of Q⁢Q⁢q¯⁢q¯𝑄𝑄¯𝑞¯𝑞QQ\bar{q}\bar{q}italic_Q italic_Q over¯ start_ARG italic_q end_ARG over¯ start_ARG italic_q end_ARG tetraquarks to Hydrogen atom within Quantum chromodynamics (QCD) implies usage of Born-Oppenheimer approximation for these multiquark states. In this work, we use dynamical diquark model to calculate mass spectra and sizes of doubly charmed and charged tetraquark states denoted as Tc⁢c++superscriptsubscript𝑇𝑐𝑐absentT_{cc}^{++}italic_T start_POSTSUBSCRIPT italic_c italic_c end_POSTSUBSCRIPT start_POSTSUPERSCRIPT + + end_POSTSUPERSCRIPT. Our results for mass spectra indicate some bound state candidates with respect to corresponding two-meson thresholds. Calculation of expectation values of ⟨r2⟩delimited-⟨⟩superscript𝑟2\sqrt{\langle r^{2}\rangle}square-root start_ARG ⟨ italic_r start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ⟩ end_ARG reflects that doubly charmed and charged tetraquark states are compact.","['diquark, doubly-charmed hadrons,', 'Tc\u2062c++superscriptsubscript𝑇𝑐𝑐absentT_{cc}^{++}italic_T start_POSTSUBSCRIPT italic_c italic_c end_POSTSUBSCRIPT start_POSTSUPERSCRIPT + + end_POSTSUPERSCRIPT states']",['Türkiye']
"In this paper, we conducted a comparative evaluation of three RGB-D SLAM (Simultaneous Localization and Mapping) algorithms—RTAB-Map, ORB-SLAM3, and OpenVSLAM—for SURENA-V humanoid robot localization and mapping.
Our test involves the robot to follow a full circular pattern, with an Intel® RealSense™ D435 RGB-D camera installed on its head.
In assessing localization accuracy, ORB-SLAM3 outperformed the others with an ATE of 0.1073, followed by RTAB-Map at 0.1641 and OpenVSLAM at 0.1847.
However, it should be noted that both ORB-SLAM3 and OpenVSLAM faced challenges in maintaining accurate odometry when the robot encountered a wall with limited feature points. Nevertheless, OpenVSLAM demonstrated the ability to detect loop closures and successfully relocalize itself within the map when the robot approached its initial location.
The investigation also extended to mapping capabilities, where RTAB-Map excelled by offering diverse mapping outputs, including dense, OctoMap, and occupancy grid maps. In contrast, both ORB-SLAM3 and OpenVSLAM provided only sparse maps.","['Index', 'Terms: ', 'RGB-D', 'SLAM, humanoid robot, localization, mapping, loop closure detection']","['amirhosein.vedadi@ut.ac.ir', 'aykoma@ut.ac.ir', 'parsa.yazdankhah@ut.ac.ir', 'aminmozayyan@ut.ac.ir']"
"Advances in Visually Rich Document Understanding (VrDU) have enabled information extraction and question answering over documents with complex layouts. Two tropes of architectures have emerged—transformer-based models inspired by LLMs, and Graph Neural Networks. In this paper, we introduce DocGraphLM, a novel framework that combines pre-trained language models with graph semantics. To achieve this, we propose 1) a joint encoder architecture to represent documents, and 2) a novel link prediction approach to reconstruct document graphs. DocGraphLM predicts both directions and distances between nodes using a convergent joint loss function that prioritizes neighborhood restoration and downweighs distant node detection. Our experiments on three SotA datasets show consistent improvement on IE and QA tasks with the adoption of graph features. Moreover, we report that adopting the graph features accelerates convergence in the learning process druing training, despite being solely constructed through link prediction.","['language model, graph neural network, information extraction, visual document understanding']","['StLondonUK', 'YorkUSA', 'YorkUSA', 'HampshireUSA', 'YorkUSA']"
"We present JWST/NIRCam F187N, F200W, F405N and F410M direct imaging data of the disk surrounding SAO 206462. Previous images show a very structured disk, with a pair of spiral arms thought to be launched by one or more external perturbers. The spiral features are visible in three of the four filters, with the non-detection in F410M due to the large detector saturation radius. We detect with a signal-to-noise ratio of 4.4 a companion candidate (CC1) that, if on a coplanar circular orbit, would orbit SAO 206462 at a separation of ∼300similar-toabsent300\sim 300∼ 300 au, 2.25⁢σ2.25𝜎2.25\sigma2.25 italic_σ away from the predicted separation for the driver of the eastern spiral. According to the BEX models, CC1 has a mass of MCC1=0.8±0.3⁢MJsubscript𝑀CC1plus-or-minus0.80.3subscript𝑀JM_{\mathrm{CC1}}=0.8\pm 0.3~{}M_{\mathrm{J}}italic_M start_POSTSUBSCRIPT CC1 end_POSTSUBSCRIPT = 0.8 ± 0.3 italic_M start_POSTSUBSCRIPT roman_J end_POSTSUBSCRIPT. No other companion candidates were detected. At the location predicted by simulations of both spirals generated by a single massive companion, the NIRCam data exclude objects more massive than ∼2.2⁢MJsimilar-toabsent2.2subscript𝑀J\sim 2.2~{}M_{\mathrm{J}}∼ 2.2 italic_M start_POSTSUBSCRIPT roman_J end_POSTSUBSCRIPT assuming the BEX evolutionary models. In terms of temperatures, the data are sensitive to objects with T\text⁢e⁢f⁢f∼650−850similar-tosubscript𝑇\text𝑒𝑓𝑓650850T_{\text{eff}}\sim 650-850italic_T start_POSTSUBSCRIPT italic_e italic_f italic_f end_POSTSUBSCRIPT ∼ 650 - 850 K, when assuming planets emit like blackbodies (Rpsubscript𝑅pR_{\mathrm{p}}italic_R start_POSTSUBSCRIPT roman_p end_POSTSUBSCRIPT between 1 and 3⁢RJ3subscript𝑅J3R_{\mathrm{J}}3 italic_R start_POSTSUBSCRIPT roman_J end_POSTSUBSCRIPT). From these results, we conclude that if the spirals are driven by gas giants, these must be either cold or embedded in circumplanetary material. In addition, the NIRCam data provide tight constraints on ongoing accretion processes. In the low extinction scenario we are sensitive to mass accretion rates of the order M˙∼10−9⁢MJsimilar-to˙𝑀superscript109subscript𝑀J\dot{M}\sim 10^{-9}M_{\mathrm{J}}over˙ start_ARG italic_M end_ARG ∼ 10 start_POSTSUPERSCRIPT - 9 end_POSTSUPERSCRIPT italic_M start_POSTSUBSCRIPT roman_J end_POSTSUBSCRIPT yr−11{}^{-1}start_FLOATSUPERSCRIPT - 1 end_FLOATSUPERSCRIPT. Thanks to the longer wavelengths used to search for emission lines, we reach unprecedented sensitivities to processes with M˙∼10−7⁢MJsimilar-to˙𝑀superscript107subscript𝑀J\dot{M}\sim 10^{-7}M_{\mathrm{J}}over˙ start_ARG italic_M end_ARG ∼ 10 start_POSTSUPERSCRIPT - 7 end_POSTSUPERSCRIPT italic_M start_POSTSUBSCRIPT roman_J end_POSTSUBSCRIPT yr−11{}^{-1}start_FLOATSUPERSCRIPT - 1 end_FLOATSUPERSCRIPT even towards highly extincted environments (AV≈50subscript𝐴V50A_{\mathrm{V}}\approx 50italic_A start_POSTSUBSCRIPT roman_V end_POSTSUBSCRIPT ≈ 50 mag).","['Direct imaging (387) —', 'Exoplanet formation(492) —', 'History of astronomy(1868) —', 'Interdisciplinary astronomy(804)']","['USA', 'USA', 'USA', 'Canada', 'Canada', 'USA', 'Canada', 'Canada', 'USA', 'USA', 'USA', 'USA', 'USA', 'USA', 'USA', 'USA', 'USA', 'USA', 'USA', 'USA', 'USA']"
"Many mechanisms have been proposed to alleviate the magnetic catastrophe, which prevents the Keplerian disk from forming inside a collapsing magnetized core. Such propositions include inclined field and non-ideal magnetohydrodynamics effects, and have been supported with numerical experiments. Models have been formulated for typical disk sizes when a field threads the rotating disk, parallel to the rotation axis, while observations at the core scales do not seem to show evident correlation between the directions of angular momentum and the magnetic field. In the present study, we propose a new model that considers both vertical and horizontal fields and discuss their effects on the protoplanetary disk size.",['diffusion – gravitation – magnetohydrodynamics (MHD) – protoplanetary disks'],"['Taiwan', 'Taiwan', 'Taiwan', 'India', 'Taiwan', 'France', 'France', 'France', 'France']"
"Knowing who follows whom and what patterns they are following are crucial steps to understand collective behaviors (e.g. a group of human, a school of fish, or a stock market). Time series is one of resources that can be used to get insight regarding following relations. However, the concept of following patterns or motifs and the solution to find them in time series are not obvious. In this work, we formalize a concept of following motifs between two time series and present a framework to infer following patterns between two time series. The framework utilizes one of efficient and scalable methods to retrieve motifs from time series called the Matrix Profile Method. We compare our proposed framework with several baselines. The framework performs better than baselines in the simulation datasets. In the dataset of sound recording, the framework is able to retrieve the following motifs within a pair of time series that two singers sing following each other. In the cryptocurrency dataset, the framework is capable of capturing the following motifs within a pair of time series from two digital currencies, which implies that the values of one currency follow the values of another currency patterns. Our framework can be utilized in any field of time series to get insight regarding following patterns between time series.","['time series, following relations, matrix profile, data science']","['1212BangkokThailand43017-6221', 'PathumthaniThailand']"
"The isolated globule B335 contains a single,
low luminosity Class 0 protostar associated with a bipolar nebula
and outflow system seen nearly perpendicular to its axis.
We observed the innermost regions of this outflow
as part of JWST/NIRCam GTO program 1187,
primarily intended for wide-field slitless spectroscopy of background stars behind the globule.
We find a system of expanding shock fronts with kinematic ages of only a few decades emerging
symmetrically
from the position of the embedded protostar, which is not directly
detected at NIRCam wavelengths.
The innermost and youngest of the shock fronts studied here shows strong emission from CO.
The next older shock front shows less CO and the third shock front shows only H22{}_{2}start_FLOATSUBSCRIPT 2 end_FLOATSUBSCRIPT emission in our data.
This third and most distant of these inner shock fronts shows substantial
evolution of its shape since it was last observed with high spatial resolution in 1996 with Keck/NIRC.
This may be evidence of a faster internal shock catching up with a slower one and of the two shocks merging.","['Young stellar objects (1834) —', 'Protostars (1302) —', 'Cometary globules (276) —', 'Stellar jets (1607) —']","['USA', '91125', 'USA', 'USA', 'Canada', 'Canada', 'USA', 'USA', 'USA', '91125', 'USA', 'USA', 'USA', '20024']"
"In modern pathology, multiplexed immunofluorescence (mIF) and multiplex immunohistochemistry (mIHC) bring both vast opportunities and challenges. These techniques illuminate complex tumor microenvironment interactions, necessitating intuitive visualization tools. With the rise of electronic health records (EHR) and information overload for physicians, integrating advanced technologies becomes essential. Enter SpatialVisVR: a versatile VR platform for comparing medical images, adaptable for data privacy on embedded hardware. Clinicians can capture pathology slides in real-time via mobile, then SpatialVisVR employs a deep learning algorithm to match and display similar mIF images. This interface allows for adding or removing up to 100 multiplexed protein channels, aiding immuno-oncology decisions. Ultimately, SpatialVisVR aims to refine diagnostic processes, promoting a holistic, efficient approach to immuno-oncology research and treatment.","['Virtual', 'Reality,', 'Augmented', 'Reality,', 'Mixed', 'Reality,', 'H&E,', 'CODEX,', 'Digital', 'Pathology,', 'Edge', 'Detection,', 'Search']","['ArlingtonArlingtonTexasUSA76010', 'ArlingtonArlingtonTexasUSA', 'ArlingtonArlingtonTexasUSA', 'ArlingtonArlingtonTexasUSA', 'ArlingtonArlingtonTexasUSA', 'ArlingtonArlingtonTexasUSA', 'ArlingtonArlingtonTexasUSA', 'ArlingtonArlingtonTexasUSA', 'ArlingtonArlingtonTexasUSA', 'AngelesCaliforniaUSA', 'ArlingtonArlingtonTexasUSA']"
"We report dynamical mass measurements of the individual stars in the most luminous and massive stellar member of the nearby Ophiuchus star-forming region, the young tight binary system S1. We combine 28 archival datasets with seven recent, proprietary VLBA observations obtained as part of the Dynamical Masses of Young Stellar Multiple Systems with the VLBA project (DYNAMO–VLBA), to constrain the astrometric and orbital parameters of the system, and recover high accuracy dynamical masses. The primary component, S1A, is found to have a mass of 4.11±plus-or-minus\pm±0.10 M⊙direct-product{}_{\odot}start_FLOATSUBSCRIPT ⊙ end_FLOATSUBSCRIPT, significantly less than the typical value, ∼similar-to\sim∼ 6 M⊙direct-product{}_{\odot}start_FLOATSUBSCRIPT ⊙ end_FLOATSUBSCRIPT previously reported in the literature. We show that the spectral energy distribution of S1A can be reproduced by a reddened blackbody with a temperature between roughly 14,000 K and 17,000 K. According to evolutionary models, this temperature range corresponds to stellar masses between 4 M⊙direct-product{}_{\odot}start_FLOATSUBSCRIPT ⊙ end_FLOATSUBSCRIPT and 6 M⊙direct-product{}_{\odot}start_FLOATSUBSCRIPT ⊙ end_FLOATSUBSCRIPT so the SED is not a priori inconsistent with the dynamical mass of S1A. The luminosity of S1 derived from SED-fitting, however, is only consistent with models for stellar masses above 5 M⊙direct-product{}_{\odot}start_FLOATSUBSCRIPT ⊙ end_FLOATSUBSCRIPT. Thus, we cannot reconcile the evolutionary models with the dynamical mass measurement of S1A: the models consistent with the location of S1A in the HR diagram correspond to masses at least 25% higher than the dynamical mass. For the secondary component, S1B, a mass of 0.831 ±plus-or-minus\pm± 0.014 M⊙direct-product{}_{\odot}start_FLOATSUBSCRIPT ⊙ end_FLOATSUBSCRIPT is determined, consistent with a low-mass young star. While the radio flux of S1A remains roughly constant throughout the orbit, the flux of S1B is found to be higher near the apastron.",['astrometry — stars:formation — stars:kinematics'],"['México', 'México', 'Germany', 'France', 'México', 'México', 'Germany', 'USA', 'México', 'Germany', 'Brazil', 'UK', 'México', 'USA']"
"As the decarbonization of power systems accelerates, there has been increasing interest in capacity expansion models for their role in guiding this transition. Representative period selection is an important component of capacity expansion modeling, enabling computational tractability of optimization while ensuring fidelity between the representative periods and the full year. However, little attention has been devoted to selecting representative periods longer than a single day. This prevents the capacity expansion model from directly simulating interday energy sharing, which is of key importance as energy generation becomes more variable and storage more important. To this end, we propose a novel method for selecting representative periods of any length. The method is validated using a capacity expansion model and production cost model based on California’s decarbonization goals. We demonstrate that the representative period length has a substantial impact in the results of the capacity expansion investment plan.","['Index', 'Terms: ', 'Capacity expansion planning, representative period selection, production cost modeling.']","['CA', 'WA']"
"This paper presents a two stage approach for the processing of frequency-stacked mobile sub-bands. The frequency stacking is performed in the analogue domain to enable the use of a wideband ADC, instead of employing multiple narrowband ADCs, to support multiple antenna elements for digital satellite beamforming. This analogue front end provides a common broadband digital interface to the on-board processor and can be configured to support multiple satellite missions, reducing the cost of commissioning a digital processor for individual satellite missions. This paper proposes a framework on the specification of digital prototype filter for the Analysis of frequency-stacked mobile sub-bands. The computational complexity of the Analysis operation, with two digital filter alternatives, are evaluated. A series of results, taken from our European Space Agency sponsored project, are presented here to demonstrate the applicability of the proposed two stage approach, reporting on the savings in power consumption when an Nth-band all-pass based recursive filter having an Infinite Impulse Response is used as the digital prototype filter.","['Index', 'Terms: ', 'Satellite', 'Communication', 'Systems,', 'Digital', 'Signal', 'Processing,', 'Discrete', 'Fourier', 'Transform,', 'Digital', 'Filter', 'Bank,', 'Analog to', 'Digital', 'Convertor.']","['christoph.ernst}@esa.int', 'robert.ro.hughes}@airbus.com', 'kalei@westminster.ac.uk']"
"Supporting the interactive exploration of large datasets is a popular and challenging use case for data management systems.
Traditionally, the interface and the back-end system are built and optimized separately, and interface design and system optimization require different skill sets that are difficult for one person to master.
To enable analysts to focus on visualization design, we contribute VegaPlus, a system that automatically optimizes interactive dashboards to support large datasets.
To achieve this, VegaPlus leverages two core ideas. First, we introduce an optimizer that can reason about execution plans in Vega, a back-end DBMS, or a mix of both environments. The optimizer also considers how user interactions may alter execution plan performance, and can partially or fully rewrite the plans when needed.
Through a series of benchmark experiments on seven different dashboard designs, our results show that VegaPlus provides superior performance and versatility compared to standard dashboard optimization techniques.","['data analytics, scalable visualization']","['WashingtonSeattleUSA', 'UniversityPittsburghUSA', 'ParkUSA', 'UniversityPittsburghUSA', 'WashingtonSeattleUSA']"
"Semantic embeddings play a crucial role in natural language-based information retrieval.
Embedding models represent words and contexts as vectors whose spatial configuration is derived from the distribution of words in large text corpora.
While such representations are generally very powerful, they might fail to account for fine-grained domain-specific nuances.
In this article, we investigate this uncertainty for the domain of characterizations of expressive piano performance.
Using a music research dataset of free text performance characterizations and a follow-up study sorting the annotations into clusters, we derive a ground truth for a domain-specific semantic similarity structure.
We test five embedding models and their similarity structure for correspondence with the ground truth.
We further assess the effects of contextualizing prompts, hubness reduction, cross-modal similarity, and k-means clustering.
The quality of embedding models shows great variability with respect to this task; more general models perform better than domain-adapted ones and the best model configurations reach human-level agreement.","['Semantic', 'Similarity,', 'Embeddings,', 'Evaluation,', 'Music', 'Performance']","['UniversityLinzAustria', 'UniversityLinzAustria', 'UniversityLinzAustria', 'UniversityLinzAustria']"
"Large Language Models (LLMs) have gained considerable notoriety in the field of natural language to SQL tasks (NL2SQL). In this study, we show how task decomposition can greatly benefit LLMs in database understanding and query generation in order to answer human questions with an SQL query.
We fined-tuned open source models, specifically Llama-2 and Code Llama, by combining 2 different models each designated to focus on one of two tasks in order to leverage each model’s core competency to further increase the accuracy of the final SQL query.
We propose a new framework to divide the schema into chunks in order to fit more information into a limited context. Our results are comparable with those obtained by GPT-4 at the same time being ~135 times smaller, 90 times faster and more than 100 times cheaper than GPT-4.","['NL2SQL,', 'LLM,', 'AI,', 'Prompt', 'Decomposition']","['SpaSantiagoChile', 'SpaSantiagoChile', 'SpaSantiagoChile']"
"The clustering coefficient is a valuable tool for understanding the structure of complex networks. It is widely used to analyze social networks, biological networks, and other complex systems. While there is generally a single common definition for the local clustering coefficient, there are two different ways to calculate the global clustering coefficient. The first approach takes the average of the local clustering coefficients for each node in the network. The second one is based on the ratio of closed triplets to all triplets. It is shown that these two definitions of the global clustering coefficients are strongly inequivalent and may significantly impact the accuracy of the outcome.",['complex networks; statistical mechanics; graph ensembles; clustering coefficient; hidden variables; graph temperature'],['México']
"Lifshitz transitions are topological transitions of a Fermi surface, whose signatures typically appear in the conduction properties of a host metal. Here, we demonstrate, using an extended Falicov-Kimball model of a two-flavor fermion system, that a Lifshitz transition which occurs in the non-interacting limit impacts interaction-induced insulating phases, even though they do not host Fermi surfaces.
For strong interactions we find a first order transition between states of different polarization
This transition line ends in a very unusual quantum critical endpoint, whose presence is stabilized by the onset of inter-flavor coherence. We demonstrate that the surfaces of maximum coherence in these states reflect the distinct Fermi surface topologies of the states separated by the non-interacting Lifshitz transition.
The phenomenon is shown to be independent of the band topologies involved. Experimental realizations of our results are discussed for both electronic and optical lattice systems.","['first keyword, second keyword, third keyword']","['USA', 'USA', 'USA', 'USA']"
"In this paper, we investigate the quantum entanglement induced by phase-space noncommutativity. Both the position-position and momentum-momentum noncommutativity are incorporated to study the entanglement properties of coordinate and momentum degrees of freedom under the shade of oscillators in noncommutative space. Exact solutions for the systems are obtained after the model is re-expressed in terms of canonical variables, by performing a particular Bopp’s shift to the noncommuting degrees of freedom.
It is shown that the bipartite Gaussian state for an isotropic oscillator is always separable. To extend our study for the time-dependent system, we allow arbitrary time dependency on parameters. The time-dependent isotropic oscillator is solved with the Lewis-Riesenfeld invariant method. It turns out that even for arbitrary time-dependent scenarios, the separability property does not alter.

We extend our study to the anisotropic oscillator, which provides an entangled state even for time-independent parameters. The Wigner quasi-probability distribution is constructed for a bipartite Gaussian state. The noise matrix (covariance matrix) is explicitly studied with the help of Wigner distribution. Simon’s separability criterion (generalized Peres-Horodecki criterion) has been employed to find the unique function of the (mass and frequency) parameters, for which the bipartite states are separable. In particular, we show that the mere inclusion of non-commutativity of phase-space is not sufficient to generate the entanglement, rather anisotropy is important at the same footing.","['Noncommutative space;', 'Peres-Horodecki separability criterion;', 'Wigner distribution;', 'Time-dependent system;', 'Anisotropic oscillator']","['700131.', '700131.', 'India-700108']"
"PDS 70 hosts two massive, still-accreting planets and the inclined orientation of its protoplanetary disk presents a unique opportunity to directly probe the vertical gas structure of a planet-hosting disk. Here, we use high-spatial-resolution (≈0⁢.′′⁢1absent0arcsecond1{\approx}0\farcs 1≈ 0 start_ID start_POSTFIX SUPERSCRIPTOP . ′ ′ end_POSTFIX end_ID 1;10 au) observations in a set of CO isotopologue lines and HCO+{}^{+}start_FLOATSUPERSCRIPT + end_FLOATSUPERSCRIPT J=4–3 to map the full 2D (r,z)𝑟𝑧(r,z)( italic_r , italic_z ) disk structure from the disk atmosphere, as traced by 1212{}^{12}start_FLOATSUPERSCRIPT 12 end_FLOATSUPERSCRIPTCO, to closer to the midplane, as probed by less abundant isotopologues and HCO+{}^{+}start_FLOATSUPERSCRIPT + end_FLOATSUPERSCRIPT. In the PDS 70 disk, 1212{}^{12}start_FLOATSUPERSCRIPT 12 end_FLOATSUPERSCRIPTCO traces a height of z/r≈0.3𝑧𝑟0.3z/r\approx 0.3italic_z / italic_r ≈ 0.3, 1313{}^{13}start_FLOATSUPERSCRIPT 13 end_FLOATSUPERSCRIPTCO is found at z/r≈0.1𝑧𝑟0.1z/r\approx 0.1italic_z / italic_r ≈ 0.1, and C1818{}^{18}start_FLOATSUPERSCRIPT 18 end_FLOATSUPERSCRIPTO originates at, or near, the midplane. The HCO+{}^{+}start_FLOATSUPERSCRIPT + end_FLOATSUPERSCRIPT surface arises from z/r≈0.2𝑧𝑟0.2z/r\approx 0.2italic_z / italic_r ≈ 0.2 and is one of the few non-CO emission surfaces constrained with high fidelity in disks to date. In the 1212{}^{12}start_FLOATSUPERSCRIPT 12 end_FLOATSUPERSCRIPTCO J=3–2 line, we resolve a vertical dip and steep rise in height at the cavity wall, making PDS 70 the first transition disk where this effect is directly seen in line emitting heights. In the outer disk, the CO emission heights of PDS 70 appear typical for its stellar mass and disk size and are not substantially altered by the two inner embedded planets. By combining CO isotopologue and HCO+{}^{+}start_FLOATSUPERSCRIPT + end_FLOATSUPERSCRIPT lines, we derive the 2D gas temperature structure and estimate a midplane CO snowline of ≈{\approx}≈56-85 au. This implies that both PDS 70b and 70c are located interior to the CO snowline and are likely accreting gas with a high C/O ratio of ≈{\approx}≈1.0, which provides context for future planetary atmospheric measurements from, e.g., JWST, and for properly modeling their formation histories.","['Protoplanetary disks (1300) —', 'Planet formation (1241) —', 'CO line emission (262) —', 'High angular resolution (2167)']","['Fellow', 'USA', 'USA', 'France', 'France', 'Italy', 'USA', 'USA', 'USA', 'Netherlands', 'USA', 'Netherlands', 'Italy']"
"Multi-wavelength spectroscopy of NGC 5548 revealed remarkable changes due to presence of an obscuring wind from the accretion disk. This broadened our understanding of obscuration and outflows in AGN. Swift monitoring of NGC 5548 shows that over the last 10 years the obscuration has gradually declined. This provides a valuable opportunity for analyses that have not been feasible before because of too much obscuration. The lowered obscuration, together with the high energy spectral coverage of Chandra HETG, facilitate the first study of X-ray absorption lines in the obscured state. The comparison of the lines (Mg xi, Mg xii, Si xiii, and Si xiv) between the new and historical spectra reveals interesting changes, most notably the He-like absorption being significantly diminished in 2022. Our study finds that the changes are caused by an increase in both the ionization parameter and the column density of the warm-absorber outflow in the obscured state. This is contrary to the shielding scenario that is evident in the appearance of the UV lines, where the inner obscuring wind shields outflows that are located further out, thus lowering their ionization. The X-ray absorption lines in the HETG spectra appear to be unaffected by the obscuration. The results suggest that the shielding is complex since various components of the ionized outflow are impacted differently. We explore various possibilities for the variability behavior of the X-ray absorption lines and find that the orbital motion of a clumpy ionized outflow traversing our line of sight is the most likely explanation.","['accretion disks – galaxies: active – galaxies: individual (NGC 5548) — quasars: absorption lines — techniques: spectroscopic —', 'X-rays: galaxies']","['mmehdipour@stsci.edu', 'mmehdipour@stsci.edu', 'Netherlands', 'Netherlands', 'Netherlands', 'Netherlands', 'Netherlands', 'Japan', 'Netherlands', 'UK', 'China', 'Netherlands', 'USA', 'USA']"
"With the rise of human-machine communication, machines are increasingly designed with humanlike characteristics, such as gender, which can inadvertently trigger cognitive biases. Many conversational agents (CAs), such as voice assistants and chatbots, are predominantly designed with female personas. This leads to concerns about perpetuating gender stereotypes and inequality. Critiques have emerged regarding the potential objectification of females and the reinforcement of gender stereotypes by these technologies. This research, situated in conversational AI design, aims to delve deeper into the impacts of gender biases in human-CA interactions. From a behavioral and communication research standpoint, this program focuses not only on perceptions but also the linguistic styles of users when interacting with CAs, as previous research has rarely explored. It aims to understand how pre-existing gender biases might be triggered by CAs’ gender designs. It further investigates how CAs’ gender designs may reinforce gender biases and extend them to human-human communication. The findings aim to inform the ethical design of conversational agents, addressing whether gender assignment in CAs is appropriate and how to promote gender equality in design.","['conversational agent, cognitive biases, gender,', 'UX research']",['4116ChampaignILUSA61820']
"We report multi-epoch astrometric VLBI observations of the chromospherically active binary HR 1099 (V711 Tau, HD 22468) at six epochs over 63 days using the Very Long Baseline Array at 22.2 GHz. We determined hourly radio centroid positions at each epoch with a positional uncertainty significantly smaller than the component separation. The aggregate radio positions at all epochs define an ellipse in the co-moving reference frame with an inclination i=39.5⁢°⁢−3.5⁢°+3.6⁢°𝑖39.5°subscriptsuperscriptabsent3.6°3.5°i={39.5\arcdeg}{\raisebox{2.15277pt}{\tiny${}^{+3.6\arcdeg}_{-3.5\arcdeg}$}}italic_i = 39.5 ° start_FLOATSUPERSCRIPT + 3.6 ° end_FLOATSUPERSCRIPT start_POSTSUBSCRIPT - 3.5 ° end_POSTSUBSCRIPT and longitude of ascending node Ω=212⁢°±22⁢°Ωplus-or-minus212°22°\Omega=212\arcdeg\pm 22\arcdegroman_Ω = 212 ° ± 22 °. The ellipse center is offset from the Third Gaia Celestial Reference Frame position by Δ⁢αΔ𝛼\Delta\alpharoman_Δ italic_α = −0.81⁢−0.37+0.250.81subscriptsuperscriptabsent0.250.37{-0.81}{\raisebox{2.15277pt}{\tiny${}^{+0.25}_{-0.37}$}}- 0.81 start_FLOATSUPERSCRIPT + 0.25 end_FLOATSUPERSCRIPT start_POSTSUBSCRIPT - 0.37 end_POSTSUBSCRIPT mas, Δ⁢δΔ𝛿\Delta\deltaroman_Δ italic_δ = 0.45⁢−0.25+0.230.45subscriptsuperscriptabsent0.230.25{0.45}{\raisebox{2.15277pt}{\tiny${}^{+0.23}_{-0.25}$}}0.45 start_FLOATSUPERSCRIPT + 0.23 end_FLOATSUPERSCRIPT start_POSTSUBSCRIPT - 0.25 end_POSTSUBSCRIPT mas. All radio centroids are well-displaced from the binary center of mass at all epochs, ruling out emission from the inter-binary region. We examined the motion of the radio centroids within each epoch by comparing hourly positions over several hours. The measured speeds were not statistically significant for five of the six epochs, with 2⁢σ2𝜎2\sigma2 italic_σ upper limits in the range 200–1000 km sec−11{}^{-1}start_FLOATSUPERSCRIPT - 1 end_FLOATSUPERSCRIPT. However, for one flaring epoch, there was a ∼3⁢σsimilar-toabsent3𝜎\sim 3\sigma∼ 3 italic_σ detection v⟂=228±85subscript𝑣perpendicular-toplus-or-minus22885v_{\perp}=228\pm 85italic_v start_POSTSUBSCRIPT ⟂ end_POSTSUBSCRIPT = 228 ± 85 km sec−11{}^{-1}start_FLOATSUPERSCRIPT - 1 end_FLOATSUPERSCRIPT. This speed is comparable to the mean speed of observed coronal mass ejections on the Sun.","['Stellar coronae (305) —', 'Interferometric binary stars (806) —', 'Magnetic stars (995) —', 'Plasma astrophysics (1261) —', 'Radio astrometry (1337) —', 'Radio continuum emission (1340) —', 'RS', 'Canum', 'Venaticorum variable stars (1416) —', 'Spectroscopic binary stars (1557) —', 'Starlight polarization (1571) —', 'Starspots (1572) —', 'Stellar magnetic fields (1610)']","['USA', 'USA', 'USA', 'USA']"
"We present Very Large Array (VLA) 1.3 cm continuum and 22.2 GHz H22{}_{2}start_FLOATSUBSCRIPT 2 end_FLOATSUBSCRIPTO maser observations of the high-mass protostellar object IRAS 19035+0641 A. Our observations unveil an elongated bipolar 1.3 cm continuum structure at scales ≲500less-than-or-similar-toabsent500\lesssim 500\,≲ 500au which, together with a rising in-band spectral index, strongly suggests that the radio emission toward IRAS 19035+0641 A arises from an ionized jet. In addition, eight individual water maser spots well aligned with the jet axis were identified.
The Stokes V spectrum of the brightest H22{}_{2}start_FLOATSUBSCRIPT 2 end_FLOATSUBSCRIPTO maser line (∼100similar-toabsent100\sim 100\,∼ 100Jy) shows a possible Zeeman splitting and is well represented by the derivatives of two Gaussian components fitted to the Stokes I profile.
The measured Blossubscript𝐵losB_{\mathrm{los}}italic_B start_POSTSUBSCRIPT roman_los end_POSTSUBSCRIPT are 123⁢(±27)123plus-or-minus27123\,(\pm 27)123 ( ± 27 ) and 156⁢(±8)156plus-or-minus8156\,(\pm 8)\,156 ( ± 8 )mG, translating to a pre-shock magnetic field of ≈7absent7\approx 7\,≈ 7mG.
Subsequent observations to confirm the Zeeman splitting showed intense variability in all the water maser spots, with the brightest maser completely disappearing.
The observed variability in a one-year time scale could be the result of an accretion event. These findings strengthen our interpretation of IRAS 19035+0641 A as a high-mass protostar in an early accretion/outflow evolutionary phase.","['Unified', 'Astronomy', 'Thesaurus concepts:', 'Jets (870);', 'Radio jets (1347);', 'Star forming regions (1565);', 'Star formation (1569);', 'Astrophysical masers (103);', 'Star formation (1569);', 'Protostars (1302);', 'Young stellar objects (1834);', 'Interstellar magnetic fields (845)']","['USA.', 'USA.', 'USA', 'USA.', 'USA.', 'USA', 'USA.', 'USA.']"
"We present molecular line observations of the protostellar outflow associated with HH270mms1 in the Orion B molecular cloud with ALMA.
The \ce^12CO(J𝐽Jitalic_J= 3 – 2) emissions show that the outflow velocity structure consists of four distinct components of low (≲10⁢km s−1less-than-or-similar-toabsent10superscriptkm s1\lesssim 10\,\text{km\,s}^{-1}≲ 10 km s start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT), intermediate (∼10−25⁢km s−1similar-toabsent1025superscriptkm s1\sim 10-25\,\text{km\,s}^{-1}∼ 10 - 25 km s start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT) and high (≳40⁢km s−1greater-than-or-equivalent-toabsent40superscriptkm s1\gtrsim 40\,\text{km\,s}^{-1}≳ 40 km s start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT) velocities in addition to the entrained gas velocity (∼25−40⁢km s−1similar-toabsent2540superscriptkm s1\sim 25-40\,\text{km\,s}^{-1}∼ 25 - 40 km s start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT).
The high- and intermediate-velocity flows have well-collimated structures surrounded by the low-velocity flow.
The chain of knots is embedded in the high-velocity flow or jet, which is the evidence of episodic mass ejections induced by time-variable mass accretion.
We could detect the velocity gradients perpendicular to the outflow axis in both the low- and intermediate-velocity flows.
We confirmed the rotation of the envelope and disk in the \ce^13CO and \ceC^17O emission and found that their velocity gradients are the same as those of the outflow.
Thus, we concluded that the velocity gradients in the low- and intermediate-velocity flows are due to the outflow rotation.
Using observational outflow properties, we estimated the outflow launching radii to be 67.1−77.1⁢au67.177.1au67.1-77.1\,\text{au}67.1 - 77.1 au for the low-velocity flow and 13.3−20.8⁢au13.320.8au13.3-20.8\,\text{au}13.3 - 20.8 au for the intermediate-velocity flow.
Although we could not detect the rotation in the jets due to the limited spatial resolution, we estimated the jet launching radii to be (2.36−3.14)×10−2⁢au2.363.14superscript102au(2.36-3.14)\times 10^{-2}\,\text{au}( 2.36 - 3.14 ) × 10 start_POSTSUPERSCRIPT - 2 end_POSTSUPERSCRIPT au using the observed velocity of each knots.
Thus, the jet is driven from the inner disk region.
We could identify the launching radii of distinct velocity components within a single outflow with all the prototypical characteristics expected from recent theoretical works.","['Star formation(1569) ;', 'Protostars(1302)']","['Japan', 'Japan', 'Japan', 'Japan']"
"As a powerful tool for modeling graph data, Graph Neural Networks (GNNs) have received increasing attention in both academia and industry. Nevertheless, it is notoriously difficult to deploy GNNs on industrial scale graphs, due to their huge data size and complex topological structures. In this paper, we propose GLISP, a sampling based GNN learning system for industrial scale graphs. By exploiting the inherent structural properties of graphs, such as power law distribution and data locality, GLISP addresses the scalability and performance issues that arise at different stages of the graph learning process. GLISP consists of three core components: graph partitioner, graph sampling service and graph inference engine. The graph partitioner adopts the proposed vertex-cut graph partitioning algorithm AdaDNE to produce balanced partitioning for power law graphs, which is essential for sampling based GNN systems. The graph sampling service employs a load balancing design that allows the one hop sampling request of high degree vertices to be handled by multiple servers. In conjunction with the memory efficient data structure, the efficiency and scalability are effectively improved. The graph inference engine splits the K𝐾Kitalic_K-layer GNN into K𝐾Kitalic_K slices and caches the vertex embeddings produced by each slice in the data locality aware hybrid caching system for reuse, thus completely eliminating redundant computation caused by the data dependency of graph. Extensive experiments show that GLISP achieves up to 6.53×6.53\times6.53 × and 70.77×70.77\times70.77 × speedups over existing GNN systems for training and inference tasks, respectively, and can scale to the graph with over 10 billion vertices and 40 billion edges with limited resources.","['Index', 'Terms: ', 'Machine', 'Learning', 'Techniques,', 'Graph', 'Graph', 'Neural', 'Network,', 'Load', 'Balancing']","['zhongshu.zzs@antgroup.com', 'jingbin.jb@antgroup.com', 'ranghou.wxp@antgroup.com', 'zhizhen.lzz@antgroup.com', 'leywar.liang@antgroup.com', 'jun.zhoujun@antfin.com']"
"Due to manufacturing variabilities and temperature gradients within an electric vehicle’s battery pack, the capacities of cells in it decrease differently over time.
This reduces the usable capacity of the battery – the charge levels of one or more cells might be at the minimum threshold while most of the other cells have residual charge.
Active cell balancing (i.e., transferring charge among cells) can equalize their charge levels, thereby increasing the battery pack’s usable capacity.
But performing balancing means additional charge transfer, which can result in energy loss and cell aging, akin to memory aging in storage technologies due to writing.
This paper studies when cell balancing should be optimally triggered to minimize aging while maintaining the necessary driving capability.
In particular, we propose optimization strategies for cell balancing while minimizing their impact on aging. By borrowing terminology from the storage domain, we refer to this as “wear leveling-aware” active balancing.","['Index', 'Terms: \n', 'Active', 'Cell', 'Balancing,', 'Charge', 'Equalization,', 'Battery', 'Management,', 'Modeling,', 'Simulation']","['Italy.', 'Korea.', 'USA.', 'USA.', 'Germany.']"
"Novice programmers need to learn how to write basic code but often face difficulties when coding independently. To assist struggling students, we have recently implemented personalized Parsons problems as a pop-up scaffolding. Students found them to be more engaging and helpful for learning compared to simply receiving the correct answer, such as the response they might get from Large Language Model (LLM) tools like ChatGPT. However, a drawback of using Parsons problems as scaffolding is that students may be able to put the code blocks back in place without fully understanding the rationale of the correct solution. As a result, the learning benefits of such scaffolding are compromised. Our goal is to enhance the advantages of using personalized Parsons problems as scaffolding by improving their comprehension through code explanations. In this poster, we propose designs that incorporate multiple levels of textual explanations in the Parsons problems. This design will be used for future technical evaluation and classroom experiments. These experiments will explore the effectiveness of adding textual explanations to Parsons problems to improve instructional benefits.","['Introductory', 'Programming,', 'Code', 'Explanations,', 'Parsons', 'Problems,', 'Code', 'Writing,', 'Scaffolding,', 'Hint,', 'Large', 'Language', 'Models']","['ArborMichiganUSA', 'ArborMichiganUSA', 'ArborMichiganUSA']"
"Log anomaly detection plays a critical role in ensuring the security and maintenance of modern software systems. At present, the primary approach for detecting anomalies in log data is through supervised anomaly detection. Nonetheless, existing supervised methods heavily rely on labeled data, which can be frequently limited in real-world scenarios. In this paper, we propose a semi-supervised log anomaly detection method that combines the DQN algorithm from deep reinforcement learning, which is called DQNLog. DQNLog leverages a small amount of labeled data and a large-scale unlabeled dataset, effectively addressing the challenges of imbalanced data and limited labeling. This approach not only learns known anomalies by interacting with an environment biased towards anomalies but also discovers unknown anomalies by actively exploring the unlabeled dataset. Additionally, DQNLog incorporates a cross-entropy loss term to prevent model overestimation during Deep Reinforcement Learning (DRL). Our evaluation on three widely-used datasets demonstrates that DQNLog significantly improves recall rate and F1-score while maintaining precision, validating its practicality.","['Log', 'Analysis,', 'Anomaly', 'Detection,', 'Deep', 'Reinforcement learning,', 'Imbalanced data']","['ShiChina', 'ShiChina', 'ShiChina']"
"With the rapid growth of cloud services driven by advancements in web service technology, selecting a high-quality service from a wide range of options has become a complex task. This study aims to address the challenges of data sparsity and the cold-start problem in web service recommendation using Quality of Service (QoS). We propose a novel approach called QoS-aware graph contrastive learning (QAGCL) for web service recommendation. Our model harnesses the power of graph contrastive learning to handle cold-start problems and improve recommendation accuracy effectively. By constructing contextually augmented graphs with geolocation information and randomness, our model provides diverse views. Through the use of graph convolutional networks and graph contrastive learning techniques, we learn user and service embeddings from these augmented graphs. The learned embeddings are then utilized to seamlessly integrate QoS considerations into the recommendation process. Experimental results demonstrate the superiority of our QAGCL model over several existing models, highlighting its effectiveness in addressing data sparsity and the cold-start problem in QoS-aware service recommendations. Our research contributes to the potential for more accurate recommendations in real-world scenarios, even with limited user-service interaction data.","['Index', 'Terms: ', 'Quality of', 'Service,', 'Web', 'Service,', 'Service', 'Recommendation,', 'Graph', 'Contrastive', 'Learning']","['jeongwhan.choi@yonsei.ac.kr', 'duksan.ryu@jbnu.ac.kr']"
"Text-video retrieval is a challenging task that aims to identify relevant videos given textual queries. Compared to conventional textual retrieval, the main obstacle for text-video retrieval is the semantic gap between the textual nature of queries and the visual richness of video content.
Previous works primarily focus on aligning the query and the video by finely aggregating word-frame matching signals.
Inspired by the human cognitive process of modularly judging the relevance between text and video, the judgment needs high-order matching signal due to the consecutive and complex nature of video contents.
In this paper, we propose chunk-level text-video matching, where the query chunks are extracted to describe a specific retrieval unit, and the video chunks are segmented into distinct clips from videos.
We formulate the chunk-level matching as n-ary correlations modeling between words of the query and frames of the video and introduce a multi-modal hypergraph for n-ary correlation modeling.
By representing textual units and video frames as nodes and using hyperedges to depict their relationships, a multi-modal hypergraph is constructed. In this way, the query and the video can be aligned in a high-order semantic space.
In addition, to enhance the model’s generalization ability, the extracted features are fed into a variational inference component for computation, obtaining the variational representation under the Gaussian distribution.
The incorporation of hypergraphs and variational inference allows our model to capture complex, n-ary interactions among textual and visual contents.
Experimental results demonstrate that our proposed method achieves state-of-the-art performance on the text-video retrieval task.","['text-video retrieval, multi-modal hypergraph, hypergraph neural networks']","['Inc.BeijingChina', 'Inc.BeijingChina', 'UniversityWaterlooCanada', 'Inc.BeijingChina', 'CASBeijingChina', 'Inc.BeijingChina', 'Inc.BeijingChina', 'Inc.BeijingChina', 'Inc.BeijingChina']"
"Intelligent metasurfaces are one of the favorite technologies for integrating sixth-generation (6G) networks, especially the reconfigurable intelligent surface (RIS) that has been extensively researched in various applications. In this context, a feature that deserves further exploration is the frequency scattering occurs when the elements are periodically switched, referred to as Space-Time-Coding metasurface (STCM) topology. This type of topology causes impairments to the established communication methods by generating undesirable interference both in frequency and space, which is worsened when using wideband signals. Nevertheless, it has the potential to bring forward useful features for sensing and localization. This work exploits STCM sensing capabilities in target detection, localization, and classification using narrowband downlink pilot signals at the base station (BS). The results of this novel approach reveal the ability to retrieve a scattering point (SP) localization within the sub-centimeter and sub-decimeter accuracy depending on the SP position in space. We also analyze the associated detection and classification probabilities, which show reliable detection performance in the whole analyzed environment. In contrast, the classification is bounded by physical constraints, and we conclude that this method presents a promising approach for future integrated sensing and communications (ISAC) protocols by providing a tool to perform sensing and localization services using legacy communication signals.","['Index', 'Terms: \n', 'Integrated', 'Sensing and', 'Communication,', 'Space-time-coding digital metasurfaces,', 'Cramér-Rao bounds.']",['petarp@es.aau.dk']
"Large-language models (LLMs) hold significant promise in improving human-robot interaction, offering advanced conversational skills and versatility in managing diverse, open-ended user requests in various tasks and domains.
Despite the potential to transform human-robot interaction, very little is known about the distinctive design requirements for utilizing LLMs in robots, which may differ from text and voice interaction and vary by task and context.
To better understand these requirements, we conducted a user study (n=32𝑛32n=32italic_n = 32) comparing an LLM-powered social robot against text- and voice-based agents, analyzing task-based requirements in conversational tasks, including choose, generate, execute, and negotiate.
Our findings show that LLM-powered robots elevate expectations for sophisticated non-verbal cues and excel in connection-building and deliberation, but fall short in logical communication and may induce anxiety.
We provide design implications both for robots integrating LLMs and for fine-tuning LLMs for use with robots.",['Social robots; large language\nmodels; human-robot interaction'],"['USA', 'USA', 'USA']"
"The advent of MiniApps, operating within larger SuperApps, has revolutionized user experiences by offering a wide range of services without the need for individual app downloads. However, this convenience has raised significant privacy concerns, as these MiniApps often require access to sensitive data, potentially leading to privacy violations. Our research addresses the critical gaps in the analysis of MiniApps’ privacy practices, especially focusing on WeChat MiniApps in the Android ecosystem. Despite existing privacy regulations and platform guidelines, there is a lack of effective mechanisms to safeguard user privacy fully.
We introduce MiniScope, a novel two-phase hybrid analysis approach, specifically designed for the MiniApp environment. This approach overcomes the limitations of existing static analysis techniques by incorporating dynamic UI exploration for complete code coverage and accurate privacy practice identification. Our methodology includes modeling UI transition states, resolving cross-package callback control flows, and automated iterative UI exploration. This allows for a comprehensive understanding of MiniApps’ privacy practices, addressing the unique challenges of sub-package loading and event-driven callbacks. Our empirical evaluation of over 120K MiniApps using MiniScope demonstrates its effectiveness in identifying privacy inconsistencies. The results reveal significant issues, with 5.7% of MiniApps over-collecting private data and 33.4% overclaiming data collection. These findings emphasize the urgent need for more precise privacy monitoring systems and highlight the responsibility of SuperApp operators to enforce stricter privacy measures.","['MiniApps,', 'Privacy', 'Compliance']","['TechnologyWuhanChina', 'WalesSydneyAustralia', 'TechnologyWuhanChina', 'UniversitySingapore', 'TechnologyWuhanChina', 'UniversityMelbourneAustralia', 'UniversitySingapore', 'UniversitySingapore', 'UniversityXi’anChina', 'UniversitySingapore', 'TechnologyWuhanChina']"
"Split Learning (SL) is a promising Distributed Learning approach in electromyography (EMG) based prosthetic control, due to its applicability within resource-constrained environments. Other learning approaches, such as Deep Learning and Federated Learning (FL), provide suboptimal solutions, since prosthetic devices are extremely limited in terms of processing power and battery life. The viability of implementing SL in such scenarios is caused by its inherent model partitioning, with clients executing the smaller model segment. However, selecting an inadequate cut layer hinders the training process in SL systems. This paper presents an algorithm for optimal cut layer selection in terms of maximizing the convergence rate of the model. The performance evaluation demonstrates that the proposed algorithm substantially accelerates the convergence in an EMG pattern recognition task for improving prosthetic device control.","['Index', 'Terms: ', 'Convergence rate maximization,', 'Optimal cut layer,', 'Split learning,', 'Prosthetic control,', 'Electromyography,', 'Deep learning']","['matea.marinova2000@gmail.com', 'danield@feit.ukim.edu.mk', 'hristijang@feit.ukim.edu.mk', 'zoranhv@feit.ukim.edu.mk', 'valentin@feit.ukim.edu.mk']"
"PageRank is a popular centrality metric that assigns importance to the vertices of a graph based on its neighbors and their score. Efficient parallel algorithms for updating PageRank on dynamic graphs is crucial for various applications, especially as dataset sizes have reached substantial scales. This technical report presents our Dynamic Frontier approach. Given a batch update consisting of edge insertions and deletions, it progressively identifies affected vertices that are likely to change their ranks with minimal overhead. On a server equipped with a 64-core AMD EPYC-7742 processor, our Dynamic Frontier PageRank outperforms Static, Naive-dynamic, and Dynamic Traversal PageRank by 7.8×7.8\times7.8 ×, 2.9×2.9\times2.9 ×, and 3.9×3.9\times3.9 × respectively - on uniformly random batch updates of size 10−7⁢|E|superscript107𝐸10^{-7}|E|10 start_POSTSUPERSCRIPT - 7 end_POSTSUPERSCRIPT | italic_E | to 10−3⁢|E|superscript103𝐸10^{-3}|E|10 start_POSTSUPERSCRIPT - 3 end_POSTSUPERSCRIPT | italic_E |. In addition, our approach improves performance at an average rate of 1.8×1.8\times1.8 × for every doubling of threads.","['Parallel', 'PageRank algorithm,', 'Dynamic', 'Frontier approach']",['GachibowliHyderabadTelanganaIndia500032']
"The experimentally-observed non-trivial electronic structure of Cr22{}_{2}start_FLOATSUBSCRIPT 2 end_FLOATSUBSCRIPT dimer
has made the calculation of the potential energy curve a theoretical challenge
in the last decades. By matching the perturbation theory at small internuclear
distances R𝑅Ritalic_R, the multipole expansion at large distances R𝑅Ritalic_R (supposedly both
of asymptotic nature) and by adding a few RKR turning points, extracted from experimental data,
the analytic form for the potential energy curve for the ground state X1⁢Σ+superscript𝑋1superscriptΣX^{1}\Sigma^{+}italic_X start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT roman_Σ start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT
of the Cr22{}_{2}start_FLOATSUBSCRIPT 2 end_FLOATSUBSCRIPT dimer is found for the whole range of internuclear distances R𝑅Ritalic_R.
This has the form of a two-point Pade approximant and provides an accuracy of 3-4 decimal
digits in 29 experimental vibrational energies. The resulting ground state X1⁢Σ+superscript𝑋1superscriptΣX^{1}\Sigma^{+}italic_X start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT roman_Σ start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT
potential curve supports 19694
rovibrational states with a maximal vibrational number νmax=104subscript𝜈max104\nu_{\text{max}}=104italic_ν start_POSTSUBSCRIPT max end_POSTSUBSCRIPT = 104 at zero angular momentum and with a maximal angular momentum Lmax=312subscript𝐿max312L_{\text{max}}=312italic_L start_POSTSUBSCRIPT max end_POSTSUBSCRIPT = 312 with energies
>10−4absentsuperscript104>10^{-4}> 10 start_POSTSUPERSCRIPT - 4 end_POSTSUPERSCRIPT Hartree, and additionally 218 weakly-bound states
(close to the dissociation limit) with energies <10−4absentsuperscript104<10^{-4}< 10 start_POSTSUPERSCRIPT - 4 end_POSTSUPERSCRIPT Hartree.","['Potential curve ', 'Diatomic molecules ', 'Two-point', 'Pade approximation ', 'Rotational and \nvibrational states']","['Mexico', 'Mexico', 'Mexico']"
"One of the most important problems in computer vision and remote sensing is object detection, which identifies particular categories of diverse things in pictures. Two crucial data sources for public security are the thermal infrared (TIR) remote sensing multi-scenario photos and videos produced by unmanned aerial vehicles (UAVs). Due to the small scale of the target, complex scene information, low resolution relative to the viewable videos, and dearth of publicly available labeled datasets and training models, their object detection procedure is still difficult. A UAV TIR object detection framework for pictures and videos is suggested in this study. The Forward-looking Infrared (FLIR) cameras used to gather ground-based TIR photos and videos are used to create the “You Only Look Once” (YOLO) model, which is based on CNN architecture. Results indicated that in the validating task, detecting human object had an average precision at IOU (Intersection over Union) = 0.5, which was 72.5%, using YOLOv7 (YOLO version 7) state of the art model [1], while the detection speed around 161 frames per second (FPS/second). The usefulness of the YOLO architecture is demonstrated in the application, which evaluates the cross-detection performance of people in UAV TIR videos under a YOLOv7 model in terms of the various UAVs’ observation angles. The qualitative and quantitative evaluation of object detection from TIR pictures and videos using deep-learning models is supported favorably by this work.","['Index', 'Terms: \n', 'Human detection —', 'Human tracking —', 'Thermal', 'Imaging —', 'YOLOv7 —', 'UAV']","['walidguettala@gmail.com', 'Sayah.Ali@hotmail.com', 'l.kahloul@univ-biskra.dz', 'ahmed.tibermacine@univ-biskra.dz']"
"A critical issue faced by open-source software projects is the risk of key personnel leaving the project. This risk is exacerbated in large projects that have been under development for a long time and experienced growth in their development teams. One way to quantify this risk is to measure the concentration of knowledge about the project among its developers. Formally known as the Bus Factor (BF) of a project and defined as “the number of key developers who would need to be incapacitated to make a project unable to proceed” (inproceedings, ). Most of the proposed algorithms for BF calculation measure a developer’s knowledge of a file based on the number of commits. In this work, we propose using other metrics like lines of code changes (LOCC) and cosine difference of lines of code (change-size-cos) to calculate the BF. We use these metrics for BF calculation for five open-source GitHub projects using the CST algorithm and the RIG algorithm, which is git-blame-based. Moreover, we calculate the BF on project sub-directories that have seen the most active development recently. Lastly, we compare the results of the two algorithms in accuracy, similarity in results, execution time, and trends in BF values over time.","['Bus factor,', 'Open-Source,', 'Code ownership,', 'Risk management,', 'Mining', 'GitHub repositories']",['AveEugeneOregonUSA97403']
"Digital twins are an important technology for advancing mobile communications,
specially in use cases that
require simultaneously simulating the wireless channel, 3D scenes
and machine learning.
Aiming at providing a solution to this demand,
this work describes a modular co-simulation methodology called CAVIAR.
Here, CAVIAR is upgraded to support a message passing library
and enable the virtual counterpart of a
digital twin system using different 6G-related simulators.
The main contributions of this work
are the detailed description of different CAVIAR architectures, the
implementation of this methodology to assess a 6G use case of UAV-based
search and rescue mission (SAR), and the generation of benchmarking data
about the computational resource usage. For executing the SAR
co-simulation we adopt
five open-source solutions: the physical and link level network simulator
Sionna, the simulator for autonomous vehicles AirSim, scikit-learn
for training a decision tree for MIMO beam selection, Yolov8 for the
detection of rescue targets and NATS for message passing.
Results for the implemented SAR use case suggest that
the methodology can run in a single machine, with the main demanded resources
being the CPU processing and the GPU memory.","['Index', 'Terms: \n6G,', 'AI, co-simulation, digital twin, ray tracing.']","['aldebaro}@ufpa.br', 'pedro.batista@ericsson.com']"
"Scientific discoveries are increasingly constrained by limited storage space and I/O capacities. For time-series simulations and experiments, their data often need to be decimated over timesteps to accommodate storage and I/O limitations. In this paper, we propose a technique that addresses storage costs while improving post-analysis accuracy through spatiotemporal adaptive, error-controlled lossy compression. We investigate the trade-off between data precision and temporal output rates, revealing that reducing data precision and increasing timestep frequency lead to more accurate analysis outcomes. Additionally, we integrate spatiotemporal feature detection with data compression and demonstrate that performing adaptive error-bounded compression in higher dimensional space enables greater compression ratios, leveraging the error propagation theory of a transformation-based compressor.
To evaluate our approach, we conduct experiments using the well-known E3SM climate simulation code and apply our method to compress variables used for cyclone tracking. Our results show a significant reduction in storage size while enhancing the quality of cyclone tracking analysis, both quantitatively and qualitatively, in comparison to the prevalent timestep decimation approach. Compared to three state-of-the-art lossy compressors lacking feature preservation capabilities, our adaptive compression framework improves perfectly matched cases in TC tracking by 26.4-51.3% at medium compression ratios and by 77.3-571.1% at large compression ratios, with a merely 5-11% computational overhead.","['Index', 'Terms: \nspatiotemporal data, timestep decimation, region-wise error-controlled lossy compression, feature preservation']",['1gongq@ornl.gov']
"Hate speech has emerged as a major problem plaguing our social spaces today.
While there have been significant efforts to address this problem, existing methods are still significantly limited in effectively detecting hate speech online.
A major limitation of existing methods is that hate speech detection is a highly contextual problem, and these methods cannot fully capture the context of hate speech to make accurate predictions.
Recently, large language models (LLMs) have demonstrated state-of-the-art performance in several natural language tasks.
LLMs have undergone extensive training using vast amounts of natural language data, enabling them to grasp intricate contextual details. Hence, they could be used as knowledge bases for context-aware hate speech detection. However, a fundamental problem with using LLMs to detect hate speech is that there are no studies on effectively prompting LLMs for context-aware hate speech detection.
In this study, we conduct a large-scale study of hate speech detection, employing five established hate speech datasets. We discover that LLMs not only match but often surpass the performance of current benchmark machine learning models in identifying hate speech.
By proposing four diverse prompting strategies that optimize the use of LLMs in detecting hate speech.
Our study reveals that a meticulously crafted reasoning prompt can effectively capture the context of hate speech by fully utilizing the knowledge base in LLMs, significantly outperforming existing techniques.
Furthermore, although LLMs can provide a rich knowledge base for the contextual detection of hate speech, suitable prompting strategies play a crucial role in effectively leveraging this knowledge base for efficient detection.","['Index', 'Terms: \nhate speech, large language model, prompt engineering, few-shot learning']",['nishant.vishwamitra@utsa.edu']
"Molecular property prediction refers to the task of labeling molecules with some biochemical properties, playing a pivotal role in the drug discovery and design process. Recently, with the advancement of machine learning, deep learning-based molecular property prediction has emerged as a solution to the resource-intensive nature of traditional methods, garnering significant attention. Among them, molecular representation learning is the key factor for molecular property prediction performance. And there are lots of sequence-based, graph-based, and geometry-based methods that have been proposed. However, the majority of existing studies focus solely on one modality for learning molecular representations, failing to comprehensively capture molecular characteristics and information. In this paper, a novel multi-modal representation learning model, which integrates the sequence, graph, and geometry characteristics, is proposed for molecular property prediction, called SGGRL. Specifically, we design a fusion layer to fusion the representation of different modalities. Furthermore, to ensure consistency across modalities, SGGRL is trained to maximize the similarity of representations for the same molecule while minimizing similarity for different molecules. To verify the effectiveness of SGGRL, seven molecular datasets, and several baselines are used for evaluation and comparison. The experimental results demonstrate that SGGRL consistently outperforms the baselines in most cases. This further underscores the capability of SGGRL to comprehensively capture molecular information. Overall, the proposed SGGRL model showcases its potential to revolutionize molecular property prediction by leveraging multi-modal representation learning to extract diverse and comprehensive molecular insights. Our code is released at https://github.com/Vencent-Won/SGGRL.","['Index', 'Terms: ', 'Sequence,', 'Graph,', 'Geometry,', 'Molecular', 'Representation', 'Learning']","['vencent_wang@outlook.com', 'josieyi0319@163.com', 'jhwang@zjut.edu.cn', 'xuanqi@zjut.edu.cn']"
"We show that the propagator derived from an EFT incorporating Weinbeger’s compositeness theorem is the more general formula to describe the S-wave near threshold states. Using the propagator to fit the lineshape, one can extract Z𝑍Zitalic_Z for these states and clarify the structure of these states.","['Exotic state,', 'Compositeness,', 'Effective field theory,', 'Lineshape,', 'Scattering amplitude']","['China', 'China']"
"During the COVID-19 pandemic, a major driver of new surges has been the emergence of new variants. When a new variant emerges in one or more countries, other nations monitor its spread in preparation for its potential arrival. The impact of the variant and the timing of epidemic peaks in a country highly depend on when the variant arrives. The current methods for predicting the spread of new variants rely on statistical modeling, however, these methods work only when the new variant has already arrived in the region of interest and has a significant prevalence.
The question arises: Can we predict when (and if) a variant that exists elsewhere will arrive in a given country and reach a certain prevalence? We propose a variant-dynamics-informed Graph Neural Network (GNN) approach. First, We derive the dynamics of variant prevalence across pairs of regions (countries) that applies to a large class of epidemic models. The dynamics suggest that ratios of variant proportions lead to simpler patterns. Therefore, we use ratios of variant proportions along with some parameters estimated from the dynamics as features in a GNN. We develop a benchmarking tool to evaluate variant emergence prediction over 87 countries and 36 variants. We leverage this tool to
compare our GNN-based approach against our dynamics-only model and a number of machine learning models. Results show that the proposed dynamics-informed GNN method retrospectively outperforms all the baselines, including the currently pervasive framework of Physics-Informed Neural Networks (PINNs) that incorporates the dynamics in the loss function.","['Index', 'Terms: ', 'Graph', 'Neural', 'Networks,', 'COVID-19', 'Variants,', 'Forecasting']","['malaawar@usc.edu', 'mutnuri@usc.edu', 'mmontaze@usc.edu', 'ajiteshs@usc.edu']"
"Accurate prediction of flight-level passenger traffic is of paramount importance in airline operations, influencing key decisions from pricing to route optimization. This study introduces a novel, multimodal deep learning approach to the challenge of predicting flight-level passenger traffic, yielding substantial accuracy improvements compared to traditional models. Leveraging an extensive dataset from American Airlines, our model ingests historical traffic data, fare closure information, and seasonality attributes specific to each flight. Our proposed neural network integrates the strengths of Recurrent Neural Networks (RNN) and Convolutional Neural Networks (CNN), exploiting the temporal patterns and spatial relationships within the data to enhance prediction performance. Crucial to the success of our model is a comprehensive data processing strategy. We construct 3D tensors to represent data, apply careful masking strategies to mirror real-world dynamics, and employ data augmentation techniques to enrich the diversity of our training set. The efficacy of our approach is borne out in the results: our model demonstrates an approximate 33% improvement in Mean Squared Error (MSE) compared to traditional benchmarks. This study, therefore, highlights the significant potential of deep learning techniques and meticulous data processing in advancing the field of flight traffic prediction.","['Index', 'Terms: ', 'Multimodal', 'Deep', 'Learning,', 'Spatial and', 'Sequential', 'Relations,', 'Traffic', 'Prediction,', 'Airline', 'Industry,', 'Machine', 'Learning']","['sinaehsani@arizona.edu', 'elina.sergeeva@aa.com', 'wendy.murdy@aa.com', 'benjamin.fox1@aa.com']"
"An article usually includes an abstract, a concise summary of the work
covered at length in the main body of the article. It is used for
secondary publications and for information retrieval purposes.",['Suggested keywords'],"['[', 'address', 'address']"
"We present a 24μ𝜇\muitalic_μm-selected spectroscopic sample z>0.13𝑧0.13z>0.13italic_z > 0.13 (median ⟨z⟩=0.41delimited-⟨⟩𝑧0.41\langle z\rangle=0.41⟨ italic_z ⟩ = 0.41) in the Lockman Hole field, consisting of 4035 spectra.
Our aim is to identify AGNs and determine their fraction in this mid-infrared selected sample.
In this work, we use the [Ne v]λ𝜆\lambdaitalic_λ3426 emission line to spectroscopically identify AGNs.
Combined with broad-line Type I AGNs selected in our previous study,
our sample consists of 887 (∼similar-to\sim∼22%) spectroscopically confirmed AGNs.
We perform a stacking analysis on the remaining spectra,
and find that in various
MIR-wedge-selected AGN candidates,
the stacked spectra still show significant [Ne v]λ𝜆\lambdaitalic_λ3426 emission,
In contrast, no clear [Ne v]λ𝜆\lambdaitalic_λ3426 signal is detected in non-AGN candidates falling outside the wedges.
Assuming a range of AGN mid-IR SED slope of -0.3<α<absent𝛼absent<\alpha<< italic_α <0.7,
and an average star-forming relation derived from 65 star-forming templates,
we develop a robust method to separate the AGN and star-forming contributions to the mid-IR SEDs
using the rest-frame L1212{}_{12}start_FLOATSUBSCRIPT 12 end_FLOATSUBSCRIPT/L1.61.6{}_{1.6}start_FLOATSUBSCRIPT 1.6 end_FLOATSUBSCRIPT vs L4.54.5{}_{4.5}start_FLOATSUBSCRIPT 4.5 end_FLOATSUBSCRIPT/L1.61.6{}_{1.6}start_FLOATSUBSCRIPT 1.6 end_FLOATSUBSCRIPT diagram.
We separate the objects into bins of L1212{}_{12}start_FLOATSUBSCRIPT 12 end_FLOATSUBSCRIPT,
and find that AGN fraction increases with increasing L1212{}_{12}start_FLOATSUBSCRIPT 12 end_FLOATSUBSCRIPT.
We also find that the stacked [Ne v]λ𝜆\lambdaitalic_λ3426 strength
scales with L1212{}_{12}start_FLOATSUBSCRIPT 12 end_FLOATSUBSCRIPT.
The pure AGN luminosity at 12μ𝜇\muitalic_μm exhibits
a positive correlation with the star formation rates,
indicating possible co-evolution and common gas supply
between the AGN and their host galaxies.
Varying population properties across the redshift range explored contribute to the observed correlation.","['Active galactic nuclei —', 'Star formation']","['China', 'China', 'China', 'China', 'USA', 'UK', 'China']"
"Entity resolution, the task of identifying and consolidating records that pertain to the same real-world entity, plays a pivotal role in various sectors such as e-commerce, healthcare, and law enforcement. The emergence of Large Language Models (LLMs) like GPT-4 has introduced a new dimension to this task, leveraging their advanced linguistic capabilities. This paper explores the potential of LLMs in the entity resolution process, shedding light on both their advantages and the computational complexities associated with large-scale matching. We introduce strategies for the efficient utilization of LLMs, including the selection of an optimal set of matching questions, namely MQsSP, which is proved to be a NP-hard problem. Our approach optimally chooses the most effective matching questions while keep consumption limited to your budget . Additionally, we propose a method to adjust the distribution of possible partitions after receiving responses from LLMs, with the goal of reducing the uncertainty of entity resolution. We evaluate the effectiveness of our approach using entropy as a metric, and our experimental results demonstrate the efficiency and effectiveness of our proposed methods, offering promising prospects for real-world applications.","['Index', 'Terms: ', 'Entity', 'Resolution,', 'Large', 'Language', 'Models,', 'Uncertainty.']","['Technology', 'leichen@cse.ust.hk']"
"Text-conditional image editing based on large diffusion generative model has attracted the attention of both the industry and the research community.
Most existing methods are non-reference editing, with the user only able to provide a source image and text prompt. However, it restricts user’s control over the characteristics of editing outcome. To increase user freedom, we propose a new task called Specific Reference Condition Real Image Editing, which allows user to provide a reference image to further control the outcome, such as replacing an object with a particular one. To accomplish this, we propose a fast baseline method named SpecRef. Specifically, we design a Specific Reference Attention Controller to incorporate features from the reference image, and adopt a mask mechanism to prevent interference between editing and non-editing regions. We evaluate SpecRef on typical editing tasks and show that it can achieve satisfactory performance. The source code is available on https://github.com/jingjiqinggong/specp2p.","['Index', 'Terms: ', 'AIGC, large generative model, text-to-image generation, real image editing, diffusion model.']",['jc.huang@siat.ac.cn']
"Network dynamic (e.g., traffic burst in data center networks and channel fading in cellular WiFi networks) has a great impact on the performance of communication networks (e.g., throughput, capacity, delay, and jitter). This article proposes a unified prediction-based method to handle the dynamic of various network systems. From the view of graph deep learning, I generally formulate the dynamic prediction of networks as a temporal link prediction task and analyze the possible challenges of the prediction of weighted networks, where link weights have the wide-value-range and sparsity issues. Inspired by the high-resolution video frame prediction with generative adversarial network (GAN), I try to adopt adversarial learning to generate high-quality predicted snapshots for network dynamic, which is expected to support the precise and fine-grained network control. A novel high-quality temporal link prediction (HQ-TLP) model with GAN is then developed to illustrate the potential of my basic idea. Extensive experiments for various application scenarios further demonstrate the powerful capability of HQ-TLP.","['Index', 'Terms: ', 'Network', 'Dynamic,', 'Temporal', 'Link', 'Prediction,', 'Weighted', 'Dynamic', 'Networks,', 'Adversarial', 'Learning,', 'High-Quality', 'Prediction', 'Results.']",['mengqin_az@foxmail.com']
"We investigate the warm inflationary scenario within the
context of the linear version of f⁢(Q,T)𝑓𝑄𝑇f(Q,T)italic_f ( italic_Q , italic_T ) gravity, coupled with
both the inflaton scalar field and the radiation field, under the
conditions of the strong dissipation regime. First, we calculate
the modified Friedmann equations and the modified slow-roll
parameters. Subsequently, we apply the slow-roll approximations to
derive the scalar power spectrum and the tensor power spectrum.
Also, we develop formulations of the scalar and tensor
perturbations for the f⁢(Q,T)𝑓𝑄𝑇f(Q,T)italic_f ( italic_Q , italic_T ) gravity with warm inflation
scenario. Furthermore, we scrutinize two different forms of the
dissipation coefficient, a constant and a function of the inflaton
field to determine the scalar spectral index, the tensor-to-scalar
ratio and the temperature for the power-law potential case. By
imposing some constraints on the free parameters of the model, we
attain results in good agreement with both the Planck 2018201820182018 data
and the joint Planck, BK15151515 and BAO data for the tensor-to-scalar
ratio, and consistent results aligned with the Planck 2018201820182018 data
for the scalar spectral index. Consequently, we are able to revive
the power-law potential that was previously ruled out by
observational data. Moreover, for the variable dissipation
coefficient, the model leads to the scalar spectral index with the
blue and red tilts in agreement with the WMAP three years data.","['Warm', 'Inflation;', 'Modified', 'Gravity;', 'Scalar', 'Power', 'Spectrum;', 'Tensor', 'Power', 'Spectrum;', 'Cosmological', 'Inflation;', 'Strong', 'Dissipation', 'Regime']",['Iran']
"As a crucial facilitator of future autonomous driving applications, wireless simultaneous localization and mapping (SLAM) has drawn growing attention recently. However, the accuracy of existing wireless SLAM schemes is limited because the antenna gain is constrained given the cost budget due to the expensive hardware components such as phase arrays. To address this issue, we propose a reconfigurable holographic surface (RHS)-aided SLAM system in this paper. The RHS is a novel type of low-cost antenna that can cut down the hardware cost by replacing phased arrays in conventional SLAM systems. However, compared with a phased array where the phase shifts of parallel-fed signals are adjusted, the RHS exhibits a different radiation model because its amplitude-controlled radiation elements are series-fed by surface waves, implying that traditional schemes cannot be applied directly. To address this challenge, we propose an RHS-aided beam steering method for sensing the surrounding environment and design the corresponding SLAM algorithm. Simulation results show that the proposed scheme can achieve more than there times the localization accuracy that traditional wireless SLAM with the same cost achieves.","['Index', 'Terms: ', 'Simultaneous localization and mapping, reconfigurable holographic surface, point cloud']","['China.', 'China.']"
"Taking into account the importance of the unified theory of quantum mechanics and gravity, and the existence of a minimum length of the order of the Planck scale, we consider a modified Schrödinger equation resulting from a generalised uncertainty principle, which finds applications from the realm of quantum information to large-scale physics, with a quantum mechanically corrected gravitational interaction proposed very recently. As the resulting equation cannot be solved by common exact approaches, including Heun or Lie algebraic ones, we propose a Bethe-Ansatz approach, which will be applied and whose results we discuss.","['minimal length, generalized uncertainty principle,', 'Planck scale,', 'Schrödinger equation, quantum correction,', 'Coulomb potential']","['Czechia', 'Spain', 'Spain', 'Iran']"
"The Sun is one of the most luminous γ𝛾\gammaitalic_γ-ray sources in the sky and continues to challenge our understanding of its high-energy emission mechanisms. This study provides an in-depth investigation of the solar disk γ𝛾\gammaitalic_γ-ray emission, using data from the Fermi Large Area Telescope (LAT) spanning August 2008 to January 2022. We focus on γ𝛾\gammaitalic_γ-ray events with energies exceeding 5 GeV, originating from 0.5∘{}^{\circ}start_FLOATSUPERSCRIPT ∘ end_FLOATSUPERSCRIPT angular aperture centered on the Sun, and implement stringent time cuts to minimize potential sample contaminants. We employ a helioprojection method to resolve the γ𝛾\gammaitalic_γ-ray events relative to the solar rotation axes, and combine statistical tests to investigate the distribution of events over the solar disk. We found that integrating observations over large time windows may overlook relevant asymmetrical features, which we reveal in this work through a more refined time-dependent morphological analysis. We describe significant anisotropic trends and confirm compelling evidence of energy-dependent asymmetry in the solar disk γ𝛾\gammaitalic_γ-ray emission. Intriguingly, the asymmetric signature coincides with the Sun’s polar field flip during the cycle 24 solar maximum, around June 2014. Our findings suggest that the Sun’s magnetic configuration plays a significant role in shaping the resulting γ𝛾\gammaitalic_γ-ray signature, highlighting a potential link between the observed anisotropies, solar cycle, and the solar magnetic fields. These insights pose substantial challenges to established emission models, prompting fresh perspectives on high-energy solar astrophysics.",['Sun () — gamma-rays () — cosmic-rays ()'],"['Portugal', 'Portugal', 'Italia', 'Italy', 'Italia', 'Italy', 'USA']"
"Deep learning based automatic modulation classification (AMC) has received significant attention owing to its potential applications in both military and civilian use cases. Recently, data-driven subsampling techniques have been utilized to overcome the challenges associated with computational complexity and training time for AMC. Beyond these direct advantages of data-driven subsampling, these methods also have regularizing properties that may improve the adversarial robustness of the modulation classifier. In this paper, we investigate the effects of an adversarial attack on an AMC system that employs deep learning models both for AMC and for subsampling. Our analysis shows that subsampling itself is an effective deterrent to adversarial attacks. We also uncover the most efficient subsampling strategy when an adversarial attack on both the classifier and the subsampler is anticipated.","['Index', 'Terms:', 'Automatic modulation classification,', 'Adversarial', 'Deep', 'Learning,', 'Data-driven', 'Subsampling.']","['USA', 'USA', 'akshay.malhotra@interdigital.com']"
"Sophisticated cyber attacks present significant challenges for organizations in detecting and preventing such threats. To address this critical need for advanced defense mechanisms, we propose an Ensemble Defense System (EDS). An EDS is a cybersecurity framework aggregating multiple security tools designed to monitor and alert an organization during cyber attacks. The proposed EDS leverages a comprehensive range of Intrusion Detection System (IDS) capabilities by introducing a hybrid of signature-based IDS and anomaly-based IDS tools. It also incorporates Elasticsearch, an open-source Security Information and Event Management (SIEM) tool, to facilitate data analysis and interactive visualization of alerts generated from IDSs. The effectiveness of the EDS is evaluated through a payload from a bash script that executes various attacks, including port scanning, privilege escalation, and Denial-of-Service (DoS). The evaluation demonstrates the EDS’s ability to detect diverse cyber attacks.","['Index', 'Terms: ', 'Ensemble', 'Defense', 'Systems, network security, hybrid', 'IDS,', 'Security', 'Information and', 'Event', 'Management (SIEM)']","['sarahalh@udel.edu', 'arshiyak@udel.edu']"
"We report a ∼5.2⁢σsimilar-toabsent5.2𝜎\sim 5.2\sigma∼ 5.2 italic_σ detection of the kinetic Sunyaev–Zel’dovich (kSZ) effect in Fourier space, by combining the DESI galaxy groups and the Planck data. We use the density-weighted pairwise kSZ power spectrum as the summary statistic, and the detailed procedure of its measurement is presented in this paper. Meanwhile, we analyze the redshift space group density power spectrum to constrain its bias parameters and photo-z uncertainties. These best fitted parameters are substituted to a non-linear kSZ model, and we fit the measured kSZ power spectrum with this model to constrain the group optical depth τ¯¯𝜏\bar{\tau}over¯ start_ARG italic_τ end_ARG. Selected by a varying lower mass threshold Mthsubscript𝑀thM_{\rm th}italic_M start_POSTSUBSCRIPT roman_th end_POSTSUBSCRIPT, the galaxy group catalogs with different median masses (M~~𝑀\tilde{M}over~ start_ARG italic_M end_ARG) are constructed from the DR9 data of the DESI Legacy Imaging Surveys. M~~𝑀\tilde{M}over~ start_ARG italic_M end_ARG spans a wide range of ∼1013−1014⁢M⊙/hsimilar-toabsentsuperscript1013superscript1014subscriptMdirect-productℎ\sim 10^{13}-10^{14}{\rm M}_{\odot}/h∼ 10 start_POSTSUPERSCRIPT 13 end_POSTSUPERSCRIPT - 10 start_POSTSUPERSCRIPT 14 end_POSTSUPERSCRIPT roman_M start_POSTSUBSCRIPT ⊙ end_POSTSUBSCRIPT / italic_h and the heaviest M~∼1014⁢M⊙/hsimilar-to~𝑀superscript1014subscriptMdirect-productℎ\tilde{M}\sim 10^{14}{\rm M}_{\odot}/hover~ start_ARG italic_M end_ARG ∼ 10 start_POSTSUPERSCRIPT 14 end_POSTSUPERSCRIPT roman_M start_POSTSUBSCRIPT ⊙ end_POSTSUBSCRIPT / italic_h is larger than those of most other kSZ detections. When the aperture photometric filter radius θAPsubscript𝜃AP\theta_{\rm AP}italic_θ start_POSTSUBSCRIPT roman_AP end_POSTSUBSCRIPT is set to be 4.24.24.24.2 arcmin, the M~=1.75×1013⁢M⊙/h~𝑀1.75superscript1013subscriptMdirect-productℎ\tilde{M}=1.75\times 10^{13}{\rm M}_{\odot}/hover~ start_ARG italic_M end_ARG = 1.75 × 10 start_POSTSUPERSCRIPT 13 end_POSTSUPERSCRIPT roman_M start_POSTSUBSCRIPT ⊙ end_POSTSUBSCRIPT / italic_h group sample at the median redshift z~=0.64~𝑧0.64\tilde{z}=0.64over~ start_ARG italic_z end_ARG = 0.64 has the highest kSZ detection S/N=5.2SN5.2{\rm S/N}=5.2roman_S / roman_N = 5.2. By fitting τ¯¯𝜏\bar{\tau}over¯ start_ARG italic_τ end_ARGs from various samples against their M~~𝑀\tilde{M}over~ start_ARG italic_M end_ARGs, we obtain a linear log⁡τ¯−log⁡M~¯𝜏~𝑀\log\bar{\tau}-\log\tilde{M}roman_log over¯ start_ARG italic_τ end_ARG - roman_log over~ start_ARG italic_M end_ARG relation: log⁡τ¯=γ⁢(log⁡M~−14)+log⁡β¯𝜏𝛾~𝑀14𝛽\log\bar{\tau}=\gamma(\log\tilde{M}-14)+\log\betaroman_log over¯ start_ARG italic_τ end_ARG = italic_γ ( roman_log over~ start_ARG italic_M end_ARG - 14 ) + roman_log italic_β, in which γ=0.55±0.1𝛾plus-or-minus0.550.1\gamma=0.55\pm 0.1italic_γ = 0.55 ± 0.1. We also vary the aperture photometric filter radius and measure the τ¯¯𝜏\bar{\tau}over¯ start_ARG italic_τ end_ARG profiles of group samples, whose constraints on the baryon distribution within and around dark matter halos will be discussed in a companion paper.","['methods: data analysis, numerical — cosmology: large-scale structure of', 'Universe, theory']","['China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China']"
"Finetuned large language models (such as ChatGPT and Qwen-chat) can generate Chinese classical poetry following human’s instructions. LLMs perform well in content, but are usually lacking in format, with occasionally excess or insufficient number of characters in each line.
Since most SOTA LLMs are token-based, we assume that the format inaccuracy is due to the difficulty of the ”token planning” task, which means that the LLM need to know exactly how much characters are contained in each token and do length-control planning based on that knowledge.
In this paper, we first confirm our assumption by showing that existing token-based large language models has limited knowledge on token-character relationship. We use a spelling bee probing procedure, and find that Qwen-chat failed in nearly 15% Chinese spelling test.
We then show that a token-based model can be easily tailored into a token-free model (in terms of Chinese), which can largely solve the format accuracy problem. Our tailoring procedure removes long-token from vocabulary and keeps only character-level or byte-level tokens.
As part of our contribution, we release the finetuned token-free model (which is based on Qwen-chat-7B), which can generate chinese classical poetry following complex instructions like LLMs (such as story paraphrasing), and also perform well in format. On the test set, our token-free model achives an format accuracy of 0.96, compared to 0.84 for token-based counterparts and 0.38 for GPT-4.","['Token', 'Free,', 'Controllable', 'Text', 'Generation,', 'Large', 'Language', 'Models,', 'Classical', 'Poetry']",['Group']
"The JCMT Transient Survey has been monitoring eight Gould Belt low-mass star-forming regions since December 2015 and six somewhat more distant intermediate-mass star-forming regions since February 2020 with SCUBA-2 on the JCMT at 450 μ𝜇\muitalic_μm and 850 μ𝜇\muitalic_μm and with an approximately monthly cadence. We introduce our Pipeline v2 relative calibration procedures for image alignment and flux calibration across epochs, improving on our previous Pipeline v1 by decreasing measurement uncertainties and providing additional robustness. These new techniques work at both 850 μ𝜇\muitalic_μm and 450 μ𝜇\muitalic_μm, where v1 only allowed investigation of the 850 μ𝜇\muitalic_μm data. Pipeline v2 achieves better than 0.5′′superscript0.5′′0.5^{\prime\prime}0.5 start_POSTSUPERSCRIPT ′ ′ end_POSTSUPERSCRIPT relative image alignment, less than a tenth of the submillimeter beam widths. The v2 relative flux calibration is found to be 1% at 850 μ𝜇\muitalic_μm and <5absent5<5< 5% at 450 μ𝜇\muitalic_μm. The improvement in the calibration is demonstrated by comparing the two pipelines over the first four years of the survey and recovering additional robust variables with v2. Using the full six years of the Gould Belt survey the number of robust variables increases by 50 %, and at 450 μ𝜇\muitalic_μm we identify four robust variables, all of which are also robust at 850 μ𝜇\muitalic_μm. The multi-wavelength light curves for these sources are investigated and found to be consistent with the variability being due to dust heating within the envelope in response to accretion luminosity changes from the central source.","['stars: formation – techniques: image processing – stars: variables:', 'T', 'Tauri,', 'Herbig', 'Ae/Be – stars: protostars – submillimetre:', 'ISM – surveys']","['Canada', 'USA', 'USA', 'Korea', 'Canada', 'Canada', 'Canada', 'Korea', 'China', 'China', 'USA', 'China', 'Korea', 'Canada', 'Canada', 'UK', 'Korea', 'R.O.C.', 'Korea', '210023', 'R.O.C.', 'R.O.C.', '210023']"
"The Competition on Legal Information Extraction/Entailment (COLIEE) is held annually to encourage advancements in the automatic processing of legal texts. Processing legal documents is challenging due to the intricate structure and meaning of legal language. In this paper, we outline our strategies for tackling Task 2, Task 3, and Task 4 in the COLIEE 2023 competition. Our approach involved utilizing appropriate state-of-the-art deep learning methods, designing methods based on domain characteristics observation, and applying meticulous engineering practices and methodologies to the competition. As a result, our performance in these tasks has been outstanding, with first places in Task 2 and Task 3, and promising results in Task 4.
Our source code is available at https://github.com/Nguyen2015/CAPTAIN-COLIEE2023/tree/coliee2023.","['COLIEE competition,', 'Legal', 'Text', 'Processing,', 'CAPTAIN', 'Team']",['TechnologyIshikawaJapan']
"The merchant-regulatory mechanism represents a promising tool that combines the benefits of merchant investment and regulated investment, thereby providing efficient incentives for merchant Transmission Companies (Transcos) subject to regulatory compliance. However, one of the drawbacks of the H-R-G-V merchant-regulated mechanism is that it allows the Transco to capture the entire surplus increase resulting from investment, without any economic benefits for consumers and generators. To address this issue, we propose an incentive tuning parameter, which is incorporated into the calculation of the incentive fee for the Transco. Accordingly, the regulatory framework can effectively manage the Transco’s profit and allow market participants to access economic benefits, thus ensuring a fair distribution of economic advantages among the stakeholders, while the impact on overall social welfare remains relatively modest.","['Index', 'Terms: ', 'Economic benefits, fairness, incentive tuning parameter, merchant-regulatory mechanism.']","['thomas.morstyn@ed.ac.uk', 'iacopo.savelli@unibocconi.it']"
"In this study, a path-based Statistical Static Timing Analysis (SSTA) is formulated as a problem within the statistics of correlated extremes. For extreme value statistics with correlations, a novel approach to studying such systems, when the correlations are small, is developed. The approach considers a system from the first principals, starting from the multivariable characteristic function. Analytical solutions to the problem of weakly correlated extremes are obtained in the form of corrections to the Gumbel distribution. These solutions are compared with Monte Carlo simulations. The applicability limits of the proposed solutions are studied. An algorithm to estimate the covariance matrix of a timing graph is proposed.","['Index', 'Terms: ', 'Timing', 'Graph,', 'Statistical', 'Timing', 'Analysis (SSTA),', 'Delay,', 'Very', 'Large', 'Scale', 'Integration (VLSI),', 'Extreme', 'Value', 'Distribution']",['Ireland']
"Network Intrusion Detection Systems (IDS) aim to detect the presence of an intruder by analyzing network packets arriving at an internet connected device. Data-driven deep learning systems, popular due to their superior performance compared to traditional IDS, depend on availability of high quality training data for diverse intrusion classes. A way to overcome this limitation is through transferable learning, where training for one intrusion class can lead to detection of unseen intrusion classes after deployment. In this paper, we provide a detailed study on the transferability of intrusion detection. We investigate practical federated learning configurations to enhance the transferability of intrusion detection. We propose two techniques to significantly improve the transferability of a federated intrusion detection system. The code for this work can be found at https://github.com/ghosh64/transferability.","['Index', 'Terms: ', 'Network intrusion detection,', 'Transferability,', 'Federated learning.']","['USA', 'elgamala}@purdue.edu']"
"Using the Karl G. Jansky Very Large Array (VLA), we have detected absorption lines due to carbon-monoxide, CO(J𝐽Jitalic_J=0→→\rightarrow→1), and the cyano radical, CN(N𝑁Nitalic_N=0→→\rightarrow→1), associated with radio galaxy B2 0902+34 at redshift z𝑧zitalic_z = 3.4. The detection of millimeter-band absorption observed 1.5 Gyr after the Big Bang facilitates studying molecular clouds down to gas masses inaccessible to emission-line observations. The CO absorption in B2 0902+34 has a peak optical depth of τ𝜏\tauitalic_τ ≥\geq≥ 8.6%percent\%% and consists of two components, one of which has the same redshift as previously detected 21-cm absorption of neutral hydrogen (H I) gas. Each CO component traces an integrated H22{}_{2}start_FLOATSUBSCRIPT 2 end_FLOATSUBSCRIPT column density of NH2subscript𝑁subscriptH2N_{\rm H_{2}}italic_N start_POSTSUBSCRIPT roman_H start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_POSTSUBSCRIPT ≳greater-than-or-equivalent-to\gtrsim≳ 3 ×\times× 102020{}^{20}start_FLOATSUPERSCRIPT 20 end_FLOATSUPERSCRIPT cm−22{}^{-2}start_FLOATSUPERSCRIPT - 2 end_FLOATSUPERSCRIPT. CN absorption is detected for both CO components, as well as for a blueshifted component not detected in CO, with CO/CN line ratios ranging from ≲less-than-or-similar-to\lesssim≲0.4 to 2.4. We discuss the scenario that the absorption components originate from collections of small and dense molecular clouds that are embedded in a region with more diffuse gas and high turbulence, possibly within the influence of the central Active Galactic Nucleus or starburst region. The degree of reddening in B2 0902+34, with a rest-frame color B−K𝐵𝐾B-Kitalic_B - italic_K ∼similar-to\sim∼ 4.2, is lower than the very red colors (B−K𝐵𝐾B-Kitalic_B - italic_K >>> 6) found among other known redshifted CO absorption systems at z𝑧zitalic_z <<< 1. Nevertheless, when including also the many non-detections from the literature, a potential correlation between the absorption-line strength and B−K𝐵𝐾B-Kitalic_B - italic_K color is evident, giving weight to the argument that the red colors of CO absorbers are due to a high dust content.",['Molecular clouds — radio sources — radio jets — radio spectroscopy — quasar absorption line spectroscopy — cosmological parameters — extinction — galaxy evolution'],"['USA', 'Zealand', 'Netherlands', 'France', 'USA', 'USA', 'Netherlands.', 'Netherlands', 'Netherlands', 'Spain', 'France', 'Netherlands.', 'Netherlands', 'Netherlands.', 'Netherlands']"
"Mindfulness-based therapies have been shown to be effective in improving mental health, and technology-based methods have the potential to expand the accessibility of these therapies. To enable real-time personalized content generation for mindfulness practice in these methods, high-quality computer-synthesized text-to-speech (TTS) voices are needed to provide verbal guidance and respond to user performance and preferences. However, the user-perceived quality of state-of-the-art TTS voices has not yet been evaluated for administering mindfulness meditation, which requires emotional expressiveness. In addition, work has not yet been done to study the effect of physical embodiment and personalization on the user-perceived quality of TTS voices for mindfulness. To that end, we designed a two-phase human subject study. In Phase 1, an online Mechanical Turk between-subject study (N=471) evaluated 3 (feminine, masculine, child-like) state-of-the-art TTS voices with 2 (feminine, masculine) human therapists’ voices in 3 different physical embodiment settings (no agent, conversational agent, socially assistive robot) with remote participants. Building on findings from Phase 1, in Phase 2, an in-person within-subject study (N=94), we used a novel framework we developed for personalizing TTS voices based on user preferences, and evaluated user-perceived quality compared to best-rated non-personalized voices from Phase 1. We found that the best-rated human voice was perceived better than all TTS voices; the emotional expressiveness and naturalness of TTS voices were poorly rated, while users were satisfied with the clarity of TTS voices. Surprisingly, by allowing users to fine-tune TTS voice features, the user-personalized TTS voices could perform almost as well as human voices, suggesting user personalization could be a simple and very effective tool to improve user-perceived quality of TTS voice.","['human-robot interaction, text-to-speech, mindfulness therapy']","['AngelesCAUSA', 'AngelesCAUSA', 'AngelesCAUSA', 'AngelesCAUSA', 'AngelesCAUSA', 'AngelesCAUSA']"
"Systolic arrays are popular for executing deep neural networks (DNNs) at the edge. Low latency and energy efficiency are key requirements in edge devices such as drones and autonomous vehicles.
Monolithic 3D (Mono3D) is an emerging 3D integration technique that offers ultra-high bandwidth among processing and memory elements with a negligible area overhead. Such high bandwidth can help meet the ever-growing latency and energy efficiency demands for DNNs. This paper presents a novel implementation for weight stationary (WS) dataflow in Mono3D systolic arrays, called WS-Mono3D. WS-Mono3D utilizes multiple resistive RAM layers and SRAM with high-density vertical interconnects to multicast inputs and perform high-bandwidth weight pre-loading while maintaining the same order of multiply-and-accumulate operations as in native WS dataflow. Consequently, WS-Mono3D eliminates input and weight forwarding cycles and, thus, provides up to 40% improvement in energy-delay-product (EDP) over the native WS implementation in 2D at iso-configuration.
WS-Mono3D also provides 10×\times× improvement in inference per second per watt per footprint due to multiple vertical tiers.
Finally, we also show that temperature impacts the energy efficiency benefits in WS-Mono3D.","['Index', 'Terms: ', 'Monolithic 3D, deep neural networks, systolic arrays, dataflow, energy efficiency, temperature.']","['acoskun)@bu.edu', 'vasileios.pavlidis@manchester.ac.uk', 'emre.salman@stonybrook.edu']"
"In the dynamic urban landscape, where interplay of vehicles and pedestrians defines the rhythm of life, integrating advanced technology for safety and efficiency is increasingly crucial. This study delves into the application of cutting-edge technological methods in smart cities, focusing on enhancing public safety through improved traffic accident detection. Action recognition plays a pivotal role in interpreting visual data and tracking object motion such as human pose estimation in video sequences. The challenges of action recognition include variability in rapid actions, limited dataset, and environmental factors such as (Weather, Illumination, and Occlusions). In this paper, we present a novel comprehensive dataset for traffic accident detection. This dataset is specifically designed to bolster computer vision and action recognition systems in predicting and detecting road traffic accidents. We integrated datasets from wide variety of data sources, road networks, weather conditions, and regions across the globe. This approach is underpinned by empirical studies, aiming to contribute to the discourse on how technology can enhance the quality of life in densely populated areas.
This research aims to bridge existing research gaps by introducing benchmark datasets that leverage state-of-the-art algorithms tailored for traffic accident detection in smart cities. These dataset is expected to advance academic research and also enhance real-time accident detection applications, contributing significantly to the evolution of smart urban environments. Our study marks a pivotal step towards safer, more efficient smart cities, harnessing the power of AI and machine learning to transform urban living.","['Index', 'Terms: ', 'Traffic', 'Surveillance,', 'Accident', 'Detection,', 'Action', 'Recognition,', 'Smart', 'City,', 'Autonomous', 'Transportation']","['Adewopva@mail.uc.edu', 'elsayeny@ucmail.uc.edu', 'elsayezs@ucmail.uc.edu', 'ozermm@ucmail.uc.edu', 'kzekios@fiu.edu', 'abdel1a@cmich.edu', 'magdy.bayoumi@louisiana.edu']"
"In this paper, the Statistical Static Timing Analysis (SSTA) is considered within the block–based approach. The statistical model of the logic gate delay propagation is systematically studied and the exact analytical solution is obtained, which is strongly non-Gaussian. The procedure of handling such (non-Gaussian) distributions is described and the corresponding algorithm for the critical path delay is outlined. Finally, the proposed approach is tested and compared with Monte Carlo simulations.","['Statistical', 'Static', 'Timing', 'Analysis; non-Gaussian; delay;', 'Integrated', 'Circuit']",['Ireland']
"Recommendation algorithms have been pivotal in handling the overwhelming volume of online content. However, these algorithms seldom consider direct user input, resulting in superficial interaction between them. Efforts have been made to include the user directly in the recommendation process through conversation, but these systems too have had limited interactivity. Recently, Large Language Models (LLMs) like ChatGPT have gained popularity due to their ease of use and their ability to adapt dynamically to various tasks while responding to feedback. In this paper, we investigate the effectiveness of ChatGPT as a top-n conversational recommendation system. We build a rigorous pipeline around ChatGPT to simulate how a user might realistically probe the model for recommendations: by first instructing and then reprompting with feedback to refine a set of recommendations. We further explore the effect of popularity bias in ChatGPT’s recommendations, and compare its performance to baseline models. We find that reprompting ChatGPT with feedback is an effective strategy to improve recommendation relevancy, and that popularity bias can be mitigated through prompt engineering.","['Recommender', 'Systems,', 'Generative', 'AI,', 'LLM,', 'Prompt', 'Engineering']","['LouisvilleLouisvilleKentuckyUSA', 'LouisvilleLouisvilleKentuckyUSA', 'MicrosoftSeattleWashingtonUSA', 'LouisvilleLouisvilleKentuckyUSA']"
"Collaborative Edge Computing (CEC) is a new edge computing paradigm that enables neighboring edge servers to share computational resources with each other.
Although CEC can enhance the utilization of computational resources, it still suffers from resource waste.
The primary reason is that end-users from the same area are likely to offload similar tasks to edge servers, thereby leading to duplicate computations.
To improve system efficiency, the computation results of previously executed tasks can be cached and then reused by subsequent tasks.
However, most existing computation reuse algorithms only consider one edge server, which significantly limits the effectiveness of computation reuse.
To address this issue, this paper applies computation reuse in CEC networks to exploit the collaboration among edge servers.
We formulate an optimization problem that aims to minimize the overall task response time and decompose it into a caching subproblem and a scheduling subproblem.
By analyzing the properties of optimal solutions, we show that the optimal caching decisions can be efficiently searched using the bisection method.
For the scheduling subproblem, we utilize projected gradient descent and backtracking to find a local minimum.
Numerical results show that our algorithm significantly reduces the response time in various situations.","['Index', 'Terms: ', 'Computation', 'Reuse,', 'Collaborative', 'Edge', 'Computing,', 'Collaborative', 'Caching,', 'Scheduling', 'Algorithm']",['3tonyquek@sutd.edu.sg']
"In this paper, we propose a new message passing algorithm that utilizes hybrid vector message passing (HVMP) to solve the generalized bilinear factorization (GBF) problem. The proposed GBF-HVMP algorithm integrates expectation propagation (EP) and variational message passing (VMP) via variational free energy minimization, yielding tractable Gaussian messages.
Furthermore, GBF-HVMP enables vector/matrix variables rather than scalar ones in message passing, resulting in a loop-free Bayesian network that improves convergence. Numerical results show that GBF-HVMP significantly outperforms state-of-the-art methods in terms of NMSE performance and computational complexity.","['Index', 'Terms: ', 'Generalized bilinear factorization, message passing, variational free energy,\nexpectation propagation.']","['China', 'qguo@uow.edu.au']"
"The precise assessment of speed-space time delay (TD) significantly facilitates the differentiation between pedestrian anticipation behavior and reaction behavior. Importantly, the computation of TD within a crowd is demonstrated to be instrumental in the evaluation of potential collision risks inherent in the crowd, thereby offering crucial quantitative metrics for crowd risk. This article introduces the CosIn algorithm for calculating TD during pedestrian motion, comprising the CosIn-1 and CosIn-2 algorithms. The CosIn-1 algorithm specifically addresses the precise computation issue associated with the TD of individual pedestrians, while the CosIn-2 algorithm is employed for assessing TD at a crowd scale, concurrently addressing the imperative of real-time computation. Efficacy analyses of the CosIn-1 and CosIn-2 algorithms are conducted using data from single-file pedestrian experiments and crowd cross experiments, respectively. The results obtained demonstrate commendable precision in the algorithmic solutions. This algorithm is poised to make a significant contribution to the precise assessment of both pedestrian and crowd dynamics.","['Algorithm,', 'Statistical analysis,', 'Speed-Space time delay,', 'Crowd dynamic,', 'Experiment']","['China', 'Lv)', 'China', 'China']"
"We investigate the flow of an electrolyte through a rigid nanochannel decorated with a surface charge pattern. Employing lattice Boltzmann and dissipative particle dynamics methods, as well as analytical theory, we show that electro-hydrodynamic coupling leads to two distinct flow profiles. The accompanying discontinuous transition between slow, ionic and fast, Poiseuille regime is observed only at intermediate ion concentrations, channel widths and electrostatic coupling strengths, implying that the phenomenon is a mesoscopic collective effect that is not captured by the weak or the strong coupling limits.
These findings indicate routes to design nanochannels containing a typical aqueous electrolyte that exhibit a digital on/off flux response, which could be useful for nanofluidics and ionotronic applications.","['nanochannel, fluid flow, lattice', 'Boltzmann, dissipative particle dynamics']","['USA', 'Spain', 'Spain']"
"††‡‡{\ddagger}‡Zhou Yang is the corresponding author.
Educators are increasingly concerned about the usage of Large Language Models (LLMs) such as ChatGPT in programming education, particularly regarding the potential exploitation of imperfections in Artificial Intelligence Generated Content (AIGC) Detectors for academic misconduct.
In this paper, we present an empirical study where the LLM is examined for its attempts to bypass detection by AIGC Detectors. This is achieved by generating code in response to a given question using different variants. We collected a dataset comprising 5,069 samples, with each sample consisting of a textual description of a coding problem and its corresponding human-written Python solution codes. These samples were obtained from various sources, including 80 from Quescol, 3,264 from Kaggle, and 1,725 from LeetCode. From the dataset, we created 13 sets of code problem variant prompts, which were used to instruct ChatGPT to generate the outputs. Subsequently, we assessed the performance of five AIGC detectors. Our results demonstrate that existing AIGC Detectors perform poorly in distinguishing between human-written code and AI-generated code.","['Software', 'Engineering', 'Education,', 'AI-Generated', 'Code,', 'AI-Generated', 'Code', 'Detection']","['JayaMalaysia', 'JayaMalaysia', 'JayaMalaysia', 'JayaMalaysia', 'JayaMalaysia', 'UniversitySingaporeSingapore', 'JayaMalaysia', 'UniversitySingaporeSingapore', 'JayaMalaysia']"
"Communication among healthcare professionals (HCPs) is crucial for the quality of patient treatment.
Surrounding each patient’s treatment, communication among HCPs can be examined as temporal networks, constructed from Electronic Health Record (EHR) access logs.
This paper introduces EHRFlow, a visual analytics system designed to study the effectiveness and efficiency of temporal communication networks mediated by the EHR system. We present a method that associates network measures with patient survival outcomes and devises effectiveness metrics based on these associations.
To analyze communication efficiency, we extract the latencies and frequencies of EHR accesses.
EHRFlow is designed to assist in inspecting and understanding the composed communication effectiveness metrics and to enable the exploration of communication efficiency by encoding latencies and frequencies in an information flow diagram. We demonstrate and evaluate EHRFlow through multiple case studies and an expert review.","['Index', 'Terms: ', 'Visual', 'Analytics,', 'Network', 'Measures,', 'Network', 'Comparison,', 'Temporal', 'Networks,', 'Dynamic', 'Network', 'Visualization,', 'Healthcare,', 'Electronic', 'Health', 'Records']","['hyllu@ucdavis.edu', 'ranli@ucdavis.edu', 'klma@ucdavis.edu']"
"Time-series data exists in every corner of real-world systems and services, ranging from satellites in the sky to wearable devices on human bodies. Learning representations by extracting and inferring valuable information from these time series is crucial for understanding the complex dynamics of particular phenomena and enabling informed decisions. With the learned representations, we can perform numerous downstream analyses more effectively. Among several approaches, deep learning has demonstrated remarkable performance in extracting hidden patterns and features from time-series data without manual feature engineering. This survey first presents a novel taxonomy based on three fundamental elements in designing state-of-the-art universal representation learning methods for time series. According to the proposed taxonomy, we comprehensively review existing studies and discuss their intuitions and insights into how these methods enhance the quality of learned representations. Finally, as a guideline for future studies, we summarize commonly used experimental setups and datasets and discuss several promising research directions. An up-to-date corresponding resource is available at https://github.com/itouchz/awesome-deep-time-series-representations.","['time series, representation learning, neural networks, temporal modeling']",['Korea']
"In Federated Recommendation (FedRec) systems, communication costs are a critical bottleneck that arises from the need to transmit neural network models between user devices and a central server.
Prior approaches to these challenges often lead to issues such as computational overheads, model specificity constraints, and compatibility issues with secure aggregation protocols. In response, we propose a novel framework, called Correlated Low-rank Structure (CoLR), which leverages the concept of adjusting lightweight trainable parameters while keeping most parameters frozen. Our approach substantially reduces communication overheads without introducing additional computational burdens. Critically, our framework remains fully compatible with secure aggregation protocols, including the robust use of Homomorphic Encryption.
Our approach resulted in a reduction of up to 93.75% in payload size, with only an approximate 8% decrease in recommendation performance across datasets. Code for reproducing our experiments can be found at https://github.com/NNHieu/CoLR-FedRec.","['Recommendation', 'System,', 'Federated', 'Learning,', 'Communication efficiency']","['VinUniversityHanoiVietnam', 'VinUniversityHanoiVietnam', 'VinUniversityHanoiVietnam', 'VinUniversityHanoiVietnam', 'VinUniversityHanoiVietnam', 'VinUniversityHanoiVietnam']"
